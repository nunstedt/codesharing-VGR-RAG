22 to the TEU and TFEU and may decide not to fully apply the prohibitions based on Article 16 of the TFEU (see Recital 41).
5 See for definitions of these terms also the Commission Notice - The 'Blue Guide' on the implementation of EU product rules 2022, 2022/C 247/01, Section 2.
6 Article 3(10) AI Act.
7 Article 3(12) AI Act.
For example, a provider builds a RBI system outside the Union and supplies that system to a law enforcement authority or to a private company established in a Member State to be used for the first time, thereby putting it into service.
For example, a public authority develops a scoring system in-house and deploys it to predict the risk of fraud of household allowance beneficiaries, thereby putting it into service.
- (14) While the ' use' of an AI system is not explicitly defined in the AI Act, it should be understood in a broad manner to cover the use or deployment of the system at any moment of its lifecycle after having been placed on the market or put into service. This may also cover the integration of the AI system in the services and processes of the person(s) making use of the AI system, including as part of more complex systems, processes or infrastructure. While providers of AI systems must consider the conditions of use which may be reasonably foreseen prior to placing their AI systems on the market (intended use and reasonably foreseeable misuse 8 ),  deployers remain responsible for taking the lawful conditions for the use of the system into account. 9 For the purposes of Article 5 AI Act, the reference to 'use' should be understood to include any misuse of an  AI  system  ('reasonably  foreseeable'  or  not)  that  may  amount  to  a  prohibited practice. 10
For example, an AI system used by an employer to infer emotions at the workspace is prohibited, except when used for medical or safety purposes (Article 5(1)(f) AI Act). The prohibition applies to deployers regardless of whether the provider (the supplier of the system) has excluded such use in its contractual relationships with the deployer (the employer), i.e. in the terms of use.
## . Personal scope: responsible actors
- (15) The AI Act distinguishes between different categories of operators in relation to AI systems: providers, deployers, importers, distributors, and product manufacturers. The present Guidelines will focus only on providers and deployers given the scope of the prohibited practices in Article 5 AI Act.
- (16) According  to  Article  3(3)  AI  Act, providers are  natural  or  legal  persons,  public authorities, agencies or other bodies, that develop AI systems or have them developed and place them on the Union market, or put them into service under their own name or trademark 11 (see section  above). Providers established or located outside the Union
8 See Article 3(12) and (13) AI Act.
9 See for definitions of these terms also the Commission Notice - The 'Blue Guide' on the implementation of EU product rules 2022, 2022/C 247/01, Section .
10 Recital 28 AI Act.
11 Article 3(3), (9) and (11) AI Act. In relation to high-risk AI systems, Article 25 AI Act envisages that 1. Any distributor, importer, deployer or other third-party shall be considered to be a provider of a high-risk AI system for the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of the following circumstances: (a) they put their name or trademark on a high-risk AI system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations are otherwise allocated; (b) they make a substantial modification to a high-risk AI system that has already been placed on the market or has already
are subject to the provisions of the AI Act if they place those systems on the market or put them into service in the Union, 12 or if the output of the AI system is used in the Union 13 . Providers must ensure their AI systems meet all relevant requirements before placing them on the market or putting them into service.
For example, a provider of a RBI system is the manufacturer of the system that markets the system in the Union under its trademark. The provider of such a system could also be a public authority that develops the system in-house and puts it into service for its own use.
- (17) Deployers are  natural  or  legal  persons,  public  authorities,  agencies  or  other  bodies using AI systems under their authority, unless the use is for a personal non-professional activity. 14 'Authority' over an AI system should be understood as assuming responsibility over the decision to deploy the system and over the manner of its actual use. Deployers fall within the scope of the AI Act, if their place of establishment or location is within the Union 15 or, if they are located in a third country, the output of the AI system is used in the Union 16 .
- (18) Where the deployer of an AI system is a legal person under whose authority the system is used, i.e. a law enforcement authority or a private security company, the individual employees that act within the procedures and under the control of that person should not  be  considered  to  be  the  deployer.  A  legal  person  also  remains  a  deployer  if  it involves third parties (e.g., contractors, external staff) in the operation of the system on its behalf and under its responsibility and control.
- (19) Operators may fulfil more than one role concurrently in relation to an AI system. For example, if an operator develops its own AI system that it uses afterwards, it will be considered both the provider and the deployer of that system, even if that system is also used by other deployers to whom the system has been provided in return for payment or free of charge.
- (20) Continuous compliance with the AI Act is required during all phases of the AI lifecycle. This necessitates ongoing monitoring of and updates to AI systems placed on the market or put into service in the Union to ensure that an AI system remains compliant with the AI Act throughout its lifecycle and that it does not result in a practice prohibited under Article 5 AI Act. Providers and deployers of AI systems have different responsibilities depending on their roles and control over the design, the development and the actual use of the system to avoid a prohibited practice. For each of the prohibitions, these roles and responsibilities should be interpreted in a proportionate manner, taking into account
been put into service in such a way that it remains a high-risk AI system pursuant to Article 6; (c) they modify the intended purpose of an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service in such a way that the AI system concerned becomes a high-risk AI system in accordance with Article 6.
12 Article 2(1)(a) AI Act.
13 Article 2(1)(c) AI Act.
14 Article 3(4) AI Act.
15 Article 2(1)(b) AI Act.
16 Article 2(1)(c) AI Act.
