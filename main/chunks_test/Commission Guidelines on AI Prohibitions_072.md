EU:C:2016:800 , Case C-611/14, para 73.
69 Judgment of the Court of Justice of 19 December 2013, Trento Sviluppo and Centrale Adriatica, C-281/12, EU:C:2013:859.
70 Commission Notice - Guidance on the interpretation and application of Directive 2005/29/EC of the European Parliament and of the Council concerning unfair business-to-consumer commercial practices in the internal market (OJ C 526, , p. 1).
the point of view of the 'average' consumer, which is the benchmark developed by the CJEU, now integrated in the UCPD 71 .
- (81) In  the  context  of  the  prohibition  in  Article  5(1)(a)  AI  Act,  market  surveillance authorities must also investigate each case's specific facts and circumstances, assessing whether the subliminal, purposefully manipulative or deceptive technique deployed by the AI system is likely to appreciably impair the decision-making, individual autonomy and free choice of an 'average' individual within a targeted group when the system affects a group of persons in a manner that is reasonably likely to cause significant harm.  Such  an  interpretation  seems  justified  given  that  the  AI  Act  intends  to complement the UCPD 72  and must be applied in a consistent manner. At the same time, given that Article 5(1)(a) AI Act also refers to the possibility to distort the 'behaviour of a natural person' and, if the perspective of the 'average' individual proves difficult or  ineffective  to  assess  in  certain  contexts  (for  example  due  to  very  tailored  or 'personalised' manipulation or harmful effects on specific vulnerable groups), specific cases may be examined also from the perspective of specific individuals by assessing to what extent an AI system deploying subliminal, purposefully manipulate or deceptive techniques is capable of undermining their individual autonomy in concrete cases and significant harm has occurred or is likely to occur.
## b) Scenario 1: Prohibited AI systems 'with the objective to' materially distort behaviour
- (82) Article 5(1)(a) AI Act applies to AI systems deploying the above-mentioned techniques and having as a first scenario 'the objective to  materially distort the behaviour of a person or a group of persons' . Such an objective may be pursued by the provider or the deployer of the AI system, or by the system itself within the implicit objectives it may pursue 73 . This objective should be distinguished from the 'intended purpose' of the AI system  (Article  3(12)  AI  Act).  Even  if  intended  by  the  provider,  the  manipulative objective is in most cases not the purpose of the use for which the system is offered and it is often neither transparent, nor specified as such in the information supplied by the provider (e.g. in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation).
For  example,  a  chatbot  that  may  be  used  in  different  contexts  is  designed  to  use subliminal  messaging techniques,  such  as  flashing  brief  visual  cues  and  embedding
71 See Recitals 18 and 19 UCPD. 'Average consumer' is a person who is reasonably well informed and reasonably observant and circumspect, considering social, cultural and linguistic factors. The average consumer test is not a statistical test (i.e., it does not require to prove that a certain percentage of consumers would have been materially distorted/appreciably impaired by a business practice). The test is based on the principle of proportionality. The UCPD adopted this notion to strike the right balance between the need to protect consumers and the promotion of free trade in an openly competitive market. Courts and authorities will have to exercise their own faculty of judgment to  determine the typical reaction of the average consumer in a given case. In the UCPD Guidance, the Commission advised them to make use of behavioural insights and other data. Case C-646/22, Compass Banca , clarifies that the definition of the average consumer does not exclude the possibility that an individual's decision-making capacity may be impaired by constraints, such as cognitive biases. Judgment of the Court (Fifth Chamber) of 14 November 2024. Compass Banca SpA v Autorit√† Garante della Concorrenza e del Mercato (AGCM), Case C-646/22, EU:C:2024:957.
72 Recital 29 AI Act.
73 See Article 3(1) AI Act which states that the AI system may pursue implicit or explicit objectives when performing its functions which may include as well implicit manipulative or deceptive objectives even if the system has not been explicitly programmed in this way.
inaudible auditory signals or to exploit emotional dependency or specific vulnerabilities of  users  in  advertisements.  These  techniques  are  deployed  'with  the  objective  to' materially distort users' behaviour, since they are objectively a design feature that aims to  influence  consumers'  purchasing  decisions  without  their  conscious  awareness,  to push people to take significantly harmful financial decisions.
Deploying AI systems to impersonate other persons could also be considered as an AI system deployed 'with the objective' to deceive and materially distort the behaviour of persons if the person is effectively deceived, thus substantially affecting their ability to make informed decisions about the identity of the person.
If  in  both  examples  the  other  conditions  in  Article  5(1)(a)  AI  Act  are  fulfilled,  in particular  regarding  the  significant  harm,  those  systems  are  likely  to  fall  within  the scope of the prohibition, but this will require a case-by-case assessment.
## c) Scenario 2: Prohibited AI systems 'with the effect of' materially distorting behaviour
- (83) The intent of the provider or deployer to materially distort the behaviour of a person or group of persons is a sufficient, but not a necessary, condition for the prohibition in Article 5(1)(a) AI Act to apply. That prohibition also applies where no such intent is present,  but  the  effect  of  the  technique(s)  deployed  by  an  AI  system  is  capable  of materially distorting the behaviour of a person or a group of persons to a point that undermines their individual autonomy and free choices.
- (84) A  plausible/reasonably  likely causal link between  the  subliminal, purposefully manipulative or deceptive technique deployed by the AI system and its effects on the behaviour is, however, always necessary for the prohibition to apply. In consistency with consumer protection law, these effects do not need to have fully materialised, but there must be sufficient indication that they are likely or capable to materialise and undermine individual autonomy based on an objective assessment of all circumstances of  the  case  and  existing  scientific  knowledge  and  methods,  as  well  as  available information about the impact of the system on the individuals' behaviour in real life. In this context, the fact that a system is capable of triggering behaviours that appreciably impair  individuals'  ability  to  make  an  informed  decision  and  undermine  their  free choices suffice to fulfil that condition and is not dependent on considerations relating to the 'timing' for the harm to materialize (e.g. in case of addition-like behaviours) as long as it is reasonably likely to occur.
For example, an AI-powered well-being chatbot is intended by the provider to support and  steer  users  in  maintaining  a  healthy  lifestyle  and  provide  tailored  advice  for psychological  and  physical  exercises.  However,  if  the  chatbot  exploits  individual vulnerabilities  to  adopt  unhealthy  habits  or  to  engage  in  dangerous  activities  (e.g. engage in excessive sports without rest or drinking water) where it can reasonably be expected that certain users will follow that advice, which they would otherwise not have
done, and suffer significant harm (e.g. a heart attack, or other serious health problem), that AI system would fall under the prohibition in Article 5(1)(a) AI Act, even if the provider  might  not  have  intended  this  behaviour  and  harmful  consequences  for  the persons.
The mere fact that the chatbot is capable of appreciably impairing individual autonomy and materially distorting the behaviour of certain users in a significantly harmful way and that the provider has not taken appropriate preventive and mitigating measures to avoid those significantly harmful effects suffices for the prohibition to apply (see more for relevant considerations of the reasonably likeliness of the harm in section . and out of scope section .).
## . (Reasonably likely to) cause significant harm
- (85) Finally,  for  the  prohibition  in  Article  5(1)(a)  AI  Act  to  apply,  the  distortion  of  the behaviour of a person or group of persons must cause or be reasonably likely to cause that  person,  another  person,  or  group  of  persons  significant  harm.  In  this  context, important  concepts  that  require  clarification  are  the  types  of  harms  covered  by  the prohibition, the threshold of significance of the harm, and its reasonable likelihood and causal  link  between  the  harm  and  the  manipulative  or  deceptive  technique  and  the person's behaviour.
## a) Types of harms
- (86) The AI Act addresses various types of harmful effects associated with manipulative and deceptive AI systems, each with distinct implications for individual persons and groups of persons that may be affected 74 . The main types of harms relevant for Article 5(1)(a) AI Act include physical, psychological, financial, and economic harms 75 that may be compound with broader societal harms in certain cases 76 .
- (87) Physical harm encompasses any injury or damage to a person's life, health and material damage to property. Physical harm to a person's life and health have, in many cases, immediate, serious, and irreversible consequences. In line with its product safety logic, the  AI  Act  aims  to  prohibit  AI-enabled  manipulation  and  deception  resulting  in significant physical harm.
For example, an AI chatbot promotes self-harm to users or incentivises them to commit suicide or harm other persons or groups of persons by promoting terrorist content or incentivising violence against certain persons or groups of persons (i.e., minorities).
- (88) Psychological  harm  is  particularly  relevant  in  the  context  of  AI  systems  deploying manipulative  techniques  that  exploit  cognitive  and  emotional  vulnerabilities  and
74 See Article 5(1)(a) AI Act.
75 Recital 29 AI Act.
76 See Recital 28 AI Act, which explains the prohibitions can also cause broader societal harms and contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and fundamental rights enshrined in the Charter. See also Article 1 AI Act that aims to protect democracy and the rule of law as EU values.
influence  an  individual's  behaviours  in  ways  that  can  cause  significant  harm. Psychological  harms  encompass  adverse  effects  on  a  person's  mental  health  and psychological  and  emotional  well-being.  Such  harms  are  particularly  significant because they may accumulate over time and may not be immediately apparent, but may produce long-lasting and severe consequences. However, it is more difficult to measure them,  which  requires  a  case-by-case  assessment,  in  particular  to  determine  their severity, taking into account all relevant circumstances of the case.
For  example,  an  AI  companionship  application  designed  to  emulate  human  speech patterns, behaviours and emotions uses anthropomorphic features and emotional cues to influence users' feelings, dispositions, and opinions, making those users emotionally dependent on the service incentivising addiction-like behaviour and potentially causing significant harms, such as suicidal behaviours and risks of harming other persons. 77
- (89) Financial  and  economic  harm may encompass a range of adverse effects, including financial loss, financial exclusion, economic instability.
For example, a chatbot that offers fraudulent products that cause significant financial harms.
- (90) In assessing the harms caused by AI systems when applying Article 5(1)(a) AI Act, it is important to highlight that the harms are often not isolated, but manifest themselves in combination, leading to compounded and multifaceted negative impacts. Understanding the combination of harm is crucial to effectively assess their significance (see also see section . b) below), whereby physical, psychological, financial and economic harm may be combined and exacerbate the overall impact on individuals and communities and may even have broader adverse impacts.
## For example,
- - An AI system that causes physical harm may also lead to psychological trauma, stress, and anxiety and vice versus. For example, addictive design of AI systems used in products  and  other  AI-enabled  applications  may  lead  to  psychological  harm  by fostering addictive behaviours, anxiety, and depression. The psychological distress may subsequently result in physical harm, such as insomnia and other stress-related health issues and physical problems.
- -AI-driven  harassment  may  lead  to  both  psychological  distress and  physical manifestations of stress, such as insomnia, deteriorated physical health or weakened immune system.
77   Renwen Zhang, Han Li, Han Meng, Jinyuan Zhan, Hongyuan Gan, and Yi-Chieh Lee. 2024. The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships. 1, 1 (November 2024), 28 pages.
- -  Psychological  harm resulting  from  the  use  of  AI  may  also  lead  to  physical  harm, including death.  For example, AI systems used  online  may facilitate gender-based violence through harassment, stalking, cyberbullying and sexual extortion.
- - Individual psychological harms for example due to AI-enabled generation of 'deep fakes' impersonating actual persons to deceive and undermine the decision-making, individual  autonomy  and  free  choices  of  individuals  can  also  be  combined  with significant harms for groups of persons (e.g., sharing the same ethnic or racial origin or gender as the victims depicted on the deep fakes).
## b) Threshold for significance of the harm
- (91) The  prohibition  in  Article  5(1)(a)  AI  Act  only  applies  if  the  harm  caused  by  the subliminal, manipulative and deceptive techniques is ' significant '. The AI Act does not provide a definition for the concept of 'significant harm', but it should be understood as implying significant adverse impacts on physical, psychological health or financial and  economic  interests of  persons  and  groups  of  persons 78 .  The  determination  of 'significant  harm'  is  fact  -specific,  requiring  careful  consideration  of  each  case's individual  circumstances  and  a  case-by-case  assessment,  but  the  individual  effects should be always material and significant in each case.
- (92) In other Union laws, the concept of ' significant harm ' is  also used as a nuanced and context-dependent  concept  guided  by  high-level  protection  and  preventive  action goals. 79 By analogy, the following key considerations can be derived and could be taken into account when assessing what constitutes significant harm within the meaning of Article 5(1)(a) AI Act:
- ¬∑ The severity of harm refers to the degree of harm that has resulted or is reasonably likely to result from using an AI system with objective and observable effects for the significant harms. It is particularly important to consider in this context the AI system's  interdependencies,  the  combination  of  various  types  of  harms,  and  the adverse effects on individuals or groups of persons.
- ¬∑ Context  and  cumulative  effects 80 : The  specific  context,  including  the  existing state,  and  the  cumulative  effects  of  multiple  actions,  play  an  important  role  in assessing the severity of the harm.
- ¬∑ Scale and intensity: The extent of the harm and the intensity of adverse effects are critical in evaluating whether the harm is significant. Whether the harm impacts a large number of people is also relevant for assessing its significance.
- ¬∑ Affected persons' vulnerability: Certain groups, such as children, the elderly, or persons with disabilities may be more susceptible to harm from specific AI systems.
78 Recital 29 AI Act.
79 See  judgments  of  the  Court  of  Justice  of  7  September  2004, Waddenvereniging  and  Vogelbeschermingsvereniging ,  C-127/02, EU:C:2004:482 and of 11 April 2013, Sweetman and Others , C-258/11, EU:C:2013:220.
80 See Recital 29 AI Act.
What  may  be  considered  less  significant  harm  for  persons  in  general  might  be considered  significant  and  unacceptable  for  such  vulnerable  groups,  especially children.
- ¬∑ Duration and reversibility: Long-lasting or irreversible harm typically meets the threshold for significant harm. Short-term and reversible effects might  be considered less significant, unless they occur frequently.
- (93) The objective of the AI Act to ensure 'a high level of protection', in conjunction with Article 191(2) TFEU, suggests a comprehensive approach to protection when assessing the significance of the harm. This means considering both immediate and direct harms and systemic, indirect adverse impacts associated with AI  systems  deploying subliminal, purposefully manipulative or deceptive techniques that are intended to or capable of impairing individual autonomy, decision-making and free choices of persons and groups of persons.
For example, significant physical harm that is reasonably likely to be caused by an AI system includes injuries or fatalities or  a sufficiently serious impact on individuals' health or the destruction of property. AI systems that suggest to an individual to commit criminal acts such as sexual abuse and exploitation, extreme violent or terrorist content or incentivise individuals to commit crimes, self-harm or harm to other persons should be considered to reach such a threshold.
By contrast, minor physical harms may include less severe injuries, such as bruises or temporary discomfort, which do not have significant or lasting consequences and will therefore not reach the threshold of significance within the meaning of Article 5(1)(a) AI Act. Whether the physical harm specifically concerns vulnerable groups, such as children,  should  be  assessed,  as  should  the  scale  of  the  harm  and  whether  it  is compounded with other types of harms, such as psychological, financial etc. This will require  a  case-by-case  assessment,  taking  into  account  the  circumstances  and  the criteria presented above to guide that assessment.
There are numerous cases where the threshold of significant harm will likely not be reached even if the systems may be deploying subliminal, purposefully manipulative, or deceptive techniques (see for examples section . below).
## c) Causal link and threshold for reasonable likelihood of the harm
- (94) The  concept  of  'reasonably  likely'  is  used  in  Article  5(1)(a)  AI  Act  to  determine whether there is a plausible/reasonably likely causal link between the manipulative or deceptive  technique  capable  of  distorting  the  person's  behaviour  in  a  manner  that undermines their free choices and the potential significant harm. This concept allows the application of the prohibition not only in cases where the harm has occurred, but also where it is reasonably likely to occur in line with the safety logic of the AI Act. In this context, it is particularly relevant to assess whether the provider or deployer of the
AI system could have reasonably foreseen the significant harm that is reasonably likely to result from the subliminal, purposefully manipulative or the deceptive techniques deployed  and  whether  they  implemented  appropriate  preventive  and  mitigating measures  to  avoid  or  mitigate  the  risk  of  such  significant  harms.  This  implies  a judgement  of  reasonableness  on  an  objective  basis  and  according  to  universally accepted criteria (e.g. technical and scientific), including a criterion of rationality in establishing plausible causality between the AI practice and the significant harm that may arise. The opacity or transparency of the AI system and its functioning may affect the conclusion regarding this causal link and, hence, the application of the prohibition.
- (95) To avoid providing or using AI systems that are likely to be prohibited, providers and deployers  of  AI  systems  that  deploy  such  manipulative  or  deceptive  techniques  are encouraged to take appropriate measures such as:
- 1. Transparency  and  individual  autonomy: provide  transparency  in  how  the  AI system operates, clear disclosures about its capabilities and limitations, and relevant information to ensure informed decisions; respect individual autonomy and avoid engaging in exploitative or deceptive practices that are likely to appreciably impair an individual's autonomy, decision-making and free choices in potentially harmful ways; integrate appropriate user control and safeguard measures to ensure that the system is not deceptive and operates within the boundaries of lawful persuasion that is outside the scope of the prohibition (see section ).
- 2. Compliance with relevant applicable legislation: in many cases compliance with relevant applicable legislation will mitigate the risks of harm and indicate that the practice does not constitute a purposefully manipulative or deceptive practice and that mitigating measures have been put in place to prevent likely significant harms (see section . and .).
- 3. State of the art practices and industry standards: adherence to professional due diligence practices and industry standards for the responsible development and use of safe and ethical AI systems and measures to mitigate the harms can help to preempt and mitigate unintended significant harms.
- (96) By contrast, harms and distortion of the behaviour of individuals which result from factors external to the AI system and which are not within the control and reasonably foreseeable by the provider or the deployer to pre-empt and mitigate risks would not be relevant for the assessment whether there is a plausible causal/reasonably likely link between  the  distorted  behaviour  of  the  persons  interacting  with  the  system  and  the significant harm 81 .
For example, the provider of an AI system may assess and try to mitigate potential harmful  manipulative  effects  in  the  design  of  the  system  and  the  interactions  with humans through the design, prior testing, and other proportionate mitigating measures,
81 See Recital 29 AI Act.
but it may not be in a position to foresee if a person may get depressed or change their behaviour due to other external factors in their personal life that are not known and beyond their interactions with the system.
- (97) Other  examples  that  fall  outside  the  scope  of  the  prohibition  as  not  fulfilling  all conditions (e.g. in case of lawful persuasion) are provided in section . below.
## . Main components of the prohibition in Article 5(1)(b) AI Act - harmful exploitation of vulnerabilities
## Article 5(1)(b) AI Act provides:
- 1. The following AI practices shall be prohibited:
- (b) the placing on the market, the putting into service or the use of an AI system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm;
- (98) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(b) AI Act to apply:
- (i) The practice must constitute the 'placing on the market', the 'putting into service', or the 'use' of an AI system.
- (ii) The AI system must exploit vulnerabilities due to age, disability, or socio-economic situations.
- (iii)The exploitation enabled by the AI system must have the objective, or the effect of materially distorting the behaviour of a person or a group of persons.
- (iv) The distorted behaviour must cause or be reasonably likely to cause significant harm to that person, another person, or a group of persons.
- (99) For the prohibition to apply all four conditions must be simultaneously fulfilled and there must be a plausible causal link between the exploitation, the material distortion of the behaviour of the person, and the significant harm that has resulted, or is reasonably likely to result, from that behaviour.
- (100) The first condition, i.e. the 'placing on the market', the 'putting into service' or the 'use' of an AI system, has been already analysed in section ., while the third and fourth conditions have been examined in sections . and . in relation to the prohibition in  Article  5(1)(a)  AI  Act.  The  next  sections  will  focus  on  the  additional  specific conditions listed above, i.e. those that relate to the exploitation of the vulnerabilities and the specific harm.
- . Exploitation  of  vulnerabilities  due  to  age,  disability,  or  a specific socio-economic situation
- (101) To fall within the scope of the prohibition in Article 5(1)(b) AI Act, the AI system must exploit vulnerabilities inherent to certain individuals or groups of persons due to their age,  disability  or  a  specific  socio-economic  situation,  making  them  particularly susceptible to manipulative and exploitative practices.
- (102) The  AI  Act  does  not  define  the  concept  of ' vulnerabilities ' .  That  concept  may  be understood  to  encompass  a  broad  spectrum  of  categories,  including  cognitive, emotional, physical, and other forms of susceptibility that can affect the ability of an individual or a group of persons to make informed decisions or otherwise influence their behaviour. While Article 5(1)(b) AI Act refers to 'any' vulnerability, it limits the relevant persons covered by the prohibition to those defined by their age, disability, or socio-economic situations, who in principle have more limited capacity to recognise or resist  the  AI  manipulative  or  exploitative  practices  and  are  in  need  of  enhanced protection. 82 It  follows  from  the  wording  of  Article  5(1)(b)  AI  Act  that  this susceptibility must be the result of the person belonging to one of the groups ('due to').
- (103) 'Exploitation' should be understood as objectively making use of such vulnerabilities in a manner that is harmful for the exploited (groups of) persons or other persons and should  be  clearly  distinguished  from  lawful  practices  that  are  not  affected  by  the prohibition  (see  section    out  of  scope).  Exploitation  of  the  vulnerabilities  of persons  belonging  to  those  clearly  defined  groups  may  be  cumulative  (reference  to 'any') which in combination may also constitute an aggravating factor that is likely to increase  the  harm.  Exploitation  of  vulnerabilities  of  persons  and  groups  of  persons belonging to vulnerable groups other than those defined by age, disability or specific socio-economic situation are outside the scope of Article 5(1)(b) AI Act.
## a) Age
(104) Age is a primary vulnerability category covered by the prohibition in Article 5(1)(b) AI Act,  including  both  young  and  older  people.  That  prohibition  aims  to  prevent  AI systems from exploiting cognitive and other limitations that children and older people may  have,  and  to  protect  them  from  harmful  undue  influence,  manipulation  and exploitation.  This  aligns  with  the  objectives  of  the  AI  Act 83 and  other  Union  and national legal frameworks and policies aimed at ensuring child safety 84 .
- (105) Children 85 that is, persons below the age of 18 years, are particularly susceptible to manipulation due to their developmental stage, which limits their ability to assess and understand  what  is  real  and  the  intentions  behind  AI-driven  interactions  critically. Children, due to their cognitive and socio-emotional immaturity, are also particularly
82 See in particular Articles 24, 25 and 26 of the Charter. See also United Nations Educational, Scientific and Cultural Organization (UNESCO) Recommendation on the Ethics of Artificial Intelligence (2021) which emphasises inclusivity and fairness in AI development and deployment. It calls for special attention to vulnerable groups, including children, older people, and people with disabilities.
83 Recital 48 AI Act highlights that children have specific rights as enshrined in Article 24 of the Charter and in the United Nations Convention on the Rights of the Child, further developed in the UNCRC General Comment No 25 as regards the digital environment, both of which require consideration of the children's vulnerabilities and provision of such protection and care as necessary for their well-being.
84 See, the new European strategy for a better internet for kids (BIK+), COM/2022/212 final.
85 Union law generally considers a child to be any person under 18, aligning with the United Nations Convention on the Rights of the Child (UNCRC).
vulnerable to forming attachments to AI agents and applications, and are therefore more susceptible to manipulation, exploitation, and addictive behaviour.
## For example,
- -An  AI-powered  toy  designed  to  interact  with  children  keeps  them  interested  in interactions  with  the  toy  by  encouraging  them  to  complete  increasingly  risky challenges,  such  as  climbing  furniture,  exploring  high  shelves,  or  handling  sharp objects,  in  exchange  for  digital  rewards  and  virtual  praise,  pushing  them  towards dangerous behaviours that are likely to cause them significant physical harm. Such a system exploits children's vulnerabilities by abusing their natural curiosity and desire for rewards.
- -  A game uses AI to analyse children's individual behaviour and preferences on the basis of which it creates personalised and unpredictable rewards through addictive reinforcement schedules and dopamine-like loops to encourage excessive play and compulsive  usage.  The  game  is  designed  to  be  highly  addictive,  exploiting  the vulnerabilities inherent to children, including their limited ability to understand longterm  consequences,  susceptibility  to  pressure,  lack  of  self-control,  and  inclination towards instant gratification. The consequences of this AI-enabled exploitation can be severe and long-lasting for children, including potentially additive behaviour, physical health problems due to lack of exercise and sleep, deteriorated eyesight, problems with concentration  and  reduced  cognitive  capacities,  poor  academic  performance,  and social difficulties. It can significantly impact a child's development and well-being, with potential longer-term consequences that may also extend into adulthood.
In  both  examples,  the  prohibition  in  Article  5(1)(b)  AI  Act  targets  only  such exploitation  and  addiction-like  practices  that  seriously  harm  children  and  not  AIenabled toys, games, learning applications or other digital applications in general that can  bring  benefits  and  are  not  affected  if  they  do  not  fulfil  all  conditions  for  that prohibition. See also section . out of scope.
- (106) Likewise, older people 86 might suffer from reduced cognitive capacities (even if not suffering  from  dementia)  and  might  struggle  with  the  complexities  of  modern  AI technologies, making them in those cases vulnerable to scams or coercive tactics.
## For example,
- - AI systems used to target older people with deceptive personalised offers or scams, exploiting their reduced cognitive capacity aiming to influence them to make decisions they would not have taken otherwise that are likely to cause them significant financial harm.
- - A robot aimed to assist older persons may exploit their vulnerable situation and force them to do certain activities against their free choice, which can significantly worsen their mental health and cause them serious psychological harms.
In both examples, the prohibition in Article 5(1)(b) AI Act targets only such exploitative practices that are likely to seriously harm older persons and not AI-enabled personal assistants, health applications and assistive robots in general that can bring benefits and are not affected if they do not fulfil all conditions for that prohibition. See also section . out of scope.
## b) Disability
(107) The second category of vulnerabilities which the prohibition in Article 5(1)(b) AI Act aims to protect are those due to disability. The objective is to prevent AI systems from exploiting cognitive and other limitations and weaknesses in persons with disabilities and to protect them from harmful undue influence, manipulation, and exploitation.
(108) Disability 87 encompasses a wide range of long-term physical, mental, intellectual, and sensory impairments which in interaction with other barriers hinder full and effective participation of individuals in the society on an equal basis with others. AI systems that exploit such vulnerabilities may be particularly harmful for persons with disabilities which can be more easily influenced or exploited due to their impairment compared to other persons.
## For example,
- - A therapeutic chatbot aimed to provide mental health support and coping strategies to persons  with  mental  disabilities  can  exploit  their  limited  intellectual  capacities  to influence them to buy expensive medical products or nudge them to behave in ways that are harmful to them or other persons.
- - AI systems can identify women and young girls with disabilities online with sexually abusive  content  and  targets  them  with  more  effective  grooming  practices,  thus exploiting their impairments and vulnerabilities that make them more susceptible to manipulation and abuse and less capable of protecting themselves.
By contrast, AI applications that are not designed in an accessible manner should not be  regarded  to  exploit  vulnerabilities  of  persons  with  disabilities  since  they  do  not specifically target those vulnerabilities, but are simply inaccessible to the persons with disabilities.
87 Recital 29 AIA explains that 'disability' should be understood within the meaning of Article 3(1) Directive (EU) 2019/882 of the European Parliament and of the Council of 17 April 2019 on the accessibility requirements for  products and services (Text with EEA relevance), PE/81/2018/REV/1, OJ L 151, , p. 70-115.
## c) Specific socio-economic situation
(109) The third category of vulnerabilities which the prohibition in Article 5(1)(b) AI Act seeks to protect are those due to a specific socio-economic situation that is likely to make the persons concerned more vulnerable to exploitation. 'Specific' should not be interpreted in this context as a unique individual characteristic, but rather a legal status or membership to a specific vulnerable social or economic group. Recital 29 AI Act contains a non-exhaustive list of examples of such situations, such as persons living in extreme  poverty  and  ethnic  or  religious  minorities.  The  category  aims  to  cover,  in principle, relatively stable and long-term characteristics, but transient circumstances, such as temporary unemployment, over-indebtedness or migration status, may also be covered as a specific socio-economic situation. However, situations such as grievances or loneliness that may be experienced by any person are not covered, since they are not specific from a socio-economic perspective (their exploitation may be covered though under Article 5(1)(a) AI Act).
(110) Persons in disadvantaged socio-economic situations are usually more vulnerable and have  fewer  resources  and  lower  digital  literacy  than  the  general  population,  which makes it  harder  for  them  to  discern  or  counteract  exploitative  AI  practices.  Article 5(1)(b)  AI  Act  aims  to  ensure  that  AI  technologies  do  not  perpetuate  or  exacerbate existing  financial  and  other  social  inequalities  and  injustices  by  exploiting  the vulnerabilities of those people.
For example, an AI-predictive algorithm can be used to target with advertisements for predatory financial products people who live in low-income post-codes and are in a dire financial situation, thus exploiting their susceptibility to such advertisements because of possible despair and causing them significant financial harm.
By contrast, an AI system that is inadvertently biased and disproportionately impacts socio-disadvantaged persons (indirect discrimination) due to biased training data should not  automatically  be  considered  to  exploit  persons'  socio-economic  vulnerabilities, since they are not specifically targeted as in the case of direct discrimination when such targeting is a deliberate feature of the system's design of the algorithm or when such discriminatory impact is due to targeting other proxy characteristics (e.g. postal codes) that closely correlate with the protected  characteristics. At the same time, providers or deployers  of  AI  systems  that  are  aware  that  their  systems  unlawfully  discriminate against persons or groups of persons in specific socio-economic situation should also be considered to exploit their vulnerabilities, if they are aware of the reasonably likely significant  harm  that  they  are  likely  to  suffer  and  they  have  not  taken  appropriate corrective measures (see section . c) above).
(111) In  the  context  of  a  specific  socio-economic  situation,  it  is  crucial  to  consider  the relevance of proxies linked to grounds of discrimination protected under Union equality law, such as racial origin, ethnicity, nationality or religion.
For example, socio-economic status and ethnic origin might intersect, meaning that AI systems exploiting socio-economic data might disproportionately affect ethnic minorities or persons from specific racial origin. This can exacerbate existing disparities and contribute to systemic discrimination or even exclusion of individuals from these groups.
However, Article 5(1)(b) AI Act does not apply to AI systems that target consumers based on a wide ranging variables that do not tangentially correlate with vulnerable groups  in  specific  socio-economic  situations,  such  as  what  brand  and  model  of telephone a person has, in how big city they live, how much and where they travel etc. Even  if  these  characteristics  may  reflect  socio-economic  situation  of  individuals  in general, they are not determinative of individuals in a specific socio-economic situation, whose vulnerabilities the prohibition aims to safeguard against exploitation.
(112) Other people in unique social contexts may be, for instance, migrants or refugees, who often  lack  stable  legal  status  and  socio-economic  stability  and  may  be  particularly susceptible to exploitation by AI systems.
For example, a chatbot is intended to interact in a personalised manner with users, some of  whom  happen  to  be  migrants.  The  chatbot  identifies  and  makes  use  of  the vulnerabilities  and  discontent  of  migrants,  who  are  in  principle  in  a  vulnerable  and instable specific socio-economic situation, and pushes them towards extremist views in response to their queries, including violence against (certain groups of) the population in the country.
## . With  the  objective  or  the  effect  of  materially  distorting behaviour
(113) The third condition for the prohibition in Article 5(1)(b) AI Act to apply is that the exploitation of the vulnerabilities examined above must have either a) 'the objective' or b) 'the effect of materially distorting the behaviour of a person or a group of persons'. This  implies  a  substantial  impact,  rather  than  a  minor  or  trivial  one,  but  does  not necessarily require intent, since Article 5(1)(b) AI Act covers practices that may only have the ' effect' of causing material distortions. Article 5(1)(a) and (b) AI Act make use of the same concepts and should therefore be interpreted in the same way. The explanations provided in section . are therefore equally relevant for Article 5(1)(b) AI Act. The only noteworthy difference is the need in Article 5(1)(a) AI Act for the exploitative practice to 'appreciably impair the ability to make an informed decision', which  is  not  present  in  Article  5(1)(b)  AI  Act,  since  the  specific  vulnerabilities  of children  and  other  vulnerable  persons  reduce  their  capacity  to  make  such  informed decisions and force them into adopting behaviour against which they cannot protect themselves as other adults might do.
## . (Reasonably likely to) cause significant harm
- (114) Finally,  for  the  prohibition  in  Article  5(1)(b)  AI  Act  to  apply,  the  distortion  of  the behaviour of the vulnerable person or group of persons must cause or be reasonably likely to cause that person or another person significant harm. Article 5(1)(a) and (b) AI Act make use of the same concepts and should therefore be interpreted in the same way. The explanations provided in section . in relation to the types of harms, the threshold for significance of the harm, and the causal link and its reasonableness are thus equally relevant for the interpretation of Article 5(1)(b) AI Act.
- (115) As explained  in  section  .,  significant  harm  encompasses  a  range  of  significant adverse impacts, including physical, psychological, financial, and economic harms that must be reasonably likely to occur for the prohibition in Article 5(1)(b) AI Act to apply. For vulnerable groups - children, older persons, persons with disabilities, and socioeconomically disadvantaged populations - these harms may be particularly severe and multifaceted  due  to  their  heightened  susceptibility  to  exploitation.  What  may  be considered an acceptable risk of harm for adults often represents an unacceptable harm for children and these other vulnerable groups. A precautionary approach is therefore particularly warranted in case of uncertainty and potential for significant harms.
- (116) For instance, children are  highly impressionable and may not possess the cognitive maturity to evaluate persuasive content critically or resist certain exploitative practices that  aim  to  keep  them  dependent  on  the  AI-enabled  services.  This  could,  in  turn, contribute  to  the  shaping  of  their  values,  beliefs  and  steer  behaviours  in  potentially harmful  ways.  The  significant harm  here  is both physical and  psychological, exacerbated by the children's inability to discern and resist the exploitation and the harmful effects to their development and well-being that may have a long-term impact.
## For instance,
- - AI systems used for the generation of child sexual abuse material (or manipulating existing material depicting real children to create further novel content featuring them) and the development of strategies for grooming and sexually extorting children are likely to cause serious harms and abuses of the affected children and often result in long-term physical, psychological and social consequences for survivors 88 .
- - AI systems may target the vulnerabilities of young users and use addictive reinforced schedules with the objective to keep them dependent on the service are particularly harmful  for  young  persons  and  girls.  They  may  cause  serious  psychological  and physical  harms,  including  anxiety  and  depression,  body  dissatisfaction,  eating disorders and mental health problems, including in some  cases self-harm and suicidal behaviour. 89 This may also have long term harmful consequences for child
88 Commission Staff Working Document: Impact Assessment Report accompanying the document Proposal for a Directive of the European Parliament and the Council on combating child sexual abuse and sexual exploitation and child sexual abuse material, SWD/2024/33 final. See also  for  statistics  Internet  Watch  Foundation  2024  report  which  contains  detailed  statistics  on  AI  generated  CSAM,  available  at: https://www.iwf.org.uk/about-us/why-we-exist/our-research/how-ai-is-being-abused-to-create-child-sexual-abuse-imagery,
89 Elizabeth J. et al, A meta-analysis of the association between adolescent social media use and depressive symptoms, Journal of Affective Disorders, Volume 275, 1 October 2020, Pages 165-174.
development, including  impaired  cognitive  development  and  learning  and  reduced social skills and displacement of experiences such as physical play, sleep, and faceto-face social interactions that are essential for the emotional and physical well-being of the child 90 .
- - An AI system that is designed in an anthropomorphic way and simulates human-like emotional responses in its interactions with children can exploit children's vulnerabilities in a manner that fosters unhealthy emotional attachment, manipulates engagement time and distorts children's understanding of authentic human relationships. This may hamper their normal social and emotional development and relations with other human beings and socio-emotional skills like empathy, emotional regulation, and social understanding and adaptability 91 . As a result, this may lead to psychological harms such as increased anxiety and dependency of the children on the service and longer-term harms to the child's well-being.
Such deliberate addictive and exploitative design features of the AI-enabled services that can lead to combined  significant harms, as described above, should be distinguished from other legitimate behaviour of the providers and deployers to pursue user engagement that respects individual autonomy and the safety of children and do not lead to significant harms which are outside the scope of Article 5(1)(b) AI Act (see section out of scope ).
- (117) Similarly, older people may face cognitive decline and reduced digital literacy, making them prime targets for AI-driven scams or manipulative marketing. The harm in this case is often financial and psychological, compounded by the frustration and isolation many older people experience, which may be exploited to amplify the manipulative impact.
For instance, an AI system that exploits the reduced cognitive vulnerabilities of older people by particularly targeting expensive medical treatments, unnecessary insurance policies or deceptive investments schemes to older persons may lead to significant loss of savings, increased debt, and emotional distress for older people.
Certain AI-enabled differential pricing practices in key services such as insurance that exploit the specific socio-economic situation and provide higher prices to lower income consumers can lead to a significant financial burden to pay more for the same coverage, leaving them vulnerable to shocks. 92
90 Siebers,  T.,  Beyens,  I.,  Pouwels,  J.  L.  