- 2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include  concise,  complete,  correct  and  clear  information  that  is  relevant,  accessible  and  comprehensible  to  deployers.
- 3. The instructions  for  use  shall  contain  at  least  the  following  information:
- (a) the  identity  and  the  contact  details  of  the  provider  and,  where  applicable,  of  its  authorised  representative;
- (b) the  characteristics,  capabilities  and  limitations  of  performance  of  the  high-risk  AI  system,  including:
- (i) its  intended  purpose;
- (ii) the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk  AI  system  has  been  tested  and  validated  and  which  can  be  expected,  and  any  known  and  foreseeable circumstances that  may  have  an  impact  on  that expected  level  of  accuracy,  robustness  and  cybersecurity;
- (iii) any  known  or  foreseeable  circumstance,  related  to  the  use  of  the  high-risk  AI  system  in  accordance  with  its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety  or  fundamental  rights  referred  to  in  Article  9(2);
- (iv) where applicable,  the  technical  capabilities  and  characteristics  of  the  high-risk  AI  system  to  provide  information that  is  relevant  to  explain  its  output;
- (v) when  appropriate,  its  performance  regarding  specific  persons  or  groups  of  persons  on  which  the  system  is intended  to  be  used;
- (vi) when  appropriate,  specifications  for  the  input  data,  or  any  other  relevant  information  in  terms  of  the  training, validation  and  testing  data  sets  used,  taking  into  account  the  intended  purpose  of  the  high-risk  AI  system;
- (vii) where  applicable,  information  to  enable  deployers  to  interpret  the  output  of  the  high-risk  AI  system  and  use  it appropriately;
- (c) the  changes  to  the  high-risk  AI  system  and  its  performance  which  have  been  pre-determined  by  the  provider  at  the moment of the initial  conformity  assessment,  if  any;
- (d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation  of  the  outputs  of  the  high-risk  AI  systems  by  the  deployers;
- (e) the computational and hardware resources needed, the expected lifetime of the high-risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as  regards  software  updates;
- (f) where  relevant,  a  description  of  the  mechanisms  included  within  the  high-risk  AI  system  that  allows  deployers  to properly collect,  store  and  interpret  the  logs  in  accordance  with  Article  12.
## Article  14
## Human oversight
- 1. High-risk  AI  systems  shall  be  designed  and  developed  in  such  a  way,  including  with  appropriate  human-machine interface  tools,  that  they can  be  effectively  overseen  by  natural  persons  during  the  period  in  which  they  are  in  use.
- 2. Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse,  in  particular  where  such  risks  persist  despite  the  application  of other  requirements  set  out  in  this  Section.
- 3. The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI  system,  and  shall  be  ensured  through  either  one  or  both  of  the  following  types  of  measures:
- (a) measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the  market or  put  into  service;
- (b) measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that  are  appropriate  to  be  implemented  by  the  deployer.
- 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be provided to the deployer in such  a  way  that  natural  persons  to whom  human  oversight  is  assigned  are  enabled,  as  appropriate  and  proportionate:
- (a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation,  including  in  view  of  detecting and  addressing  anomalies,  dysfunctions  and  unexpected  performance;
- (b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for decisions  to  be  taken  by  natural  persons;
- (c) to  correctly  interpret  the  high-risk  AI  system's  output,  taking  into  account,  for  example,  the  interpretation  tools  and methods available;
- (d) to decide, in any particular situation, not to use the high-risk AI system or  to otherwise disregard, override or reverse the  output  of  the  high-risk  AI  system;
- (e) to  intervene  in  the  operation  of  the  high-risk  AI  system  or  interrupt  the  system  through  a  'stop'  button  or  a  similar procedure that allows the  system  to come  to a  halt  in  a  safe  state.
- 5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting  from  the  system  unless  that  identification  has  been  separately  verified  and  confirmed  by  at  least  two  natural persons with the  necessary competence, training and  authority.
The requirement for a separate verification by at least two natural persons shall not apply to high-risk AI systems used for the  purposes  of  law  enforcement,  migration,  border  control  or  asylum,  where  Union  or  national  law  considers  the application  of  this  requirement  to  be  disproportionate.
## Article  15
## Accuracy, robustness and cybersecurity
- 1. High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness,  and  cybersecurity,  and  that  they  perform  consistently  in  those  respects  throughout  their  lifecycle.
- 2. To  address  the  technical  aspects  of  how  to  measure  the  appropriate  levels  of  accuracy  and  robustness  set  out  in paragraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholders and  organisations  such  as  metrology  and  benchmarking  authorities,  encourage,  as  appropriate,  the  development  of benchmarks and measurement methodologies.
- 3. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions  of  use.
- 4. High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other  systems.  Technical  and  organisational  measures  shall  be  taken  in  this  regard.
The  robustness  of  high-risk  AI  systems  may  be  achieved  through  technical  redundancy  solutions,  which  may  include backup or  fail-safe  plans.
