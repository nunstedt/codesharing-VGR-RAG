This could include behaviour of individuals and  groups  of  individuals  in social and  private contexts, such  as participation in cultural events, volunteering, etc., but also social behaviour in business contexts, for example the payment of debts, behaviour when using certain services, as well as relations with public and private entities, government, police, and the law (for example, whether a person obeys traffic rules). Social behaviour data from multiple contexts and data points may be collected in a centralised way by the same entity, but is most often collected in a distributed way and combined from different sources, which may  involve increased monitoring and the tracking of individuals (so called 'dataveillance').
- (158) The  second  scenario  is  where  the  scoring  is  based  on personal  or  personality characteristics ,  which  may  or  may  not  involve  specific  social  behavioural  aspects. 'Personal characteristics' may include a variety of information relating to a person, for example sex, sexual orientation or sexual characteristics, gender, gender identity, race, ethnicity, family situation, address, income, household members, profession, employment or other legal status, performance at work, economic situation, financial liquidity,  health,  personal  preferences,  interests,  reliability,  behaviour,  location  or movement, level of debt, type of car etc. 117 'Personality characteristics' should be in principle interpreted as synonymous with personal characteristics, but may also imply the creation of specific profiles of individuals as personalities. Personality characteristics may be also based on a number of factors and imply a judgement, which may be made by the individuals themselves, other persons, or generated by AI systems. In the AI Act, personality characteristics are sometimes referred to as personality traits and characteristics; 118 those concepts should be interpreted consistently.
- (159) 'Known, inferred or predicted' personal or personality characteristics are different types of information and personal data that need to be distinguished. ' Known characteristics' are based on information which has been provided to the AI system as an  input,  and  which  is  in  most  cases  verifiable  information.  By  contrast, 'inferred characteristics '  are  based  on  information  which  has  been  inferred  from  other information, with the inference usually made by an AI system. ' Predicted characteristics ' are those which are estimated based on patterns with less than 100% accuracy. The concepts of 'inferred' (or derived) data are also used in the context of profiling in Union data protection law and may therefore be a source of inspiration for
116 See Recital 31 AI Act.
117 See Recital 42 AI Act which lists some examples of such characteristics.
118 Article 5(1)(d) AI Act.
interpreting those concepts used in Article 5(1)(c) AI Act. 119 The use of these different types of data may have different implications for the accuracy and the fairness of the scoring  practices  and  therefore  may  be  taken  into  account,  in  particular  where  the processing is opaque or relies on data points whose accuracy is more difficult to be verified.
- . The  social  score  must  lead  to  detrimental  or  unfavourable treatment  in  unrelated  social  contexts  and/or  unjustified  or disproportionate treatment to the gravity of the social behaviour
## a) Causal link between the social score and the treatment
(160) For the prohibition in Article 5(1)(c) AI Act to apply, the social score created by or with the assistance of an AI system must lead to a detrimental or unfavourable treatment for the evaluated person or group of persons. In other words, the treatment must be the consequence of the score, and the score the cause of the treatment. Such a plausible causal  link  may  also  exist  in  cases  where  the  harmful  consequences  have  not  yet materialised but the AI system is intended to or capable of producing such an adverse outcome.  This  is  particularly  relevant  given  that  the  prohibited  practice  in  Article 5(1)(c) AI Act also covers the 'placing on the market' of such AI systems.
- (161) Article 5(1)(c) AI Act does not require the evaluation or classification performed by the AI system to be the sole cause of the detrimental or unfavourable treatment. It therefore also covers AI-enabled scoring practices that may also be subject to or combined with other  human assessments. At the same time, the AI output  must play a sufficiently important role in producing the social score. For example, in the case where a public authority deploys an AI system to assess the trustworthiness of persons and combines its output with a human assessment of additional facts, this AI-enabled social scoring practice will fall within the scope of the prohibition only if the AI-generated score plays a  sufficiently  important  role  in  the  final  decision,  provided  the  other  conditions  for detrimental  or  unfavourable  treatment  are  fulfilled  as  described  below  (see  section . b).
- (162) A score may lead to detrimental or unfavourable treatment even if it is produced by an organisation(s)  different  from  the  one  that  uses  the  score 120 .  For  example,  a  public authority  may  obtain  a  score  for  a  natural  person's  creditworthiness  assessment produced by another company specialised in creditworthiness and risk assessments, which are based on information about the individuals and their behaviour from a variety of sources.
119 See Article 29 Working Party, Guidelines on Automated individual decision making and Profiling for the purposes of Regulation 2016/679, WP251rev.01, , p. 7 et seq.
120 This interpretation is consistent with the CJEU judgment in the SCHUFA I judgment, where the CJEU held in the context of automated decision-making that a  'score'  (evaluation  constituting profiling) produced by  an entity other than  the  one  taking the  final decision can constitute an automated decision under Article 22 of the GDPR. See SCHUFA I judgment, paragraphs 42 to 51 and 60 to 62.
## b) Detrimental or unfavourable treatment in unrelated social contexts and/or unjustified or disproportionate treatment
- (163) The final condition for the prohibition in Article 5(1)(c) AI Act to apply is that the use of the social score must result (or be capable of resulting) in detrimental or unfavourable treatment either:
- i. in social context(s) unrelated to the contexts in which the data was originally generated or collected, or
- ii. unjustified or disproportionate to the social behaviour or its gravity.
(164) These conditions are alternative and may apply also in combination. An analysis on a case-by-case basis is necessary to assess if at least one of them is fulfilled, since many AI-enabled  scoring  and  evaluation  practices  may  not  fulfil  them  and  therefore  be outside the scope of the prohibition. In particular, this may not be the case where the AI-enabled  scoring  practices  are  for  a  specific  legitimate  evaluation  purpose  and comply with applicable Union and national laws that specify the data considered as relevant for the purposes of evaluation and ensure that the detrimental or unfavourable treatment is justified and proportionate to the social behaviour (see section . out of scope).
- (165) ' Unfavourable treatment' means that as a result from the scoring, the person or group of  persons  must  be  treated  less  favourably  compared  to  others  without  necessarily requiring a particular harm or damage (for example, in the case of scoring practices where people are singled out for additional inspections in case of fraud suspicious). By contrast,  ' detrimental' treatment  requires  the  person  or  group  of  persons  to  suffer certain harm and detriment from the treatment. Unfavourable or detrimental treatment may also be discriminatory and prohibited under EU non-discrimination law or imply the exclusion of certain individuals or groups 121 , but that is not a necessary condition for  the  prohibition  to  apply.  Article  5(1)(c)  AI  Act  could  therefore  cover  unfair treatment  beyond  EU  non-discrimination  law  that  applies  only  to  certain  protected groups (e.g., age, ethnic and racial origin, sex, religion).
## Scenario 1: Detrimental or unfavourable treatment in unrelated social contexts
(166) In  the  first  scenario  described  under  Article  5(1)(c)(i)  AI  Act,  the  detrimental  or unfavourable treatment resulting from the score must take place in social context(s) unrelated to the contexts in which the data was originally generated or collected. This implies  not  only  that  the  persons  may  be  treated  in  an  unfavourable  or  detrimental manner due to the social score, but also that the data about their social behaviour or their known, inferred or predicted personal or personality characteristics are generated or collected in social contexts unrelated to the one in which the scoring takes place. The data collected or generated from these unrelated contexts must be subsequently used by the AI system for the scoring of the persons without an apparent connection for the
121 Recital 31 AI Act.
purpose  of  the  evaluation  or  classification  or  in  a  manner  that  leads  to  generalised surveillance of the persons or the groups of persons. In most cases, this happens against the reasonable expectations of the persons and in violation of Union data protection law and possibly other applicable rules that specify the types of data and sources considered relevant and necessary for the evaluation or classification. Whether this condition is fulfilled will require a case-by-case assessment, taking into account the purpose of the evaluation and the contexts from which the data has been collected and generated.
Examples  of  detrimental  or  unfavourable  treatment  in  unrelated  social  contexts prohibited under Article 5(1)(c) i) AI Act
- -National tax authorities use an AI predictive tool on all taxpayers' tax returns in a country to select tax returns for closer inspection. The AI tool uses relevant variables, such as yearly income, assets (real estate property, cars etc.), data on family members of beneficiaries, but also unrelated data, such as taxpayers' social habits or internet connections, to single out specific individuals for inspections.
- -  A social welfare agency uses an AI system to estimate the probability of fraud by beneficiaries  of  household  allowances  that  relies  on  characteristics  collected  or inferred  from  social  contexts  with  no  apparent  connection  or  relevance  for  the assessment of fraud, such as having a spouse of a certain nationality or ethnic origin, having an internet connection, behaviour on social platforms, or performance at the workplace, etc. 122 By contrast, data that is relevant for the allocation of the benefits and  lawfully  collected  could  be  used  to  determine  the  risk  of  fraud,  since  public authorities  pursue  a  legitimate  aim  in  verifying  if  social  benefits    are  correctly allocated.
- - A public labour agency uses an AI system to score unemployed individuals based on an  interview  and  an  AI-based  assessment  for  determining  whether  an  individual should benefit from state support for employment. 