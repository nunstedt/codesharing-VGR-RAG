- (109) Compliance with the obligations applicable to the providers of general-purpose AI models should be commensurate and proportionate to the type of model provider, excluding the need for compliance for persons who develop or use models for non-professional or scientific research purposes, who should nevertheless be encouraged to voluntarily comply  with  these  requirements.  Without  prejudice  to  Union  copyright  law,  compliance  with  those  obligations should  take  due  account  of  the  size  of  the  provider  and  allow  simplified  ways  of  compliance  for  SMEs,  including start-ups,  that  should  not  represent  an  excessive  cost  and  not  discourage  the  use  of  such  models.  In  the  case  of a  modification  or  fine-tuning  of  a  model,  the  obligations  for  providers  of  general-purpose  AI  models  should  be limited to that modification or fine-tuning, for example  by  complementing  the  already  existing technical documentation with information on the modifications, including new training data sources, as a means to comply with the  value  chain  obligations  provided  in  this  Regulation.
- (110) General-purpose AI models could pose systemic risks which include, but are not limited to, any actual or reasonably foreseeable negative effects in relation to major accidents, disruptions of critical sectors and serious consequences to public health and safety; any actual or  reasonably foreseeable negative effects on democratic processes, public and economic security; the dissemination of illegal, false, or discriminatory content. Systemic risks should be understood to  increase  with  model  capabilities  and  model  reach,  can  arise  along  the  entire  lifecycle  of  the  model,  and  are influenced  by conditions of misuse, model reliability, model  fairness and  model security,  the level of autonomy of
the  model,  its  access  to  tools,  novel  or  combined  modalities,  release  and  distribution  strategies,  the  potential  to remove  guardrails  and  other  factors.  In  particular,  international  approaches  have  so  far  identified  the  need  to  pay attention  to  risks  from  potential  intentional  misuse  or  unintended  issues  of  control  relating  to  alignment  with human intent; chemical, biological, radiological, and nuclear risks, such as the ways in which barriers to entry can be lowered,  including  for  weapons  development,  design  acquisition,  or  use;  offensive  cyber  capabilities,  such  as  the ways in vulnerability  discovery,  exploitation,  or  operational  use  can  be  enabled;  the  effects  of  interaction  and  tool use,  including  for  example  the  capacity  to  control  physical  systems  and  interfere  with  critical  infrastructure;  risks from models of making copies of themselves or 'self-replicating' or training other models; the ways in which models can give rise to harmful bias and discrimination with risks to individuals, communities or societies; the facilitation of disinformation or harming privacy with threats to democratic values and human rights; risk that a particular event could  lead  to  a  chain  reaction  with  considerable  negative  effects  that  could  affect  up  to  an  entire  city,  an  entire domain activity or  an  entire  community.
- (111) It  is  appropriate to establish a methodology for  the classification of general-purpose AI models as general-purpose AI  model  with  systemic  risks.  Since  systemic  risks  result  from  particularly  high  capabilities,  a  general-purpose  AI model  should  be  considered  to  present  systemic  risks  if  it  has  high-impact  capabilities,  evaluated  on  the  basis  of appropriate  technical  tools  and  methodologies,  or  significant  impact  on  the  internal  market  due  to  its  reach. High-impact  capabilities  in  general-purpose  AI  models  means  capabilities  that  match  or  exceed  the  capabilities recorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better understood after its placing on the market or when deployers interact with the model. According to the state of the art at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of the general-purpose AI model measured in floating point operations is one of the relevant approximations for model capabilities.  The  cumulative  amount  of  computation  used  for  training  includes  the  computation  used  across  the activities  and  methods  that  are  intended  to  enhance  the  capabilities  of  the  model  prior  to  deployment,  such  as pre-training,  synthetic  data  generation  and  fine-tuning.  Therefore,  an  initial  threshold  of  floating  point  operations should  be  set, which,  if met  by  a general-purpose  AI  model,  leads  to  a presumption  that  the  model  is a general-purpose AI model with systemic risks. This threshold should be adjusted over time to reflect technological and  industrial  changes,  such  as  algorithmic  improvements  or  increased  hardware  efficiency,  and  should  be supplemented  with  benchmarks  and  indicators  for  model  capability.  To  inform  this,  the  AI  Office  should  engage with the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarks for  the  assessment  of  high-impact  capabilities,  should  be  strong  predictors  of  generality,  its  capabilities  and associated systemic risk of general-purpose AI models, and could take into account the way the model will be placed on the market or the number of users it may affect. To complement this system, there should be a possibility for the Commission  to  take  individual  decisions  designating  a  general-purpose  AI  model  as  a  general-purpose  AI  model with systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold. That decision should be taken on the basis of an overall assessment of the criteria for the designation of a general-purpose AI model with systemic risk set out in an annex to this Regulation, such as quality or size of the training  data  set,  number  of  business  and  end  users,  its  input  and  output  modalities,  its  level  of  autonomy  and scalability, or  the tools it has access to. Upon a reasoned request of a provider whose model has been designated as a  general-purpose  AI  model  with  systemic  risk,  the  Commission  should  take  the  request  into  account  and  may decide  to  reassess  whether  the  general-purpose  AI  model  can  still  be  considered  to  present  systemic  risks.
- (112) It  is  also  necessary  to  clarify  a  procedure  for  the  classification  of  a  general-purpose  AI  model  with  systemic  risks. A general-purpose AI model that meets the applicable threshold for high-impact capabilities should be presumed to be a general-purpose AI models with systemic risk. The provider should notify the AI Office at the latest two weeks after  the  requirements  are  met  or  it  becomes  known  that  a  general-purpose  AI  model  will  meet  the  requirements that  lead  to  the  presumption.  This  is  especially  relevant  in  relation  to  the  threshold  of  floating  point  operations because training of general-purpose AI models takes considerable planning which includes the upfront allocation of compute resources and, therefore,  providers  of  general-purpose  AI  models  are  able  to  know  if  their  model  would meet the threshold before the training is completed. In the context of that notification, the provider should be able to demonstrate that, because of its specific characteristics, a general-purpose AI model exceptionally does not present systemic  risks,  and  that  it  thus  should  not  be  classified  as  a  general-purpose  AI  model  with  systemic  risks.  That information is valuable for the AI Office to anticipate the placing on the market of general-purpose AI models with systemic  risks  and  the  providers  can  start  to  engage  with  the  AI  Office  early  on.  That  information  is  especially
important with regard to general-purpose AI models that are planned to be released as open-source, given that, after the open-source model release, necessary measures to ensure compliance with the obligations under this Regulation may be more difficult to implement.
- (113) If  the Commission becomes aware of the fact that a general-purpose AI model meets the requirements to classify as a  general-purpose  AI  model  with  systemic  risk,  which  previously  had  either  not  been  known  or  of  which  the relevant  provider  has  failed  to  notify  the  Commission,  the  Commission  should  be  empowered  to  designate  it  so. A system of qualified alerts should ensure that the AI Office is made aware by the scientific panel of general-purpose AI  models  that  should  possibly  be  classified  as  general-purpose  AI  models  with  systemic  risk,  in  addition  to  the monitoring activities  of  the  AI  Office.
- (114) The  providers  of  general-purpose  AI  models  presenting  systemic  risks  should  be  subject,  in  addition  to  the obligations provided for providers of general-purpose AI models, to obligations aimed at identifying and mitigating those  risks  and  ensuring  an  adequate  level  of  cybersecurity  protection,  regardless  of  whether  it  is  provided  as a standalone model or embedded in an AI system or a product. To achieve those objectives, this Regulation should require providers to perform the necessary model evaluations, in particular prior  to its first placing on the market, including  conducting  and  documenting  adversarial  testing  of  models,  also,  as  appropriate,  through  internal  or independent  external  testing.  In  addition,  providers  of  general-purpose  AI  models  with  systemic  risks  should continuously assess and mitigate systemic risks, including for example by putting in place risk-management policies, such  as  accountability  and  governance  processes,  implementing  post-market  monitoring,  taking  appropriate measures along the entire  model's  lifecycle  and  cooperating  with  relevant  actors  along  the  AI  value  chain.
- (115) Providers  of  general-purpose  AI  models  with  systemic  risks  should  assess  and  mitigate  possible  systemic  risks.  If, despite efforts to identify and prevent risks related to a general-purpose AI model that may present systemic risks, the  development  or  use  of  the  model  causes  a  serious  incident,  the  general-purpose  AI  model  provider  should without undue delay keep track of the incident and report any relevant information and possible corrective measures to  the  Commission and national competent authorities. Furthermore, providers should ensure an adequate level of cybersecurity protection for the model and its physical infrastructure, if appropriate, along the entire model lifecycle. Cybersecurity  protection  related  to  systemic  risks  associated  with  malicious  use  or  attacks  should  duly  consider accidental model leakage, unauthorised releases, circumvention of safety measures, and defence against cyberattacks, unauthorised  access  or  model  theft.  That  protection  could  be  facilitated  by  securing  model  weights,  algorithms, servers, and data sets, such as through operational security measures for information security, specific cybersecurity policies,  adequate  technical  and  established  solutions,  and  cyber  and  physical  access  controls,  appropriate  to  the relevant  circumstances  and  the  risks  involved.
- (116) The AI Office should encourage and facilitate the drawing up, review and adaptation of codes of practice, taking into account  international  approaches.  All  providers  of  general-purpose  AI  models  could  be  invited  to  participate.  To ensure that the codes of practice reflect the state of the art and duly take into account a diverse set of perspectives, the AI Office should collaborate with relevant national competent authorities, and could, where appropriate, consult with  civil  society  organisations  and  other  relevant  stakeholders  and  experts,  including  the  Scientific  Panel,  for  the drawing up of such codes. Codes of practice should cover obligations  for  providers of  general-purpose AI  models and of general-purpose AI models presenting systemic risks. In addition, as regards systemic risks, codes of practice should help to establish a risk taxonomy of the type and nature of the systemic risks at Union level, including their sources.  Codes  of  practice  should  also  be  focused  on  specific  risk  assessment  and  mitigation  measures.
- (117) The codes of practice  should  represent  a  central  tool  for  the  proper  compliance  with  the  obligations  provided  for under  this  Regulation  for  providers  of  general-purpose  AI  models.  Providers  should  be  able  to  rely  on  codes  of practice  to  demonstrate  compliance  with  the  obligations.  By  means  of  implementing  acts,  the  Commission  may decide  to  approve  a  code  of  practice  and  give  it  a  general  validity  within  the  Union,  or,  alternatively,  to  provide common rules for the implementation of the relevant obligations, if, by the time this Regulation becomes applicable, a  code  of  practice  cannot  be  finalised  or  is  not  deemed  adequate by the  AI  Office.  Once  a  harmonised  standard  is
published and assessed as suitable  to cover  the  relevant obligations  by the  AI  Office,  compliance  with  a  European harmonised  standard  should  grant  providers  the  presumption  of  conformity.  Providers  of  general-purpose  AI models should furthermore be able to demonstrate compliance using alternative adequate means, if codes of practice or  harmonised standards  are  not  available,  or  they choose  not  to  rely on  those.
- (118) This Regulation regulates AI systems and AI models by imposing certain requirements and obligations for relevant market actors that are placing them on the market, putting into service or use in the Union, thereby complementing obligations for providers of intermediary services that embed such systems or models into their services regulated by Regulation  (EU)  2022/2065.  To  the  extent  that  such  systems  or  models  are  embedded  into  designated  very  large online platforms or  very large online search engines, they are subject to the risk-management framework provided for  in  Regulation  (EU)  2022/2065.  Consequently,  the  corresponding  obligations  of  this  Regulation  should  be presumed to be fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 emerge and are identified  in  such  models.  Within  this  framework,  providers  of  very  large  online  platforms  and  very  large  online search engines are obliged to assess potential systemic risks stemming from the design, functioning and use of their services, including how the design of algorithmic systems used in the service may contribute to such risks, as well as systemic  risks  stemming  from  potential  misuses.  Those  providers  are  also  obliged  to  take  appropriate  mitigating measures in observance of fundamental rights.
- (119) Considering  the  quick  pace  of  innovation  and  the  technological  evolution  of  digital  services  in  scope  of  different instruments  of  Union  law  in  particular  having  in  mind  the  usage  and  the  perception  of  their  recipients,  the  AI systems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of Regulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems may be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot performs searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the updated knowledge to generate a single output that  combines different sources  of  information.
- (120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the detection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly relevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards the  obligations  of  providers  of  very  large  online  platforms  or  very  large  online  search  engines  to  identify  and mitigate  systemic  risks  that  may  arise  from  the  dissemination  of  content  that  has  been  artificially  generated  or manipulated, in particular risk of the actual or foreseeable negative effects on democratic processes, civic discourse and electoral  processes,  including  through  disinformation.
- (121) Standardisation  should  play  a  key  role  to  provide  technical  solutions  to  providers  to  ensure  compliance  with  this Regulation,  in  line  with  the  state  of  the  art,  to  promote  innovation  as  well  as  competitiveness  and  growth  in  the single  market.  Compliance  with  harmonised  standards  as  defined  in  Article  2,  point  (1)(c),  of  Regulation  (EU) No 1025/2012 of the European Parliament and of the Council ( 41 ),  which are normally expected to reflect the state of  the  art,  should  be  a  means  for  providers  to  demonstrate  conformity  with  the  requirements  of  this  Regulation. 