Annexes, SWD(2021)84 final, Part2/2, p. 43.  See also I. Tuomi, T he , The use of Artificial Intelligence (AI) in education, European Parliament, 2020, pp. 9- 10. 165 See Article 14 Charter.
teachers), emotion recognition systems are allowed if the results cannot impact the evaluation or certification of the person being trained.
- -  Using  an  emotion  recognition  AI  system  by  an  education  institution  during admissibility tests for new students is prohibited.
- - Using an AI system that allows to capture students talking to each other via their phones or other channels during online lectures by an education institution is not prohibited, since it does not infer emotions. By contrast, if the system is also used to detect emotions, such as emotional arousal, anxiousness and interest, this would fall within the scope of the prohibition.
- - An education institution employing an emotion recognition AI system on both teachers (workplace) and students (education) is prohibited.
## . Exceptions for medical and safety reasons
(256) The prohibition in Article 5(1)(f) AI Act contains an explicit exception for emotion recognition systems used in the area of the workplace and education institutions for medical or safety reasons, such as systems for therapeutical use. 166 In light of the AI Act's objective to ensure a high-level of fundamental rights protection, this exception should be narrowly interpreted.
(257) In  particular,  therapeutic  uses  should  be  understood  to  mean  uses  of  CE-marked medical  devices.  Moreover,  this  exception  does  not  comprise  the  use  of  emotion recognition systems to detect general aspects of wellbeing. The general monitoring of stress  levels  at  the  workplace  is  not  permitted  under  health  or  safety  aspects.  For example, an AI system intended to detect burnout or depression at the workplace or in education  institutions  would  not  be  covered  by  the  exception  and  would  remain prohibited.
(258) The notion of safety reasons within this exception should be understood to apply only in  relation  to  the  protection  of  life  and  health  and  not  to  protect  other  interests,  for example property against theft or fraud.
(259) It follows from this narrow interpretation of the exception that any use for medical and safety  reasons  should  always  remain  limited  to  what  is  strictly  necessary  and proportionate, including limits in time, personal application and scale, and should be accompanied  by  sufficient  safeguards.  Such  safeguards  could  include,  for  example, prior  written  and  motivated  expert  opinion  relating  to  the  specific  use  case.  The necessity should be assessed on an objective basis in relation to the medical and safety purpose,  and  not  refer  to  the  employer's  or  educational  institution's  'needs'.  This assessment should inquire whether less intrusive alternative means exist which would achieve the same purpose.
166 Recital 44 AI Act.
- (260) Employers and educators should only deploy emotion recognition systems for medical and safety reasons in case of an explicit need 167 . Data collected and processed in this context may not be used for any other purpose. This is particularly important given that the use of AI management software at work has proven to potentially negatively impact workers' health and safety. Continuous monitoring via wearables, for instance, may increase work-stress while affecting productivity 168 .
- (261) Since Recital 18 AI Act excludes from the definition of emotion recognition systems physical states, such as pain or fatigue, a number of AI systems used for safety reasons would already not fall under that definition, including, for example, systems used in detecting  the  state  of  fatigue  of  professional  pilots  or  drivers  for  the  purpose  of preventing accidents.
- (262) Other laws, including data protection rules, remain applicable to emotion recognition systems that fulfil the conditions of the exception in Article 5(1)(f) AI Act 169 .
- (263) Emotion recognition systems that classify as high-risk systems pursuant to Article 6(2) and Annex III(1)(c) AI Act will need to comply with the high-risk requirements in Chapter III Section 2 AI Act and the transparency obligation of Article 50(3) AI Act.
## For example:
Emotion  recognition  may  be  deployed  for  medical  reasons  to  assist  employees  or students with autism and improve accessibility for those who are blind or deaf 170 . Such uses would fall within the exception for medical reasons in Article 5(1)(f) AI Act.
By  contrast,  emotion  recognition  for  assessing  students'  or  employees'  well-being, motivation levels, and job or learning satisfaction do not qualify as 'use for medical reasons' and would be prohibited.
An  employer  would  be  prohibited  from  deploying  AI-enabled  devices  or  digital assistants at the workplace for measuring anxiety based on measured stress levels or for measuring boredom of employees, unless the elevated stress level/lack of concentration would pose a  specific  danger,  for  example  when  deploying  dangerous  machines  or dealing with dangerous chemicals. In the latter case, the employer may not use the data for other purposes, such as assessing the employee's work performance.
## . More favourable Member State law
(264) Article 2(11) AI Act provides that the Union or Member States may keep or introduce 'laws, regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of AI systems by employers'.
167 In  conformity  with  EU employment law, if such new technologies are introduced, employers shall also consult with workers or their representatives,  conform  national  procedures.  Without  respecting  these  procedural  requirements,  such  systems  cannot  be  introduced  by reference to the AI Act as such.  They will require also consent from the point of view of data protection legislation, which remains applicable. 168 The Interconnection between the AI Act and the EU's Occupational Safety and Health Legal Framework - Global Workplace Law &amp; Policy (kluwerlawonline.com) .
169 From December 2026 Directive (EU) 2024/2831 of the European Parliament and of the Council of 23 October 2024 on improving working conditions in platform work will apply.
170 Systems could be usefully employed for helping employees or students/pupils to understand the emotions of colleagues etc.
Collective bargaining agreements which are more favourable to workers may also be allowed or encouraged.
For  example,  Member  States  may  adopt  laws  providing  that  the  use  of  emotion recognition systems in the area of work may not be applied for medical purposes.
## . Out of scope
(265) As mentioned before, out of scope are:
- -AI systems inferring emotions and sentiments not on the basis of biometric data,
- -AI systems inferring physical states such as pain and fatigue.
(266) Emotion recognition systems used in all other domains other than in the areas of the workplace and education institutions do not fall under the prohibition in Article 5(1)(f) AI Act. Such systems are, however, considered high-risk AI systems. 171 At the same time, such systems may be prohibited in certain cases by virtue of Article 5(1)(a) and (b)  AI  Act  (harmful  manipulation  and  exploitation),  or  by  virtue  of  other  Union legislation.  All  other  applicable  legislation,  such  as  Union  data  protection  law, consumer protection etc. continue to apply to such systems.
## For example:
Emotion recognition systems used in a commercial context for addressing customers do not fall under the prohibition of Article 5(1)(f) AI Act, whether based on biometric data or not. Hence, examples such as AI systems that enable emotion recognition based on keystroke or based on voice messages of customers (e.g., chat messages, use of virtual  voice  assistants),  used  in  online  marketing  for  applications  for  displaying personalized messages and for advertisement purposes including in smart environments ('intelligent billboards') are not covered by the prohibition.
Nevertheless, such practices may  be  covered by the prohibitions of harmful manipulation and exploitation in Article 5(1)(a) and (b) AI Act 172 , if all conditions for the application of those prohibitions are met.
## a) Other systems out of scope
(267) 'Crowd control'  generally  refers  to  the  control  and  monitoring  of  the  behaviour  of groups to maintain (public) order and event safety. It is often associated with large crowd events (e.g., soccer or football games, concerts, etc) or specific places, such as airports  or  trains.  Crowd  control  systems  can  operate  without  inferring  emotions  of individual persons, when for example analysing the general noise and mood level at a given place. In that case, the system would not fall within the scope of Article 5(1)(f) AI Act, because it does not infer emotions of (a concrete) natural person.
171 Article 6(2) AI Act and Annex III, 1 letter c).
172 These situations might also be prohibited under other rules, such as data or consumer protection.
(268) However, there may be instances where such crowd control systems infer emotions of individuals,  for  example  whether  there  are  many  angry  faces.  Normally,  such  AI systems would not fall under the prohibition of Article 5(1)(f) AI Act, since they are typically not used in the workplace or in education institutions.
(269) Also out of scope are systems that are used in the medical field for example care robots, or medical practitioners using emotion recognition systems during an examination at their workplace, and voice monitors that analyse emergency calls.
(270) Such systems will often screen persons that are there in a work context, for example the security staff at a football stadium or at a central station (where such systems are used to recognize aggressive behaviour), or employees in the medical field. In such cases, deployers must employ safeguards to avoid the screening of employees. However, it cannot  be  completely  avoided  that  such  systems  also  infer  the  emotions  of  those employees.  Since  the  primary  objective  of  the  system  is  not  targeted  at  assessing employees' emotions, these systems should be considered to be outside the scope of the prohibition. Deployers of such systems remain responsible to ensure that employees are not adversely affected by their use.
## 8. ARTICLE 5(1)(G) AI ACT: BIOMETRIC CATEGORISATION FOR CERTAIN 'SENSITIVE' CHARACTERISTICS
(271) Article  5(1)(g)  AI  Act  prohibits  biometric  categorisation  systems  that  categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or  sexual  orientation.  This  prohibition  does  not  cover  the  labelling,  filtering,  or categorisation of biometric data sets acquired in line with Union or national law, which may be used, for example, for law enforcement purposes. 173
## . Rationale and objectives
(272) A wide variety of information,  including  'sensitive'  information,  may  be  extracted, deduced or inferred from biometric information, even without the knowledge of the persons  concerned,  to categorise those persons. This may  lead  to  unfair  and discriminatory treatment, for example when a service is denied because somebody is considered to be of a certain race. AI-based biometric categorisation systems for the purpose of assigning natural persons to specific groups or categories, relating to aspects such as sexual or political orientation or race, violate human dignity and pose significant risks  to  other  fundamental  rights,  such  as  privacy  and  non-discrimination.  They  are therefore prohibited by Article 5(1)(g) AI Act.
## . Main concepts and components of the prohibition
Article 5(1)(g) AI Act provides
173 Recital 30 AI Act.
The following AI practices shall be prohibited:
g) the placing on the market, the putting into service for this specific purpose, or the use  of  biometric  categorisation  systems  that  categorise  individually  natural  persons based on their biometric data to  deduce  or  infer  their  race,  political  opinions,  trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition  does  not  cover  any  labelling  or  filtering  of  lawfully  acquired  biometric datasets, such as images, based on biometric data or categorising of biometric data in the area of law enforcement;
(273) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(g) AI Act to apply:
- (i) The practice must constitute the 'placing on the market', 'the putting into service for this specific purpose' or 'the use' of an AI system;
- (ii) The system must be a biometric categorisation system;
- (iii) individual persons must be categorised;
- (iv) based on their biometric data;
- (v) to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation.
(274) For the prohibition to apply, all five conditions must be simultaneously fulfilled. The first condition, i.e. the placing on the market, the putting into service or the use of the AI  system,  is  analysed  in  section  .  The  prohibition,  therefore,  applies  to  both providers and deployers of AI systems, each within their respective responsibilities, not to place on the market, put into service or use such AI systems. The other conditions for the application of the prohibition 174 are further described and analysed below.
(275) The prohibition does not cover the labelling or filtering of lawfully acquired biometric datasets, including for law enforcement purposes.
## . Biometric categorisation system
(276) ' The categorisation of an individual by a biometric system is typically the process of establishing whether the biometric data of an individual belongs to a group with some pre-defined characteristic.  It is not about identifying an individual or verifying their identity,  but  about  assigning  an  individual  to  a  certain  category.  For  instance,  an advertising  display  may  show  different  adverts  depending  on  the  individual  that  is looking at it based on their age or gender.' 175 Persons may also simply be categorised for  statistical  reasons,  without  being  identified  and  without  the  objective  to  identify them.
174 For the criterion of 'AI system', 'the 'placing on the market', 'the putting into service for this specific purpose' or the use, see above.
175 See the Article 29 Working Party, Opinion 3/2012 on developments in biometric technologies, WP193, , p. 6.
- (277) Article 3(40) AI Act defines a biometric categorisation system as an AI system for the purpose  of  assigning  natural  persons  to  specific  categories  on  the  basis  of  their biometric data, unless it is ancillary to another commercial service and strictly necessary for  objective  technical  reasons.  As  explained  in  section  .d),  'biometric  data'  is defined in Article 3(34) AI Act. In particular, biometric data comprises behavioural characteristics that are based on biometric features. The scope of biometric categorisation  excludes  categorisation  according  to  clothes  or  accessories,  such  as scarfs or crosses, as well as social media activity.
- (278) Biometric categorisation may rely on categories of physical characteristics (e.g. facial features  and  form,  skin  colour)  based  on  which  persons  are  assigned  to  specific categories.  Some  of  these  categories  may  be  of  a  special  'sensitive'  nature'  or characteristics protected under Union non-discrimination law, such as race. However, biometric categorisation may also be based on DNA or on behavioural aspects, such as keystroke analysis or a person's gait 176 .
- (279) To fall outside the scope of the definition of biometric categorisation under the AI Act, two conditions - being 'ancillary to another commercial service and strictly necessary for objective technical reasons' - must be cumulatively fulfilled.
- (280) According to recital 16 AI Act, a purely ancillary feature is a feature that is intrinsically linked to another commercial service, meaning that the feature cannot, for objective technical  reasons,  be  used  without  the  principal  service,  and  the  integration  of  that feature or functionality is not a means to circumvent the applicability of the rules of the AI Act.
For example, the following uses of AI are permitted under Article 5(1)(g) AI Act:
- - Filters categorising facial or bodily features used on online marketplaces to allow a consumer to preview a product on him or herself could constitute such an ancillary feature, since they can be only used in relation to the principal service which consists in selling a product.
- - Filters integrated into online social network services which categorise facial or bodily features to allow users to add or modify pictures or videos could also be considered to be ancillary feature, since such a filter cannot be used without the principal service of the social network services consisting in the sharing of content online.
In contrast, examples of uses that would be prohibited include:
- - An AI system that categorises persons active on a social media platform according to their assumed political orientation, by analysing the biometric data from the photos they have uploaded on the platform, to send them targeted political messages. While such  a  system  may  only  be  ancillary  to  the  political  advertising,  it  would  not  be
176 See e.g., the Article 29 Working Party, Opinion 3/2012 on developments in biometric technologies, WP193, , pp.16-17. The Group refers here to 'soft recognition' (p. 17), i.e. 'detection of behaviour or specific needs of people'.
'strictly necessary for objective technical reasons', hence the conditions for excluding it from the definition of biometric categorisation are not fulfilled.
- - An AI system that categorises persons active on a social media platform according to their assumed sexual orientation by analysing the biometric data from photos shared on that platform and on that basis serves those persons advertisements would qualify as biometric categorisation within the meaning of the AI Act. Also in this case there is  no  strict  necessity  for  this ' ancillary  service',  hence  the  exclusion  from  the prohibition does not apply.
## . Persons are individually categorised based on their biometric data
(281) The  use  of  biometric  data  for  the  categorisation  of  natural  persons  is  an  essential element for the prohibition to apply (see above section . and .d)).
- (282) Furthermore,  for  the  prohibition  to  apply,  natural  persons  must  be  'individually' categorised. 