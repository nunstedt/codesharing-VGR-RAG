1).
- Regulation should apply. For example, Article 16(2) of Regulation (EU) 2017/745, establishing that certain changes should  not  be  considered  to  be  modifications  of  a  device  that  could  affect  its  compliance  with  the  applicable requirements, should continue to apply to high-risk AI systems that are medical devices within the meaning of that Regulation.
- (85) General-purpose AI systems may be used as high-risk AI systems by themselves or be components of other high-risk AI systems. Therefore, due to their particular nature and in order to ensure a fair sharing of responsibilities along the AI  value  chain,  the  providers  of  such  systems  should,  irrespective  of  whether  they  may  be  used  as  high-risk  AI systems as such by other providers or as components of high-risk AI systems and unless provided otherwise under this Regulation, closely cooperate with the providers of the relevant high-risk AI systems to enable their compliance with  the  relevant  obligations  under  this  Regulation  and  with  the  competent  authorities  established  under  this Regulation.
- (86) Where, under  the  conditions  laid  down  in  this  Regulation,  the  provider  that  initially  placed  the  AI  system  on  the market or put it into service should no longer be considered to be the provider for the purposes of this Regulation, and  when  that  provider  has  not  expressly  excluded  the  change  of  the  AI  system  into  a  high-risk  AI  system,  the former provider should nonetheless closely cooperate and make available the necessary information and provide the reasonably expected technical access and other assistance that are required for  the fulfilment of  the obligations set out  in  this  Regulation,  in  particular  regarding  the  compliance  with  the  conformity  assessment  of  high-risk  AI systems.
- (87) In  addition,  where  a  high-risk  AI  system  that  is  a  safety  component  of  a  product  which  falls  within  the  scope  of Union harmonisation legislation based on the New Legislative Framework is not placed on the market or  put into service  independently  from  the  product,  the  product  manufacturer  defined  in  that  legislation  should  comply  with the  obligations  of  the  provider  established  in  this  Regulation  and  should,  in  particular,  ensure  that  the  AI  system embedded in the final  product  complies  with  the  requirements  of  this  Regulation.
- (88) Along  the  AI  value  chain  multiple  parties  often  supply  AI  systems,  tools  and  services  but  also  components  or processes  that  are  incorporated  by  the  provider  into  the  AI  system  with  various  objectives,  including  the  model training,  model  retraining,  model  testing  and  evaluation,  integration  into  software,  or  other  aspects  of  model development. Those parties have an important role to play in the value chain towards the provider of the high-risk AI system into which their AI systems, tools, services, components or processes are integrated, and should provide by written agreement this provider with the necessary information, capabilities, technical access and other assistance based  on  the  generally  acknowledged  state  of  the  art,  in  order  to  enable  the  provider  to  fully  comply  with  the obligations set out in this Regulation, without compromising their own intellectual property rights or trade secrets.
- (89) Third parties making accessible to the public tools, services, processes, or AI components other than general-purpose  AI  models,  should  not  be  mandated  to  comply  with  requirements  targeting  the  responsibilities along  the  AI  value  chain,  in  particular  towards  the  provider  that  has  used  or  integrated  them,  when  those  tools, services, processes, or AI components are made accessible under a free and open-source licence. Developers of free and  open-source  tools,  services,  processes,  or  AI  components  other  than  general-purpose  AI  models  should  be encouraged to implement widely adopted documentation practices, such as model cards and data sheets, as a way to accelerate  information sharing along the AI value chain, allowing the promotion of  trustworthy AI systems in the Union.
- (90) The Commission could develop and recommend voluntary model contractual terms between providers of high-risk AI  systems  and  third  parties  that  supply  tools,  services,  components  or  processes  that  are  used  or  integrated  in high-risk  AI  systems,  to  facilitate  the  cooperation  along  the  value  chain.  When  developing  voluntary  model contractual  terms,  the  Commission  should  also  take  into  account  possible  contractual  requirements  applicable  in specific  sectors  or  business  cases.
- (91) Given  the  nature  of  AI  systems  and  the  risks  to  safety  and  fundamental  rights  possibly  associated  with  their  use, including as regards the need to ensure proper monitoring of the performance of an AI system in a real-life setting, it is appropriate to set specific responsibilities for deployers. Deployers should in particular take appropriate technical and organisational measures to ensure they use high-risk AI systems in accordance with the instructions of use and certain other obligations should be provided for with regard to monitoring of the functioning of the AI systems and with  regard  to  record-keeping,  as  appropriate.  Furthermore,  deployers  should  ensure  that  the  persons  assigned  to implement  the  instructions  for  use  and  human  oversight  as  set  out  in  this  Regulation  have  the  necessary
competence, in particular an adequate level of AI literacy, training and authority to properly fulfil those tasks. Those obligations  should  be  without  prejudice  to  other  deployer  obligations  in  relation  to  high-risk  AI  systems  under Union or  national  law.
- (92) This  Regulation  is  without  prejudice  to  obligations  for  employers  to  inform  or  to  inform  and  consult  workers  or their  representatives  under  Union  or  national  law  and  practice,  including  Directive  2002/14/EC  of  the  European Parliament and of the Council ( 39 ),  on decisions to put into service or use AI systems. It remains necessary to ensure information  of  workers  and  their  representatives  on  the  planned  deployment  of  high-risk  AI  systems  at  the workplace  where  the  conditions  for  those  information  or  information  and  consultation  obligations  in  other  legal instruments  are  not  fulfilled.  Moreover,  such  information  right  is  ancillary  and  necessary  to  the  objective  of protecting  fundamental  rights  that  underlies  this  Regulation.  Therefore,  an  information  requirement  to  that  effect should be laid  down  in  this  Regulation,  without  affecting  any existing  rights  of  workers.
- (93) Whilst risks related  to AI  systems can  result  from  the  way  such  systems are  designed,  risks  can  as  well  stem  from how  such  AI  systems  are  used.  Deployers  of  high-risk  AI  system  therefore  play  a  critical  role  in  ensuring  that fundamental rights are protected,  complementing  the obligations  of  the  provider  when  developing  the  AI  system. Deployers  are  best  placed  to  understand  how  the  high-risk  AI  system  will  be  used  concretely  and  can  therefore identify potential significant risks that were not foreseen in the development phase, due to a more precise knowledge of the context of use, the persons or groups of persons likely to be affected, including vulnerable groups. Deployers of high-risk AI systems listed in an annex to this Regulation also play a critical role in informing natural persons and should, when they make decisions or assist in making decisions related to natural persons, where applicable, inform the natural persons that they are subject to the use of the high-risk AI system. This information should include the intended  purpose  and  the  type  of  decisions  it  makes.  The  deployer  should  also  inform  the  natural  persons  about their  right  to  an  explanation  provided  under  this  Regulation.  With  regard  to  high-risk  AI  systems  used  for  law enforcement  purposes,  that  obligation  should  be  implemented  in  accordance  with  Article  13  of  Directive  (EU) 2016/680.
- (94) Any processing of biometric data involved in the use of AI systems for biometric identification for  the purpose of law  enforcement  needs  to  comply  with  Article  10  of  Directive  (EU)  2016/680,  that  allows  such  processing  only where strictly necessary, subject to appropriate safeguards for the rights and freedoms of the data subject, and where authorised by Union or Member State law. Such use, when authorised, also needs to respect the principles laid down in  Article  4  (1)  of  Directive  (EU)  2016/680  including  lawfulness,  fairness  and  transparency,  purpose  limitation, accuracy and storage limitation.
- (95) Without prejudice to applicable Union law, in particular  Regulation (EU) 2016/679 and Directive (EU) 2016/680, considering the intrusive nature of post-remote biometric identification systems, the use of post-remote biometric identification systems should be subject to safeguards. Post-remote biometric identification systems should always be used in a way that is proportionate, legitimate and strictly necessary, and thus targeted, in terms of the individuals to be identified,  the  location,  temporal scope and based on a closed data set of  legally acquired video footage. In any case, post-remote biometric identification systems should not be used in the framework of  law enforcement to lead to  indiscriminate  surveillance.  The  conditions  for  post-remote  biometric  identification  should  in  any  case  not provide a basis to circumvent the conditions of the prohibition and strict exceptions for real time remote biometric identification.
- (96) In order to efficiently ensure that fundamental rights are protected, deployers of high-risk AI systems that are bodies governed by public law, or  private entities  providing public services  and deployers of  certain high-risk  AI  systems listed  in  an  annex  to  this  Regulation,  such  as  banking  or  insurance  entities,  should  carry out  a  fundamental  rights impact assessment prior to putting it into use. Services important for individuals that are of public nature may also be provided by private entities. Private entities providing such public services are linked to tasks in the public interest such  as  in  the  areas  of  education,  healthcare,  social  services,  housing,  administration  of  justice.  The  aim  of  the fundamental rights impact assessment is for the deployer to identify the specific risks to the rights of individuals or groups of individuals likely to be affected, identify measures to be taken in the case of a materialisation of those risks. The  impact  assessment  should  be  performed  prior  to  deploying  the  high-risk  AI  system,  and  should  be  updated
( 39 ) Directive  2002/14/EC  of  the  European  Parliament  and  of  the  Council  of  11  March  2002  establishing  a  general  framework  for informing and consulting employees in the European Community (OJ L 80, , p. 29).
when the deployer considers that any of  the relevant factors have changed. The impact assessment should identify the deployer's relevant processes in which the high-risk AI system will be used in line with its intended purpose, and should include a description of the period of time and frequency in which the system is intended to be used as well as of specific categories of natural persons and groups who are likely to be affected in the specific context of use. The assessment  should  also  include  the  identification  of  specific  risks  of  harm  likely  to  have  an  impact  on  the fundamental  rights  of  those  persons  or  groups.  While  performing  this  assessment,  the  deployer  should  take  into account  information  relevant  to  a  proper  assessment  of  the  impact,  including  but  not  limited  to  the  information given by the provider of the high-risk AI system in the instructions for use. In light of the risks identified, deployers should  determine  measures  to  be  taken  in  the  case  of  a  materialisation  of  those  risks,  including  for  example governance arrangements in that specific context of use, such as arrangements for human oversight according to the instructions of use or, complaint handling and redress procedures, as they could be instrumental in mitigating risks to fundamental rights in concrete use-cases. After performing that impact assessment, the deployer should notify the relevant market surveillance authority. Where appropriate, to collect relevant information necessary to perform the impact  assessment,  deployers  of  high-risk  AI  system,  in  particular  when  AI  systems  are  used  in  the  public  sector, could involve relevant stakeholders, including the representatives of groups of persons likely to be affected by the AI system,  independent experts,  and  civil  society organisations in  conducting  such  impact  assessments  and  designing measures to be taken in the case of materialisation of the risks. The European Artificial Intelligence Office (AI Office) should develop a template for a questionnaire in order to facilitate compliance and reduce the administrative burden for  deployers.
- (97) The notion of general-purpose AI models should be clearly defined and set apart from the notion of AI systems to enable legal certainty. The definition should be based on the key functional characteristics of a general-purpose AI model, in particular  the generality and the capability to competently perform a wide range of distinct tasks. These models  are  typically trained on  large amounts  of  data,  through  various  methods,  such  as  self-supervised, unsupervised or reinforcement learning. General-purpose AI models may be placed on the market in various ways, including  through  libraries,  application  programming  interfaces  (APIs),  as  direct  download,  or  as  physical  copy. These  models  may  be  further  modified  or  fine-tuned  into  new  models.  Although  AI  models  are  essential components of AI systems, they do not constitute AI systems on their own. AI models require the addition of further components, such as for example a user interface, to become AI systems. AI models are typically integrated into and form  part of AI systems. This Regulation provides specific rules for general-purpose AI models  and  for general-purpose AI models that pose systemic risks, which should apply also when these models are integrated or form  part  of  an  AI  system.  It  should  be  understood  that  the  obligations  for  the  providers  of  general-purpose  AI models  should  apply  once  the  general-purpose  AI  models  are  placed  on  the  market.  When  the  provider  of a general-purpose AI model integrates an own model into its own AI system that is made available on the market or put into service, that model should be considered to be placed on the market and, therefore, the obligations in this Regulation for models should continue to apply in addition to those for AI systems. The obligations laid down for models should in any case not apply when an own model is used for purely internal processes that are not essential for  providing a product or a service to third parties and the rights of natural persons are not affected. Considering their  potential  significantly  negative  effects,  the  general-purpose  AI  models  with  systemic  risk  should  always  be subject to the relevant obligations under this Regulation. The definition should not cover AI models used before their placing  on  the  market  for  the  sole  purpose  of  research,  development  and  prototyping  activities.  This  is  without prejudice to the obligation to comply with this Regulation when, following such activities, a model is placed on the market.
- (98) Whereas the generality of a model could, inter alia, also be determined by a number of parameters, models with at least  a  billion  of  parameters  and  trained  with  a  large  amount  of  data  using  self-supervision  at  scale  should  be considered  to  display  significant  generality  and  to  competently  perform  a  wide  range  of  distinctive  tasks.
- (99) Large generative AI models are a typical example for a general-purpose AI model, given that they allow for flexible generation  of  content,  such  as  in  the  form  of  text,  audio,  images  or  video,  that  can  readily  accommodate  a  wide range  of  distinctive  tasks.
- (100) When a general-purpose AI model is integrated into or forms part of an AI system, this system should be considered to  be  general-purpose  AI  system  when,  due  to  this  integration,  this  system  has  the  capability  to  serve  a  variety  of purposes. A general-purpose AI system can be used directly,  or  it  may  be  integrated  into other  AI  systems.
- (101) Providers  of  general-purpose  AI  models  have  a  particular  role  and  responsibility  along  the  AI  value  chain,  as  the models  they  provide  may  form  the  basis  for  a  range  of  downstream  systems,  often  provided  by  downstream providers that necessitate a good understanding of the models and their capabilities, both to enable the integration of such  models  into  their products,  and  to  fulfil  their  obligations  under this  or  other regulations.  Therefore, proportionate  transparency  measures  should  be  laid  down,  including  the  drawing  up  and  keeping  up  to  date  of documentation, and the provision of information on the general-purpose AI model for its usage by the downstream providers.  Technical  documentation  should  be  prepared  and  kept  up  to  date  by  the  general-purpose  AI  model provider  for  the  purpose  of  making  it  available,  upon  request,  to  the  AI  Office  and  the  national  competent authorities. The minimal set of elements to be included in such documentation should be set out in specific annexes to  this  Regulation.  The  Commission  should  be empowered to amend those annexes by means of delegated acts in light  of  evolving  technological  developments.
- (102) Software and data, including models, released under a free and open-source licence that allows them to be openly shared  and  where  users  can  freely  access,  use,  modify  and  redistribute  them  or  modified  versions  thereof,  can contribute to research and innovation in the market and can provide significant growth opportunities for the Union economy. General-purpose AI models released under free and open-source licences should be considered to ensure high levels of  transparency and openness if  their  parameters, including the weights, the information on the model architecture, and the information on model usage are made publicly available. The licence should be considered to be free and open-source also when it allows users to run, copy, distribute, study, change and improve software and data, including models under the condition that the original provider of the model is credited, the identical or comparable terms  of  distribution  are  respected.
- (103) Free  and  open-source  AI  components  covers  the  software  and  data,  including  models  and  general-purpose  AI models, tools, services or processes of an AI system. Free and open-source AI components can be provided through different  channels,  including  their  development  on  open  repositories.  For  the  purposes  of  this  Regulation,  AI components that are provided against a price or otherwise monetised, including through the provision of technical support  or  other  services,  including  through  a  software  platform,  related  to  the  AI  component,  or  the  use  of personal data for reasons other  than exclusively for  improving the security, compatibility or interoperability of  the software,  with  the  exception  of  transactions  between  microenterprises,  should  not  benefit  from  the  exceptions provided  to  free  and  open-source  AI  components.  The  fact  of  making  AI  components  available  through  open repositories  should  not,  in  itself,  constitute  a  monetisation.
- (104) The  providers  of  general-purpose  AI  models  that  are  released  under  a  free  and  open-source  licence,  and  whose parameters, including the weights, the information on the model architecture, and the information on model usage, are  made  publicly  available  should  be  subject  to  exceptions  as  regards  the  transparency-related  requirements imposed on general-purpose AI models, unless they can be considered to present a systemic risk, in which case the circumstance that the model is transparent and accompanied by an open-source license should not be considered to be a sufficient reason to exclude compliance with the obligations under  this Regulation. In any case, given that the release  of  general-purpose  AI  models  under  free  and  open-source  licence  does  not  necessarily  reveal  substantial information on the data set used for the training or fine-tuning of the model and on how compliance of copyright law  was  thereby  ensured,  the  exception  provided  for  general-purpose  AI  models  from  compliance  with  the transparency-related requirements should not concern the obligation to produce a summary about the content used for  model training and the obligation to put in place a policy to comply with Union copyright law, in particular to identify  and  comply  with  the  reservation  of  rights  pursuant  to  Article  4(3)  of  Directive  (EU)  2019/790  of  the European Parliament and of  the Council ( 40 ).
- (105) General-purpose AI models, in particular large generative AI models, capable of generating text, images, and other content, present unique innovation opportunities but also challenges to artists, authors, and other creators and the way their creative content is created, distributed, used and consumed. The development and training of such models require access to vast amounts of text, images, videos and other data. 