Such  techniques  and  methods  should  be  sufficiently  reliable,  interoperable,  effective  and  robust  as  far  as  this  is technically  feasible,  taking  into  account  available  techniques  or  a  combination  of  such  techniques,  such  as watermarks,  metadata  identifications,  cryptographic  methods  for  proving  provenance  and  authenticity of  content, logging  methods,  fingerprints  or  other  techniques,  as  may  be  appropriate.  When  implementing  this  obligation, providers should also take into account the specificities and the limitations of the different types of content and the relevant technological and market developments in the field, as reflected in the generally acknowledged state of the art. Such techniques and methods can be implemented at the level of the AI system or at the level of the AI model, including  general-purpose  AI  models  generating  content,  thereby  facilitating  fulfilment  of  this  obligation  by  the downstream  provider  of  the  AI  system.  To  remain  proportionate,  it  is  appropriate  to  envisage  that  this  marking obligation should not cover AI systems performing primarily an assistive function for standard editing or AI systems not  substantially  altering  the  input  data  provided  by  the  deployer  or  the  semantics  thereof.
- (134) Further  to the technical solutions employed by the providers of  the AI system, deployers who use an AI system to generate or  manipulate image, audio or  video content that appreciably resembles  existing persons, objects,  places, entities  or  events  and  would falsely  appear  to a  person  to be  authentic  or  truthful  (deep  fakes),  should  also  clearly and distinguishably disclose that the content has been artificially created or manipulated by labelling the AI output accordingly  and  disclosing  its  artificial  origin.  Compliance  with  this  transparency  obligation  should  not  be interpreted as indicating that the use of the AI system or its output impedes the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter, in particular where the content is part of an evidently creative, satirical, artistic, fictional or analogous work or programme, subject to appropriate safeguards for the  rights  and  freedoms  of  third  parties.  In  those  cases,  the  transparency  obligation  for  deep  fakes  set  out  in  this Regulation  is  limited  to  disclosure  of  the  existence  of  such  generated  or  manipulated  content  in  an  appropriate manner that does not hamper the display or enjoyment of the work, including its normal exploitation and use, while maintaining  the  utility  and  quality  of  the  work.  In  addition,  it  is  also  appropriate  to  envisage  a  similar  disclosure obligation in relation to AI-generated or manipulated text to the extent it is published with the purpose of informing the public on matters of public interest unless the AI-generated content has undergone a process of human review or editorial  control  and  a  natural  or  legal  person  holds  editorial  responsibility  for  the  publication  of  the  content.
- (135) Without prejudice to the mandatory nature and full applicability of  the transparency obligations, the Commission may  also  encourage  and  facilitate  the  drawing  up  of  codes  of  practice  at  Union  level  to  facilitate  the  effective implementation  of  the  obligations  regarding  the  detection  and  labelling  of  artificially  generated  or  manipulated content,  including  to  support  practical  arrangements  for  making,  as  appropriate,  the  detection  mechanisms accessible and facilitating cooperation with other actors along the value chain, disseminating content or checking its authenticity  and  provenance  to  enable  the  public  to  effectively  distinguish  AI-generated  content.
- (136) The obligations placed on providers and deployers of certain AI systems in this Regulation to enable the detection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly relevant to facilitate  the  effective  implementation  of  Regulation  (EU)  2022/2065.  This  applies  in  particular  as  regards  the obligations of providers of very large online platforms or  very large online search engines to identify and mitigate systemic risks that may arise from the dissemination of content that has been artificially generated or manipulated, in  particular  the  risk  of  the  actual  or  foreseeable  negative  effects  on  democratic  processes,  civic  discourse  and electoral  processes,  including  through  disinformation.  The  requirement  to  label  content  generated  by  AI  systems under  this  Regulation  is  without  prejudice  to  the  obligation  in  Article  16(6)  of  Regulation  (EU)  2022/2065  for providers  of  hosting  services  to  process  notices  on  illegal  content  received  pursuant  to  Article  16(1)  of  that Regulation and should not influence the assessment and the decision on the illegality of  the specific content. That assessment should be performed solely with reference to the  rules  governing  the  legality of  the  content.
- (137) Compliance  with  the  transparency  obligations  for  the  AI  systems  covered  by  this  Regulation  should  not  be interpreted as indicating that the use of  the AI system or  its output is lawful under  this Regulation or other Union and Member State law and should be without prejudice to other transparency obligations for deployers of AI systems laid  down  in  Union  or  national  law.
- (138) AI is a rapidly developing family of  technologies that requires regulatory oversight and a safe and controlled space for  experimentation,  while  ensuring  responsible  innovation  and  integration  of  appropriate  safeguards  and  risk mitigation  measures.  To  ensure  a  legal  framework  that  promotes  innovation,  is  future-proof  and  resilient  to disruption,  Member  States  should  ensure  that  their  national  competent  authorities  establish  at  least  one  AI regulatory sandbox at national level to facilitate the development and testing of innovative AI systems under strict regulatory  oversight  before  these  systems  are  placed  on  the  market  or  otherwise  put  into  service.  Member  States could also fulfil this obligation through participating in already existing regulatory sandboxes or establishing jointly a sandbox with one or more Member States' competent authorities, insofar as this participation provides equivalent level  of  national  coverage  for  the  participating  Member  States.  AI  regulatory  sandboxes  could  be  established  in physical, digital or hybrid form and may accommodate physical as well as digital products. Establishing authorities should  also  ensure  that  the  AI  regulatory  sandboxes  have  the  adequate  resources  for  their  functioning,  including financial  and  human  resources.
- (139) The  objectives  of  the  AI  regulatory  sandboxes  should  be  to  foster  AI  innovation  by  establishing  a  controlled experimentation  and  testing  environment  in  the  development  and  pre-marketing  phase  with  a  view  to  ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover, the  AI  regulatory  sandboxes  should  aim  to  enhance  legal  certainty  for  innovators  and  the  competent  authorities' oversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory learning  for  authorities  and  undertakings,  including  with  a  view  to  future  adaptions  of  the  legal  framework,  to support  cooperation  and  the  sharing  of  best  practices  with  the  authorities  involved  in  the  AI  regulatory  sandbox, and  to  accelerate  access  to  markets,  including  by  removing  barriers  for  SMEs,  including  start-ups.  AI  regulatory sandboxes  should  be  widely  available  throughout  the  Union,  and  particular  attention  should  be  given  to  their accessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that raise  legal  uncertainty  for  providers  and  prospective  providers  to  innovate,  experiment  with  AI  in  the  Union  and contribute  to  evidence-based  regulatory  learning.  The  supervision  of  the  AI  systems  in  the  AI  regulatory  sandbox should  therefore  cover  their  development,  training,  testing  and  validation  before  the  systems  are  placed  on  the market or put into service, as well as the notion and occurrence of substantial modification that may require a new conformity  assessment  procedure.  Any  significant  risks  identified  during  the  development  and  testing  of  such  AI systems  should  result  in  adequate  mitigation  and,  failing  that,  in  the  suspension  of  the  development  and  testing process. Where appropriate, national competent authorities establishing AI regulatory sandboxes should cooperate with other relevant authorities, including those supervising the protection of fundamental rights, and could allow for the involvement of other actors within the AI ecosystem such as national or European standardisation organisations, notified bodies, testing and  experimentation  facilities, research  and experimentation  labs, European  Digital Innovation Hubs and relevant stakeholder and civil society organisations. To ensure uniform implementation across the  Union  and  economies  of  scale,  it  is  appropriate  to  establish  common  rules  for  the  AI  regulatory  sandboxes' implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes.  AI  regulatory  sandboxes  established  under  this  Regulation  should  be  without  prejudice  to  other  law allowing for the establishment of other sandboxes aiming to ensure compliance with law other than this Regulation. Where appropriate, relevant competent authorities in charge of  those  other  regulatory  sandboxes should  consider the  benefits  of  using  those  sandboxes  also  for  the  purpose  of  ensuring  compliance  of  AI  systems  with  this Regulation. Upon agreement between the national competent authorities and the participants in the AI regulatory sandbox, testing in real world conditions may also be operated and supervised in the framework of the AI regulatory sandbox.
- (140) This  Regulation  should  provide  the  legal  basis  for  the  providers  and  prospective  providers  in  the  AI  regulatory sandbox to use personal data collected for other  purposes for  developing certain  AI systems in  the public  interest within the AI regulatory sandbox, only under specified conditions, in accordance with Article 6(4) and Article 9(2), point  (g),  of  Regulation  (EU)  2016/679,  and  Articles  5,  6  and  10  of  Regulation  (EU)  2018/1725,  and  without prejudice  to  Article  4(2)  and  Article  10  of  Directive  (EU)  2016/680.  All  other  obligations  of  data  controllers  and rights of data subjects under Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remain applicable. In particular, this Regulation should not provide a legal basis in the meaning of Article 22(2), point (b) of Regulation  (EU)  2016/679  and  Article  24(2),  point  (b)  of  Regulation  (EU)  2018/1725.  Providers  and  prospective
providers  in  the  AI  regulatory  sandbox  should  ensure  appropriate  safeguards  and  cooperate  with  the  competent authorities, including by following their guidance and acting expeditiously and in good faith to adequately mitigate any  identified  significant  risks  to  safety,  health,  and  fundamental  rights  that  may  arise  during  the  development, testing  and  experimentation  in  that  sandbox.
- (141) In order to accelerate the process of development and the placing on the market of the high-risk AI systems listed in an annex to this Regulation, it is important that providers or prospective providers of such systems may also benefit from a specific regime for  testing those systems in real world conditions, without participating in an AI regulatory sandbox.  However,  in  such  cases,  taking  into  account  the  possible  consequences  of  such  testing  on  individuals,  it should be ensured that appropriate and sufficient  guarantees and conditions are introduced by this Regulation for providers  or  prospective  providers.  Such  guarantees  should  include,  inter  alia,  requesting  informed  consent  of natural persons to participate in testing in real world conditions, with the exception of  law enforcement where the seeking of  informed consent would prevent the AI system from being tested. Consent of subjects to participate in such  testing  under  this  Regulation  is  distinct  from,  and  without  prejudice  to,  consent  of  data  subjects  for  the processing of  their  personal data under  the relevant data protection law. It is also important to minimise the risks and  enable  oversight  by  competent  authorities  and  therefore  require  prospective  providers  to  have  a  real-world testing plan submitted to competent market surveillance authority, register the testing in dedicated sections in the EU database  subject  to  some  limited  exceptions,  set  limitations  on  the  period  for  which  the  testing  can  be  done  and require  additional  safeguards  for  persons  belonging  to  certain  vulnerable  groups,  as  well  as  a  written  agreement defining  the  roles  and  responsibilities  of  prospective providers and deployers and effective  oversight by competent personnel  involved  in  the  real  world  testing.  Furthermore,  it  is  appropriate  to  envisage  additional  safeguards  to ensure  that  the  predictions,  recommendations  or  decisions  of  the  AI  system  can  be  effectively  reversed  and disregarded  and  that  personal  data  is  protected  and  is  deleted  when  the  subjects  have  withdrawn  their  consent  to participate in the testing without prejudice to their rights as data subjects under  the Union data protection law. As regards transfer of data, it is also appropriate to envisage that data collected and processed for the purpose of testing in  real-world conditions should be transferred to third countries only where appropriate and applicable safeguards under Union law are implemented, in particular in accordance with bases for transfer of personal data under Union law  on  data  protection,  while  for  non-personal  data  appropriate  safeguards  are  put  in  place  in  accordance  with Union law, such as Regulations (EU) 2022/868 ( 42 )  and (EU) 2023/2854 ( 43 )  of  the  European Parliament and of  the Council.
- (142) To  ensure  that  AI  leads  to  socially  and  environmentally  beneficial  outcomes,  Member  States  are  encouraged  to support  and  promote  research  and  development  of  AI  solutions  in  support  of  socially  and  environmentally beneficial  outcomes,  such  as  AI-based  solutions  to  increase  accessibility  for  persons  with  disabilities,  tackle socio-economic inequalities, or meet environmental targets, by allocating sufficient resources, including public and Union  funding,  and,  where  appropriate  and  provided  that  the  eligibility  and  selection  criteria  are  fulfilled, considering in particular  projects which pursue such objectives. Such projects should be based on the principle of interdisciplinary  cooperation  between  AI  developers,  experts  on  inequality  and  non-discrimination,  accessibility, consumer, environmental, and digital rights,  as  well  as  academics.
- (143) In order  to promote and protect innovation, it is important that the interests of SMEs, including start-ups, that are providers or deployers of AI systems are taken into particular account. To that end, Member States should develop initiatives,  which  are  targeted  at  those  operators,  including  on  awareness  raising  and  information  communication. Member States should provide SMEs, including start-ups, that have a registered office or a branch in the Union, with priority access to the AI regulatory sandboxes provided that they fulfil the eligibility conditions and selection criteria and  without  precluding  other  providers  and  prospective  providers  to  access  the  sandboxes  provided  the  same conditions and criteria are fulfilled. Member States should utilise existing channels and where appropriate, establish new  dedicated  channels  for  communication  with  SMEs,  including  start-ups,  deployers,  other  innovators  and,  as appropriate,  local  public  authorities,  to  support  SMEs  throughout  their  development  path  by  providing  guidance and responding to queries about the implementation of this Regulation. Where appropriate, these channels should work  together  to  create  synergies  and  ensure  homogeneity  in  their  guidance  to  SMEs,  including  start-ups,  and deployers. Additionally, Member States should facilitate the participation of SMEs and other relevant stakeholders in the  standardisation  development  processes.  Moreover,  the  specific  interests  and  needs  of  providers  that  are  SMEs,
( 42 ) Regulation  (EU)  2022/868  of  the  European  Parliament  and  of  the  Council  of  30  May  2022  on  European  data  governance  and amending Regulation (EU) 2018/1724 (Data Governance Act) (OJ L 152, , p. 1).
( 43 ) Regulation  (EU)  2023/2854  of  the  European  Parliament  and  of  the  Council  of  13  December  2023  on  harmonised  rules  on  fair access to and use of data and amending Regulation (EU) 2017/2394 and Directive (EU) 2020/1828 (Data Act) (OJ L, 2023/2854, , ELI:  http://data.europa.eu/eli/reg/2023/2854/oj).
including  start-ups,  should  be  taken  into  account  when  notified  bodies  set  conformity  assessment  fees.  The Commission should regularly  assess  the  certification  and  compliance  costs  for  SMEs,  including  start-ups,  through transparent consultations and should work with Member States to lower such costs. For example, translation costs related  to  mandatory  documentation  and  communication  with  authorities  may  constitute  a  significant  cost  for providers and other operators, in particular those of a smaller scale. Member States should possibly ensure that one of  the  languages  determined  and  accepted by  them  for  relevant  providers'  documentation  and  for  communication with operators is one which is broadly understood by the largest possible number of cross-border deployers. In order to address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for the areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement Member States' efforts by providing a single information platform with easy-to-use information with regards to this Regulation for all providers and deployers, by organising appropriate communication campaigns to raise awareness about  the  obligations  arising  from  this  Regulation,  and  by  evaluating  and  promoting  the  convergence  of  best practices in public procurement procedures in relation to AI systems. Medium-sized enterprises which until recently qualified as small enterprises within the meaning of the Annex to Commission Recommendation 2003/361/EC ( 44 ) should have access to those support measures, as those new medium-sized enterprises may sometimes lack the legal resources  and  training  necessary  to  ensure  proper  understanding  of,  and  compliance  with,  this  Regulation.
- (144) In  order  to  promote  and  protect  innovation,  the  AI-on-demand  platform,  all  relevant  Union  funding  programmes and projects, such as Digital Europe Programme, Horizon Europe, implemented by the Commission and the Member States  at  Union  or  national  level  should,  as  appropriate,  contribute  to  the  achievement  of  the  objectives  of  this Regulation.
- (145) In  order  to minimise the risks  to implementation resulting from lack of knowledge and expertise in the market as well as  to facilitate  compliance  of  providers,  in particular  SMEs,  including  start-ups,  and  notified  bodies  with  their obligations  under  this  Regulation,  the  AI-on-demand  platform,  the  European  Digital  Innovation  Hubs  and  the testing  and  experimentation  facilities  established  by  the  Commission  and  the  Member  States  at  Union  or  national level  should  contribute  to  the  implementation  of  this  Regulation.  Within  their  respective  mission  and  fields  of competence, the AI-on-demand platform, the European Digital Innovation Hubs and the testing and experimentation  Facilities  are  able  to  provide  in  particular  technical  and  scientific  support  to  providers  and notified  bodies.
- (146) Moreover, in light of the very small size of some operators and in order to ensure proportionality regarding costs of innovation,  it  is  appropriate  to  allow  microenterprises  to  fulfil  one  of  the  most  costly  obligations,  namely  to establish a quality management system, in a simplified manner  which would reduce the administrative burden and the  costs  for  those  enterprises  without  affecting  the  level  of  protection  and  the  need  for  compliance  with  the requirements  for  high-risk  AI  systems.  The  Commission  should  develop  guidelines  to  specify  the  elements  of  the quality  management  system  to  be  fulfilled  in  this  simplified  manner  by  microenterprises.
- (147) It  is  appropriate  that  the  Commission  facilitates,  to  the  extent  possible,  access  to  testing  and  experimentation facilities  to  bodies,  groups  or  laboratories  established  or  accredited  pursuant  to  any  relevant  Union  harmonisation legislation  and  which  fulfil  tasks  in  the  context  of  conformity  assessment  of  products  or  devices  covered  by  that Union  harmonisation  legislation.  This  is,  in  particular,  the  case  as  regards  expert  panels,  expert  laboratories  and reference  laboratories  in  the  field  of  medical  devices  pursuant  to  Regulations  (EU)  2017/745  and  (EU)  2017/746.
- (148) This  Regulation  should  establish  a  governance  framework  that  both  allows  to  coordinate  and  support  the application of this Regulation at national level, as well as build capabilities at Union level and integrate stakeholders in the field of AI. The effective implementation and enforcement of this Regulation require a governance framework that allows to coordinate and build up central expertise at Union level. The AI Office was established by Commission Decision ( 45 ) and has as its mission to develop Union expertise and capabilities in the field of AI and to contribute to the  implementation of Union law on AI. Member States should facilitate the tasks  of  the AI Office with a view to support the development of Union expertise and capabilities at Union level and to strengthen the functioning of the digital single market. Furthermore, a Board composed of representatives of the Member States, a scientific panel to integrate the scientific community and an advisory forum to contribute stakeholder input to the implementation of this  Regulation,  at  Union  and  national  level,  should  be  established.  The  development  of  Union  expertise  and
( 44 ) Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium-sized enterprises (OJ L 124, , p.  36).
( 45 ) Commission Decision of  establishing the European Artificial  Intelligence  Office  C(2024)  390.
capabilities should also include making use of existing resources and expertise, in particular through synergies with structures built up in the context of the Union level enforcement of other law and synergies with related initiatives at Union  level,  such  as  the  EuroHPC  Joint  Undertaking  and  the  AI  testing  and  experimentation  facilities  under  the Digital  Europe  Programme.
- (149) In  order  to  facilitate  a  smooth,  effective  and  harmonised  implementation  of  this  Regulation  a  Board  should  be established. The Board should reflect the various interests of the AI eco-system and be composed of representatives of  the Member States. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or contributing to guidance on matters related to the implementation of this Regulation, including  on  enforcement  matters,  technical  specifications  or  existing  standards  regarding  the  requirements established  in  this  Regulation  and  providing  advice  to  the  Commission  and  the  Member  States  and  their  national competent authorities on specific questions related to AI. In order  to give some flexibility to Member States in the designation  of  their  representatives  in  the  Board,  such  representatives  may  be  any  persons  belonging  to  public entities  who  should  have  the  relevant  competences  and  powers  to  facilitate  coordination  at  national  level  and contribute to the achievement of the Board's tasks. The Board should establish two standing sub-groups to provide a platform for cooperation and exchange among market surveillance authorities and notifying authorities on issues related,  respectively,  to  market  surveillance  and  notified  bodies.  The  standing  subgroup  for  market  surveillance should act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020. In accordance with Article 33 of that Regulation, the Commission should support the activities  of  the  standing  subgroup  for  market  surveillance  by  undertaking  market  evaluations  or  studies,  in particular  with  a  view  to  identifying  aspects  of  this  Regulation  requiring  specific  and  urgent  coordination  among market surveillance authorities. The Board may establish other standing or temporary sub-groups as appropriate for the  purpose  of  examining  specific  issues.  The  Board  should  also  cooperate,  as  appropriate,  with  relevant  Union bodies, experts groups and networks active in the context of relevant Union law, including in particular those active under  relevant  Union  law on  data,  digital  products  and  services.
- (150) With a view to ensuring the involvement of stakeholders in the implementation and application of this Regulation, an advisory forum should be established to advise and provide technical expertise to the Board and the Commission. To ensure a varied and balanced stakeholder representation between commercial and non-commercial interest and, within  the  category  of  commercial  interests,  with  regards  to  SMEs  and  other  undertakings,  the  advisory  forum should comprise inter alia industry, start-ups, SMEs, academia, civil society, including the social partners, as well as the  Fundamental  Rights  Agency,  ENISA,  the  European  Committee  for  Standardization  (CEN),  the  European Committee  for  Electrotechnical  Standardization  (CENELEC)  and  the  European  Telecommunications  Standards Institute  (ETSI).
- (151) To support the implementation and enforcement of this Regulation, in particular the monitoring activities of the AI Office  as  regards  general-purpose  AI  models,  a  scientific  panel  of  independent  experts  should  be  established.  The independent  experts  constituting  the  scientific  panel  should  be  selected  on  the  basis  of  up-to-date  scientific  or technical  expertise  in  the  field  of  AI  and  should  perform  their  tasks  with  impartiality,  objectivity  and  ensure  the confidentiality of information and data obtained in carrying out their tasks and activities. To allow the reinforcement of  national  capacities  necessary  for  the  effective  enforcement  of  this  Regulation,  Member  States  should  be  able  to request  support  from  the  pool  of  experts  constituting  the  scientific  panel  for  their  enforcement  activities.
- (152) In order  to support adequate enforcement as regards AI systems and reinforce the capacities of the Member States, Union AI testing  support  structures  should  be  established  and  made  available  to  the  Member  States.
- (153) Member States hold a key role in the application and enforcement of this Regulation. In that respect, each Member State  should  designate  at  least  one  notifying  authority  and  at  least  one  market  surveillance  authority  as  national competent  authorities  for  the  purpose  of  supervising  the  application  and  implementation  of  this  Regulation. Member  States  may  decide  to  appoint  any  kind  of  public  entity  to  perform  the  tasks  of  the  national  competent authorities  within  the  meaning  of  this  Regulation,  in  accordance  with  their  specific  national  organisational characteristics and needs. In order to increase organisation efficiency on the side of Member States and to set a single point of contact vis-à-vis the public and other counterparts at Member State and Union levels, each Member State should designate a  market  surveillance  authority  to  act  as  a  single  point  of  contact.
- (154) The national competent authorities should exercise their powers independently, impartially and without bias, so as to safeguard the principles of objectivity of their activities and tasks and to ensure the application and implementation of this Regulation. The members of these authorities should refrain from any action incompatible with their  duties  and  should  be  subject  to  confidentiality  rules  under  this  Regulation.
- (155) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action  in  a  timely  manner,  all  providers  should  have  a  post-market  monitoring  system  in  place.  Where  relevant, post-market monitoring should include an analysis of the interaction with other AI systems including other devices and  software.  Post-market  monitoring  should  not  cover  sensitive  operational  data  of  deployers  which  are  law enforcement authorities. This system is also key to ensure that the possible risks emerging from AI systems which continue to 'learn' after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents resulting from the use of their AI systems, meaning incident or malfunctioning leading to death or serious  damage  to  health,  serious  and  irreversible  disruption  of  the  management  and  operation  of  critical infrastructure,  infringements  of  obligations  under  Union  law  intended  to  protect  fundamental  rights  or  serious damage to property or  the  environment.
- (156) In  order  to  ensure  an  appropriate  and  effective  enforcement  of  the  requirements  and  obligations  set  out  by  this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of products established  by  Regulation  (EU)  2019/1020  should  apply  in  its  entirety.  Market  surveillance  authorities  designated pursuant to this Regulation should have all enforcement powers laid down in this Regulation and in Regulation (EU) 2019/1020 and should exercise their powers and carry out their duties independently, impartially and without bias. Although the majority of AI systems are not subject to specific requirements and obligations under this Regulation, market  surveillance  authorities  may  take  measures  in  relation  to  all  AI  systems  when  they  present  a  risk  in accordance with this Regulation. Due to the specific nature of Union institutions, agencies and bodies falling within the scope of this Regulation, it is appropriate to designate the European Data Protection Supervisor as a competent market surveillance authority for  them. This should be without prejudice to the designation of national competent authorities by the Member States. 