Those tasks are of such narrow and limited nature that they pose only limited risks which are not increased through the use of an AI system in a context that is listed as a high-risk use in an annex to this Regulation. The second condition should be
( 30 ) Regulation (EU) 2018/1139 of  the European Parliament and of  the Council of 4 July 2018 on common rules in the field of civil aviation  and  establishing  a  European  Union  Aviation  Safety  Agency,  and  amending  Regulations  (EC)  No  2111/2005,  (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of  the  Council,  and  repealing  Regulations  (EC)  No  552/2004  and  (EC)  No  216/2008  of  the  European  Parliament  and  of  the Council and Council Regulation  (EEC) No 3922/91 (OJ L 212, , p. 1).
( 31 ) Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for  motor  vehicles  and  their  trailers,  and  systems,  components  and  separate  technical  units  intended  for  such  vehicles,  as  regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the  European  Parliament  and  of  the  Council  and  Commission  Regulations  (EC)  No  631/2009,  (EU)  No  406/2010,  (EU) No  672/2010,  (EU)  No  1003/2010,  (EU)  No  1005/2010,  (EU)  No  1008/2010,  (EU)  No  1009/2010,  (EU)  No  19/2011,  (EU) No  109/2011,  (EU)  No  458/2011,  (EU)  No  65/2012,  (EU)  No  130/2012,  (EU)  No  347/2012,  (EU)  No  351/2012,  (EU) No 1230/2012 and (EU) 2015/166 (OJ L 325, , p. 1).
that the task performed by the AI system is intended to improve the result of a previously completed human activity that may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those characteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk. That condition would, for example, apply to AI systems that are intended to improve the language used in previously drafted  documents,  for  example  in  relation  to  professional  tone,  academic  style  of  language  or  by  aligning  text  to a  certain brand messaging. The third condition should be that the AI system is intended to detect decision-making patterns  or  deviations  from  prior  decision-making  patterns.  The  risk  would  be  lowered  because  the  use  of  the  AI system  follows  a  previously  completed  human  assessment  which  it  is  not  meant  to  replace  or  influence,  without proper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher, can be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential inconsistencies or anomalies. The fourth condition should be that the AI system is intended to perform a task that is only preparatory to an assessment relevant for the purposes of the AI systems listed in an annex to this Regulation, thus  making  the  possible  impact  of  the  output  of  the  system  very  low  in  terms  of  representing  a  risk  for  the assessment  to  follow.  That  condition  covers,  inter  alia,  smart  solutions  for  file  handling,  which  include  various functions from indexing, searching, text and speech processing or linking data to other data sources, or AI systems used for translation of initial documents. In any case, AI systems used in high-risk use-cases listed in an annex to this Regulation should be considered to pose significant risks of harm to the health, safety or fundamental rights if the AI system  implies  profiling  within  the  meaning  of  Article  4,  point  (4)  of  Regulation  (EU)  2016/679  or  Article  3, point  (4)  of  Directive  (EU)  2016/680  or  Article  3,  point  (5)  of  Regulation  (EU)  2018/1725.  To  ensure  traceability and transparency, a provider who considers that an AI system is not high-risk on the basis of the conditions referred to  above should draw up documentation of the assessment before that system is placed on the market or put into service  and  should  provide  that  documentation  to  national  competent  authorities  upon  request.  Such  a  provider should  be  obliged  to  register  the  AI  system  in  the  EU  database  established  under  this  Regulation.  With  a  view  to providing further guidance for the practical implementation of the conditions under which the AI systems listed in an annex to this Regulation are, on an exceptional basis, non-high-risk, the Commission should, after consulting the Board, provide guidelines specifying that practical implementation, completed by a comprehensive list of practical examples of use cases  of  AI  systems  that  are  high-risk  and  use  cases  that  are  not.
- (54) As  biometric  data  constitutes  a  special  category  of  personal  data,  it  is  appropriate  to  classify  as  high-risk  several critical-use  cases  of  biometric  systems,  insofar  as  their  use  is  permitted  under  relevant  Union  and  national  law. Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased  results  and  entail  discriminatory  effects.  The  risk  of  such  biased  results  and  discriminatory  effects  is particularly  relevant  with  regard  to  age,  ethnicity,  race,  sex  or  disabilities.  Remote  biometric  identification  systems should  therefore  be  classified  as  high-risk  in  view  of  the  risks  that  they  pose.  Such  a  classification  excludes  AI systems  intended  to  be  used  for  biometric  verification,  including  authentication,  the  sole  purpose  of  which  is  to confirm that a specific natural person is who that person claims to be and to confirm the identity of a natural person for the sole purpose of having access to a service, unlocking a device or having secure access to premises. In addition, AI  systems  intended  to  be  used  for  biometric  categorisation  according  to  sensitive  attributes  or  characteristics protected under Article 9(1) of Regulation (EU) 2016/679 on the basis of biometric data, in so far as these are not prohibited  under  this  Regulation,  and  emotion  recognition  systems  that  are  not  prohibited  under  this  Regulation, should be classified as high-risk. Biometric systems which are intended to be used solely for the purpose of enabling cybersecurity  and  personal  data  protection  measures  should  not  be  considered  to  be  high-risk  AI  systems.
- (55) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of critical digital infrastructure as listed in point (8) of the Annex to Directive (EU) 2022/2557, road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable  disruptions  in  the  ordinary  conduct  of  social  and  economic  activities.  Safety  components  of  critical infrastructure,  including  critical  digital  infrastructure,  are  systems  used  to  directly  protect  the  physical  integrity  of critical infrastructure or the health and safety of persons and property but which are not necessary in order for the
system to function. The  failure  or  malfunctioning  of  such  components  might  directly  lead  to risks  to  the  physical integrity  of  critical  infrastructure  and  thus  to  risks  to  health  and  safety  of  persons  and  property.  Components intended to be used solely for cybersecurity purposes should not qualify as safety components. Examples of safety components  of  such  critical  infrastructure  may  include  systems  for  monitoring  water  pressure  or  fire  alarm controlling  systems  in  cloud  computing  centres.
- (56) The deployment of AI systems in education is important to promote high-quality digital education and training and to allow all learners and teachers to acquire and share the necessary digital skills and competences, including media literacy, and critical thinking, to take an active part in the economy, society, and in democratic processes. However, AI systems used in education or vocational training, in particular for determining access or admission, for assigning persons  to  educational  and  vocational  training  institutions  or  programmes  at  all  levels,  for  evaluating  learning outcomes of persons, for assessing the appropriate level of education for an individual and materially influencing the level of education and training that individuals will receive or will be able to access or for monitoring and detecting prohibited behaviour of students during tests should be classified as high-risk AI systems, since they may determine the  educational  and  professional  course  of  a  person's  life  and  therefore  may  affect  that  person's  ability  to  secure a  livelihood.  When  improperly designed and used, such systems may be particularly intrusive and may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of  discrimination,  for  example  against  women,  certain  age  groups,  persons  with  disabilities,  or  persons  of  certain racial  or  ethnic  origins  or  sexual  orientation.
- (57) AI  systems  used  in  employment,  workers  management  and  access  to  self-employment,  in  particular  for  the recruitment  and  selection  of  persons,  for  making  decisions  affecting  terms  of  the  work-related  relationship, promotion and termination of work-related contractual relationships, for allocating tasks on the basis of individual behaviour, personal traits or characteristics and for monitoring or evaluation of persons in work-related contractual relationships,  should also  be  classified  as  high-risk,  since  those  systems  may  have  an  appreciable  impact on  future career  prospects,  livelihoods  of  those  persons  and  workers'  rights.  Relevant  work-related  contractual  relationships should, in a meaningful manner, involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021. Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of such persons may also undermine their  fundamental rights to data protection and privacy.
- (58) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential  private  and  public  services  and  benefits  necessary  for  people  to fully  participate  in  society or  to  improve one's  standard  of  living.  In  particular,  natural  persons  applying  for  or  receiving  essential  public  assistance  benefits and  services  from  public  authorities  namely  healthcare  services,  social  security  benefits,  social  services  providing protection  in  cases  such  as  maternity,  illness,  industrial  accidents,  dependency or  old  age  and  loss  of  employment and social and housing assistance, are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities. If AI systems are used for determining whether such benefits and services should  be  granted,  denied,  reduced,  revoked  or  reclaimed  by  authorities,  including  whether  beneficiaries  are legitimately entitled to such benefits or services, those systems may have a significant impact on persons' livelihood and may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity or  an  effective  remedy  and  should  therefore  be  classified  as  high-risk.  Nonetheless,  this  Regulation  should  not hamper  the  development  and  use  of  innovative  approaches  in  the  public  administration,  which  would  stand  to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal  and  natural  persons.  In  addition,  AI  systems  used  to  evaluate  the  credit  score  or  creditworthiness  of  natural persons should be classified as high-risk AI systems, since they determine those persons' access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for those purposes may  lead  to  discrimination  between  persons  or  groups  and  may  perpetuate  historical  patterns  of  discrimination, such as that based on racial or ethnic origins, gender, disabilities, age or sexual orientation, or may create new forms of discriminatory impacts. However, AI systems provided for by Union law for the purpose of detecting fraud in the offering of financial services and for prudential purposes to calculate credit institutions' and insurance undertakings' capital requirements should not be considered to be high-risk under this Regulation. Moreover, AI systems intended
to be used for risk assessment and pricing in relation to natural persons for health and life insurance can also have a  significant  impact  on  persons'  livelihood  and  if  not  duly  designed,  developed  and  used,  can  infringe  their fundamental rights and can lead to serious consequences for  people's life and health, including financial exclusion and  discrimination.  Finally,  AI  systems  used  to  evaluate  and  classify  emergency  calls  by  natural  persons  or  to dispatch or establish priority in the dispatching of emergency first response services, including by police, firefighters and medical aid, as well as of emergency healthcare patient triage systems, should also be classified as high-risk since they  make decisions  in very critical  situations  for  the  life  and  health  of  persons  and  their  property.
- (59) Given their role and responsibility, actions by law enforcement authorities involving certain uses of AI systems are characterised  by  a  significant  degree  of  power  imbalance  and  may  lead  to  surveillance,  arrest  or  deprivation  of a  natural  person's  liberty  as  well  as  other  adverse  impacts  on  fundamental  rights  guaranteed  in  the  Charter.  In particular, if the AI system is not trained with high-quality data, does not meet adequate requirements in terms of its performance, its accuracy or  robustness, or  is not properly designed and tested before being put on the market or otherwise  put  into  service,  it  may  single  out  people  in  a  discriminatory  or  otherwise  incorrect  or  unjust  manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to  a  fair  trial  as  well  as  the  right  of  defence  and  the  presumption  of  innocence,  could  be  hampered,  in  particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate to classify as high-risk, insofar as their use is permitted under relevant Union and national law, a number of AI systems intended  to  be  used  in  the  law  enforcement  context  where  accuracy,  reliability  and  transparency  is  particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the nature  of  the  activities  and  the  risks  relating  thereto,  those  high-risk  AI  systems  should  include  in  particular  AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices, or agencies in support of law enforcement authorities for assessing the risk of a natural person to become a victim of criminal offences, as polygraphs and similar tools, for the evaluation of the reliability of evidence in in the course of investigation or prosecution of criminal offences, and, insofar as not prohibited under this Regulation, for assessing the risk of a natural person offending or reoffending not solely on the basis of the profiling of natural persons or the assessment of personality traits and characteristics or  the past criminal behaviour of natural persons or groups, for profiling  in  the  course  of  detection,  investigation  or  prosecution  of  criminal  offences.  AI  systems  specifically intended to be used for administrative proceedings by tax and customs authorities as well as by financial intelligence units carrying out administrative tasks analysing information pursuant to Union anti-money laundering law should not  be  classified  as  high-risk  AI  systems  used  by  law  enforcement  authorities  for  the  purpose  of  prevention, detection,  investigation  and  prosecution  of  criminal  offences.  The  use  of  AI  tools  by  law  enforcement  and  other relevant authorities should not become a factor of inequality, or exclusion. The impact of the use of AI tools on the defence rights of suspects should not be ignored, in particular the difficulty in obtaining meaningful information on the  functioning  of  those  systems  and  the  resulting  difficulty  in  challenging  their  results  in  court,  in  particular  by natural  persons  under  investigation.
- (60) AI systems used in migration, asylum and border control management affect persons who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. The accuracy,  non-discriminatory  nature  and  transparency  of  the  AI  systems  used  in  those  contexts  are  therefore particularly  important  to  guarantee  respect  for  the  fundamental  rights  of  the  affected  persons,  in  particular  their rights  to  free  movement,  non-discrimination,  protection  of  private  life  and  personal  data,  international  protection and good administration. It is therefore appropriate to classify as high-risk,  insofar  as their  use is  permitted under relevant Union and national law, AI systems intended to be used by or on behalf of competent public authorities or by  Union institutions,  bodies,  offices  or  agencies  charged  with  tasks  in  the  fields  of  migration,  asylum  and  border control management as polygraphs and similar  tools, for assessing certain risks posed by natural persons entering the  territory  of  a  Member  State  or  applying  for  visa  or  asylum,  for  assisting  competent  public  authorities  for  the examination, including related assessment of the reliability of evidence, of applications for asylum, visa and residence permits  and  associated  complaints  with  regard  to  the  objective  to  establish  the  eligibility  of  the  natural  persons applying  for  a  status,  for  the  purpose  of  detecting,  recognising  or  identifying  natural  persons  in  the  context  of migration,  asylum  and  border  control  management,  with  the  exception  of  verification  of  travel  documents.  AI systems in the area of migration, asylum and border control management covered by this Regulation should comply with the relevant procedural requirements set by the Regulation (EC) No 810/2009 of the European Parliament and
- of the Council ( 32 ),  the Directive 2013/32/EU of the European Parliament and of the Council ( 33 ),  and other relevant Union law. The use of AI systems in migration, asylum and border control management should, in no circumstances, be  used  by  Member  States  or  Union  institutions,  bodies,  offices  or  agencies  as  a  means  to  circumvent  their international  obligations  under  the  UN  Convention  relating  to  the  Status  of  Refugees  done  at  Geneva  on  28  July 1951  as  amended  by  the  Protocol  of  31  January  1967.  Nor  should  they  be  used  to  in  any  way  infringe  on  the principle of non-refoulement, or  to deny safe and effective legal avenues into the territory of  the Union, including the  right  to  international  protection.
