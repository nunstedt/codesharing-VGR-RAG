High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (feedback loops), and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.
- 5. High-risk  AI  systems  shall  be  resilient  against  attempts  by  unauthorised  third  parties  to  alter  their  use,  outputs  or performance by exploiting system vulnerabilities.
