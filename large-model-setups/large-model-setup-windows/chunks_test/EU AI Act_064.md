The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not  warranted  on  account  of  the  nature  of  the  high-risk  AI  system,  it  shall  be  affixed  to  the  packaging  or  to  the accompanying documentation, as appropriate.
- 4. Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43. The identification number of the notified body shall be affixed by the body itself or, under its instructions, by the provider or by the provider's authorised representative. The identification number  shall  also  be  indicated  in  any  promotional  material  which  mentions  that  the  high-risk  AI  system  fulfils  the requirements  for  CE  marking.
- 5. Where high-risk AI systems are subject to other Union law which also provides for the affixing of the CE marking, the CE marking shall indicate that  the  high-risk  AI  system  also  fulfil  the  requirements  of  that  other  law.
## Article  49
## Registration
- 1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk  AI  systems  referred  to  in  point  2  of  Annex  III,  the  provider  or,  where  applicable,  the  authorised  representative shall  register  themselves  and  their  system  in  the  EU  database  referred  to  in  Article  71.
- 2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not high-risk  according  to  Article  6(3),  that  provider  or,  where  applicable,  the  authorised  representative  shall  register themselves  and  that  system  in  the  EU  database  referred  to  in  Article  71.
- 3. Before  putting  into  service  or  using  a  high-risk  AI  system  listed  in  Annex  III,  with  the  exception  of  high-risk  AI systems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or persons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to in  Article  71.
- 4. For  high-risk  AI  systems  referred  to  in  points  1,  6  and  7  of  Annex  III,  in  the  areas  of  law  enforcement,  migration, asylum  and  border  control  management,  the  registration  referred  to  in  paragraphs  1,  2  and  3  of  this  Article  shall  be  in a secure non-public section of the EU database referred to in Article 71 and shall include only the following information, as applicable,  referred  to  in:
- (a) Section  A,  points  1  to  10,  of  Annex  VIII,  with  the  exception  of  points  6,  8  and  9;
- (b) Section  B,  points  1  to  5,  and  points  8  and  9  of  Annex  VIII;
- (c) Section  C,  points  1  to  3,  of  Annex  VIII;
- (d) points  1,  2,  3  and  5,  of  Annex  IX.
Only  the  Commission  and  national  authorities  referred  to  in  Article  74(8)  shall  have  access  to  the  respective  restricted sections  of  the  EU  database  listed  in  the  first  subparagraph  of  this  paragraph.
- 5. High-risk  AI  systems  referred  to  in  point  2  of  Annex  III  shall  be  registered  at  national  level.
## CHAPTER IV
## TRANSPARENCY OBLIGATIONS FOR PROVIDERS AND DEPLOYERS OF CERTAIN AI SYSTEMS
## Article  50
## Transparency obligations for  providers and deployers of certain AI systems
- 1. Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed in such  a  way  that  the  natural  persons  concerned  are  informed  that  they  are  interacting  with  an  AI  system,  unless  this  is obvious from the point of  view of a  natural  person who  is  reasonably  well-informed,  observant  and  circumspect,  taking into account the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third  parties,  unless  those  systems  are  available  for  the  public  to  report  a  criminal  offence.
- 2. Providers  of  AI  systems,  including  general-purpose  AI  systems,  generating  synthetic  audio,  image,  video  or  text content,  shall  ensure  that  the  outputs  of  the  AI  system  are  marked  in  a  machine-readable  format  and  detectable  as artificially generated or manipulated. Providers shall ensure their technical solutions are effective, interoperable, robust and reliable as far as this is technically feasible, taking into account the specificities and limitations of various types of content, the  costs  of  implementation  and  the  generally  acknowledged  state  of  the  art,  as  may  be  reflected  in  relevant  technical standards. This obligation shall not apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter  the input data provided by the deployer or the semantics thereof, or where authorised by law to detect,  prevent,  investigate  or  prosecute  criminal  offences.
- 3. Deployers  of  an  emotion  recognition  system  or  a  biometric  categorisation  system  shall  inform  the  natural  persons exposed thereto of  the  operation  of  the  system,  and  shall  process  the  personal  data  in  accordance  with  Regulations  (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obligation shall not apply to AI systems used  for  biometric  categorisation  and  emotion  recognition,  which  are  permitted  by  law  to  detect,  prevent  or  investigate criminal  offences,  subject  to  appropriate  safeguards  for  the  rights  and  freedoms  of  third  parties,  and  in  accordance  with Union law.
- 4. Deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shall disclose  that  the  content  has  been  artificially  generated  or  manipulated.  This  obligation  shall  not  apply  where  the  use  is authorised by law to detect, prevent, investigate or prosecute criminal offence. Where the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper  the  display or  enjoyment of  the  work.
Deployers of an AI system that generates or manipulates text which is published with the purpose of informing the public on matters of public interest shall disclose that the text has been artificially generated or manipulated. This obligation shall not  apply  where  the  use  is  authorised  by  law  to  detect,  prevent,  investigate  or  prosecute  criminal  offences  or  where  the AI-generated  content  has  undergone  a  process  of  human  review  or  editorial  control  and  where  a  natural  or  legal  person holds  editorial  responsibility  for  the  publication  of  the  content.
- 5. The information referred to in paragraphs 1 to 4 shall be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of  the first interaction or exposure. The information shall conform to the applicable  accessibility  requirements.
- 6. Paragraphs  1  to  4  shall  not  affect  the  requirements  and  obligations  set  out  in  Chapter  III,  and  shall  be  without prejudice  to  other  transparency obligations  laid  down  in  Union  or  national  law  for  deployers  of  AI  systems.
- 7. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation  of  the  obligations  regarding  the  detection  and  labelling  of  artificially  generated  or  manipulated  content. The Commission may adopt implementing acts to approve those codes of practice in accordance with the procedure laid down in Article 56 (6). If  it  deems  the  code is  not  adequate,  the  Commission  may adopt an implementing act  specifying common rules  for  the  implementation  of  those  obligations  in  accordance  with  the  examination  procedure  laid  down  in Article  98(2).
## CHAPTER V
## GENERAL-PURPOSE AI MODELS
## SECTION 1
## Classification  rules
## Article  51
## Classification of  general-purpose  AI models as general-purpose AI models with systemic risk
- 1. A general-purpose AI model shall be classified as a general-purpose AI model with systemic risk if it meets any of the following  conditions:
- (a) it  has  high  impact  capabilities  evaluated  on  the  basis  of  appropriate  technical  tools  and  methodologies,  including indicators  and  benchmarks;
- (b) based  on  a  decision  of  the  Commission, ex  officio or  following  a  qualified  alert  from  the  scientific  panel,  it  has capabilities  or  an  impact  equivalent  to  those  set  out  in  point  (a)  having  regard  to  the  criteria  set  out  in  Annex  XIII.
- 2. A general-purpose AI model shall be presumed to have high impact capabilities pursuant to paragraph 1, point (a), when the cumulative  amount of  computation  used  for  its  training  measured  in  floating  point  operations  is  greater  than 10 25 .
- 3. The  Commission  shall  adopt  delegated  acts  in  accordance  with  Article  97  to  amend  the  thresholds  listed  in paragraphs 1 and 2 of this Article, as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect  the  state  of  the  art.
## Article  52
## Procedure
- 1. Where a general-purpose AI model meets the condition referred to in Article 51(1), point (a), the relevant provider shall notify the Commission without delay and in any event within two weeks after that requirement is met or it becomes known  that  it  will  be  met.  That  notification  shall  include  the  information  necessary  to  demonstrate  that  the  relevant requirement has been met. If the Commission becomes aware of a general-purpose AI model presenting systemic risks of which it  has  not  been  notified,  it  may  decide  to  designate  it  as  a  model  with  systemic  risk.
- 2. The  provider  of  a  general-purpose  AI  model  that  meets  the  condition  referred  to  in  Article  51(1),  point  (a),  may present, with its notification, sufficiently substantiated arguments to demonstrate that, exceptionally, although it meets that requirement, the general-purpose AI model does not present, due to its specific characteristics, systemic risks and therefore should  not  be  classified  as  a  general-purpose  AI  model  with  systemic  risk.
- 3. Where  the  Commission  concludes  that  the  arguments  submitted  pursuant  to  paragraph  2  are  not  sufficiently substantiated and the relevant provider  was not able to demonstrate that the general-purpose AI model does not present, due to its specific characteristics, systemic risks, it shall reject those arguments, and the general-purpose AI model shall be considered  to  be  a  general-purpose  AI  model  with  systemic  risk.
- 4. The  Commission  may  designate  a  general-purpose  AI  model  as  presenting  systemic  risks, ex  officio or  following a qualified alert from the scientific panel pursuant to Article 90(1), point (a), on the basis of criteria set out in Annex XIII.
The  Commission is  empowered to adopt  delegated  acts  in  accordance  with  Article  97  in  order  to  amend  Annex  XIII  by specifying  and  updating  the  criteria  set  out  in  that  Annex.
- 5. Upon a reasoned request of a provider whose model has been designated as a general-purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take the request into account and may decide to reassess whether the general-purpose AI model can still be considered to present systemic risks on the basis of the criteria set out in Annex XIII. Such a request shall contain objective, detailed and new reasons that have arisen since the designation decision. Providers may request  reassessment  at  the  earliest  six  months  after  the  designation  decision.  Where  the  Commission,  following  its reassessment, decides to maintain the designation as a general-purpose AI model with systemic risk, providers may request reassessment  at  the  earliest  six  months  after  that  decision.
- 6. The Commission shall ensure that a list of general-purpose AI models with systemic risk is published and shall keep that  list  up  to  date,  without  prejudice  to  the  need  to  observe  and  protect  intellectual  property  rights  and  confidential business  information  or  trade  secrets  in  accordance  with  Union  and  national  law.
## SECTION 2
## Obligations for  providers  of  general-purpose  AI  models
## Article  53
## Obligations for  providers of general-purpose AI models
- 1. Providers  of  general-purpose  AI  models  shall:
- (a) draw up and keep up-to-date the technical documentation of the model, including its training and testing process and the results of its evaluation, which shall contain, at a minimum, the information set out in Annex XI for the purpose of providing  it,  upon  request,  to  the  AI  Office  and  the  national  competent  authorities;
- (b) draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to integrate  the  general-purpose  AI  model  into  their  AI  systems.  Without  prejudice  to  the  need  to  observe  and  protect intellectual  property  rights  and  confidential  business  information  or  trade  secrets  in  accordance  with  Union  and national  law,  the  information  and  documentation  shall:
- (i) enable providers of AI systems  to  have a  good  understanding  of the capabilities and  limitations of the general-purpose AI  model and to comply with their obligations pursuant to this  Regulation;  and
- (ii) contain,  at  a  minimum,  the  elements  set  out  in  Annex  XII;
- (c) put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply with,  including  through  state-of-the-art  technologies,  a  reservation  of  rights  expressed  pursuant  to  Article  4(3)  of Directive  (EU)  2019/790;
- (d) draw  up  and  make  publicly  available  a  sufficiently  detailed  summary  about  the  content  used  for  training  of  the general-purpose  AI  model,  according  to  a  template  provided  by  the  AI  Office.
- 2. The obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of AI models that are released under  a  free  and  open-source  licence  that  allows  for  the  access,  usage,  modification,  and  distribution  of  the  model,  and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are  made  publicly  available.  This  exception  shall  not  apply  to  general-purpose  AI  models  with  systemic  risks.
- 3. Providers  of  general-purpose  AI  models  shall  cooperate  as  necessary  with  the  Commission  and  the  national competent authorities in  the  exercise  of  their  competences  and  powers  pursuant  to  this  Regulation.
