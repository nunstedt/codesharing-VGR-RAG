- (31) As clarified in recital 25 AI Act, the AI Act aims to support innovation and recognises the importance of scientific research in advancing AI technologies and contributing to scientific progress and innovation. Article 2(6) AI Act therefore provides an exclusion
23 See Recital 22 AI Act.
24 See Recital 22 and Article 74(8) AI Act.
for 'AI systems or AI models, including their outputs, specifically developed and put into service for the sole purpose of scientific research and development'.
For example, research into cognitive and behavioural responses to AI-driven subliminal or  deceptive  stimuli  can  provide  valuable  insights  into  human-AI  interactions, informing  safer  and  more  effective  AI  applications  in  the  future.  Such  research  is permitted,  since  it  is  excluded  from  the  scope  of  the  AI  Act,  notwithstanding  the prohibition in Article 5(1)(a) AI Act.
- (32) The exclusion in Article 2(8) AIA Act is, however, without prejudice to the obligation to  comply with the AI Act where an AI system is placed on the market or put into service as a result of such research and development activity. 25 Testing in real-world conditions within the meaning of the AI Act 26 is also not covered by that exclusion.
For example, a municipality wishing to test facial recognition software using a RBI system in the streets during carnival recruits volunteers to be identified by the system in real-world conditions. Because real-world testing does not fall within the exclusion of  Article  2(8)  AI  Act,  the  planned  testing  must  be  fully  compliant  with  the requirements  for  RBI  systems  in  the  AI  Act,  unless  the  system  is  tested  in  an  AI regulatory sandbox or in accordance with the special regime for testing in real world conditions outside the sandbox, as provided for in Articles 60 and 61 AI Act. 27
- (33) In any event, any research and development activity (including when excluded from the scope of the AI Act) should be carried out in accordance with recognised ethical and professional standards for scientific research and should be conducted in accordance with applicable Union law 28 (e.g., data protection law that remains applicable).
## . Personal non-professional activity
- (34) Article 2(10) AI Act provides that the AI Act 'does not apply to obligations of deployers who  are  natural  persons  using  systems  in  the  course  of  a  purely  personal  nonprofessional activity'. The definition of deployer also excludes users engaged in such activities (see section . above). Any activity through which a natural person gains an economic benefit on a regular basis or is otherwise involved in a professional, business, trade,  occupational  or  freelance  activity  should  be  considered  as  a  'professional' activity. The specification of 'personal' is a qualifier of non-professional, meaning that the person should act in both a personal and a non-professional capacity. The exclusion
25 Recital 25 AI Act.
26 According to Article 3(57) AI Act, 'testing in real-world conditions' means the temporary testing of an AI system for its intended purpose in real-world conditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust data and to assessing and verifying the conformity of the AI system with the requirements of this Regulation. The AI Act provides a special regime for such testing in real-world conditions which does not qualify as placing the AI system on the market or putting it into service within the meaning of this Regulation, provided that all the conditions laid down in Articles 57 or 60 are fulfilled, including obtaining free and informed consent from the persons participating in the testing etc.; see Article 60 AI Act.
27 The AI Act contains detailed and specific obligations for AI Regulatory Sandboxes and real-world testing. See Article 57 AI Act et seq. 28 Recital 25 AI Act.
should therefore, for example, not encompass criminal activities since these should not be considered purely personal.
For example, an individual using a facial recognition system at home (e.g., to control access and to monitor for safety the entrance to the home) would fall under the exclusion of Article 2(10) AI Act and, hence, would not be subject to the obligations for deployers under the AI Act, even in cases where it is required to transmit (parts of) the footage to law enforcement authorities.
By contrast,  a  natural  person  using  an  AI  system  for  professional  activities  such  as freelancers,  journalists,  doctors,  etc.  would  need  to  comply  with  the  obligations  for deployers of facial recognition systems under the AI Act. Any use by natural persons where  they  are  acting  on  behalf  or  under  the  authority  of  a  deployer  acting  in  a professional capacity will also fall within the scope of the AI Act.
Furthermore, criminal activities cannot be considered purely personal activities, even if no economic benefit is sought or attained. For other unlawful activities, such as noncompliance with consumer protection or data protection law and national administrative legislation, the exclusion in the AI Act applies, but the other relevant legal frameworks continue to apply).
- (35) The  exclusion  in  Article  2(10)  AI  Act  applies  only  as  regards  the  obligations  of deployers when using the system for purely personal non-professional activities. The system as such remains within the scope of the AI Act as regards the obligations of providers placing the system on the market or putting it into service, other professional deployers, and other responsible actors, such as importers and distributors.
For example, an emotion recognition system, if intended to be used by natural persons for  purely  personal  non-professional  activities,  remains  a  high-risk  AI  system  as classified in Article 6 AI Act and must be fully in compliance with the AI Act. At the same time the deployer that uses it for purely personal non-professional use (e.g. an autistic person) will not be covered by specific obligations for deployers under the AI Act and the use would be out of scope.
## . AI systems released under free and open source licences
- (36) According to Article 2(12) AI Act, the AI Act does not apply to AI systems released under free and open-source licences 29 , unless they are placed on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 (prohibited AI practices) or Article 50 AI Act (transparency obligations for certain AI systems). This means that providers of AI systems cannot benefit from this exclusion if the AI system they place on the market or put into service constitutes a prohibited practice under Articles 5 AI Act.
29 Recital 102 AI Act describes that a release of software and data under free and open-source licence 'allows them to be openly shared and where users can freely access, use, modify and redistribute them or modified versions thereto'.
## . Interplay  of  the  prohibitions  with  the  requirements  for  high-risk  AI systems
- (37) The AI practices prohibited by Article 5 AI Act should be considered in relation to the AI systems classified as high-risk in accordance with Article 6 AI Act, in particular those listed in Annex III. 30 That is because the use of AI systems classified as high-risk may in some cases qualify as a prohibited practice in specific instances if all conditions under one or more of the prohibitions in Article 5 AI Act are fulfilled. Conversely, most AI systems that fall under an exception from a prohibition listed in Article 5 AI Act will qualify as high-risk.
For example, emotion recognition systems, where they do not fulfil the conditions for the prohibition in Article 5(1)(f) AI Act, classify as high-risk AI systems according to Article 6(2) and  Annex III, point (1)(c) AI Act. Similarly, certain AI-based scoring system,  such  as  those  used  for  credit-scoring  or  assessing  risk  in  health  and  life insurance,  will  be  considered  high-risk  AI  systems  where  they  do  not  fulfil  the conditions for the prohibition listed in Article 5(1)(c) AI Act. 31 Another example are AI systems  evaluating  persons  and  determining  if  they  are  entitled  to  receive  essential public assistance benefits and services, such as healthcare services and social security benefits that are classified as high-risk. 32 If such systems involve unacceptable social scoring and fulfil the conditions of Article 5(1)(c) AI Act, their placing on the market, putting into service and use will be prohibited in the Union.
In  such  cases,  the  risk  assessment  and  management  done  by  the  provider  and  the compliance with the other requirements for high-risk AI systems (e.g. data governance, transparency and human oversight), as well as the deployer's obligations for appropriate use in accordance with the instructions of use and human oversight (Article 26) and in some cases a fundamental rights impact assessment (Article 27), should help to ensure that the high-risk AI system placed on the market or deployed is lawful and does not constitute a prohibited practice.
- (38) Finally, AI systems that are exceptionally not considered high-risk based on Article 6(3) AI Act, despite falling under a high-risk use case of Annex III, may still fall within the scope of the prohibitions of Article 5 AI Act. Article 6(3) AI Act only results in an AI system being considered non-high-risk; it does not exclude such AI systems from the scope of the AI Act and the prohibitions.
## . Application of the prohibitions to general-purpose AI systems and systems with intended purposes
30 In  this  list,  AI  systems  based on biometrics  are  covered,  as  well  as  AI  systems used  for  specific purposes  in  certain domains  such  as employment, education, access to public and private services, law enforcement etc.
31 This is expressly mentioned in Recital 58 and in Annex III AI Act.
32 Recital 58 AI Act.
- (39) The  prohibitions  apply  to  any  AI  system,  whether  with  an  'intended purpose' 33 or 'general-purpose'  (i.e.  that  can  serve  a  variety  of  purposes),  for  direct  use  or  for integration in other AI systems. 34 Accordingly, each operator should take measures for which they are best placed based on their role and control over the system in the value chain to ensure a responsible and safe provision and use of AI systems, balancing their risks and benefits with a view to achieving the twin objectives of the AI Act.
- (40) Deployers are thus expected not to use any AI system in a manner prohibited under Article 5 AI Act, including not to bypass any safety guardrails implemented by the providers of the system. While the harm often arises from the way the AI systems are used in practice, providers also have a responsibility not to place on the market or put into  service  AI  systems,  including  general-purpose  AI  systems,  that  are  reasonably likely to behave or be directly used in a manner prohibited by Article 5 AI Act. 35 In this context, providers are also expected to take effective and verifiable measures to build in safeguards and prevent and mitigate such harmful behaviour and misuse to the extent they  are  reasonably  foreseeable  and  the  measures  are  feasible  and  proportionate depending on the specific AI system and circumstances of the case. In their contractual relationships with deployers (i.e., in the terms of use of the AI system), providers are also expected to exclude use of their AI system for prohibited practices and provide appropriate  information  in  the  instructions  of  use  for  deployers  and  regarding  the necessary human oversight.
For example, a general-purpose AI system used as a chatbot may deploy manipulative and  deceptive  techniques  which  is  likely  to  cause  significant  harm.  To  prevent prohibited behaviour of the AI system and uses that are reasonably likely to manipulate, deceive  and  cause  significant  harms  under  Article  5(1)(a)  AI  Act,  the  provider  is expected to  take  appropriate  and  proportionate  measures  (e.g.,  appropriate  safe  and ethical  design,  integration  of  technical  and  other  safeguards,  restrictions  of  use, transparency and user control, appropriate information in the instructions of use) before the AI system is placed on the market (Article 5(1)(a) AI Act) to ensure the chatbot does not cause significant harm to users or other persons or groups of persons (see also section .c)).
- (41) In certain cases, in particular where the prohibitions are linked to a very specific purpose of the system 36 , providers may have limited possibilities to integrate other preventive and  mitigating  measures  and  will  have  to  rely  on  primarily  providing  appropriate instructions and information to the deployers and the required human oversight and restricting prohibited use of the system. Where appropriate, such measures may also
33 Defined in Article 3(12) AI Act as the use for which an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.
34
See Article 3(66) AI Act.
35 This follows in particular from the reference to 'placing on the market' or 'putting into service' in all prohibitions listed in Article 5 AI Act, with the exception of the prohibition of real-time RBI systems in Article 5(1)(h) that applies only to the use.
36
Article 5(1)(d)-(h) AI Act.
include  monitoring  for  compliance  with  that  restriction,  depending  on  the  means through which the AI system is supplied and the information at the provider's disposal for  possible  misuse.  Any  possible  monitoring  measures  to  detect  misuse  should  not amount to a general monitoring of the activities of the deployers and should be in line with Union law.
For example, a general-purpose AI system that can recognise or infer emotions should not  be  used  by  deployers  at  the  workplaces  or  in  education  institutions,  unless  an exception for medical or safety reasons applies. However, the provider may not be in a position to know the specific context in which the emotion recognition functionality of the system will be used and whether an exception to the prohibition in Article 5(1)(f) AI Act may apply. Such providers may nevertheless explicitly exclude such prohibited use in their terms of use and include appropriate information in the instructions of use to guide deployers. They are also expected to take appropriate measures if they become aware  that  the  system  is  misused  for  this  specific  prohibited  purpose  by  specific deployers, for example, if such misuse is reported or the provider becomes otherwise aware, which may be the case if the system is directly operated through a platform under the control of the provider and the provider performs checks.
## . Interplay between the prohibitions and other Union law
- (42) The AI Act is a regulation that applies horizontally across all sectors without prejudice to  other  Union  legislation,  in  particular  on  the  protection  of  fundamental  rights, consumer protection, employment, the protection of workers, and product safety 37 . The AI  Act  complements  such  legislation  through  its  preventative  and  safety  logic  (AI systems  may  not  be  placed  on  the  market  or  used  in  a  certain  way)  and  provides additional protection by addressing specific harmful AI practices which may not be prohibited  by  other  laws.  Furthermore,  by  addressing  the  earlier  stages  of  the  AI systems'  lifecycle  (i.e.  the  placing  on  the  market  and  putting  into  service)  and deployment (i.e. the use), the AI Act's prohibitions enable action to be taken against harmful practices involving AI at various points in the AI value chain.
- (43) At the same time, the AI Act does not affect prohibitions that apply where an AI practice falls within other Union law 38 . Thus, even where an AI system is not prohibited by the AI Act, its use could still be prohibited or unlawful based on other primary or secondary Union law (e.g., because of the failure to respect fundamental rights in a given case, such as the lack of a legal basis for the processing of personal data required under data protection  law,  discrimination  prohibited  by  Union  law,  etc.).  The  respect  of  the prohibitions in the AI Act are therefore not a sufficient condition for compliance with other  Union  legislation  that  remains  applicable  to  providers  and  deployers  of  AI systems.
37 Article 2 and Recital 9 AI Act.
38 Article 5(8) AI Act.
For example, AI-enabled emotion recognition systems used in the workplace that are exempted from the prohibition in  Article  5(1)(f)  AI  Act,  because  they  are  used  for medical or safety reasons, remain subject to data protection law and Union and national law on employment and working conditions, including health and safety at work, which may foresee other restrictions and safeguards in relation to the use of such systems. 39
- (44) When specific activities related to the placing on the market or use of AI systems are also covered under other Union legislation, the AI Act aims to ensure the consistent implementation of the different provisions. Moreover, it enables effective cooperation between the competent authorities responsible for the enforcement of the AI Act and the authorities protecting fundamental rights pursuant to Article 77 AI Act and other provisions of the AI Act. More generally, in accordance with Article 4(3) TEU, the various authorities concerned are bound to cooperate sincerely when giving effect to their respective tasks under Union law.
- (45) In the context of the prohibitions, the interplay between the AI Act and Union data protection  law  is  particularly  relevant,  since  AI  systems  often  process  information relating to identified or identifiable natural persons ('personal data'). 40  Depending on the prohibition and the context, the most relevant legal acts in relation to such systems are Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing  of  personal  data  and  on  the  free  movement  of  such  data  (General  Data Protection Regulation, hereinafter 'GDPR'), Directive (EU) 2016/680 on the protection of  natural  persons  with  regard  to  the  processing  of  personal  data  by  competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such  data  (Law  Enforcement  Directive,  hereinafter  'LED'),  and  Regulation  (EU) 2018/1725 which lays down data protection rules for the EU Institutions, bodies, offices and agencies (hereinafter 'EUDPR'). In accordance with Article 2(7) AI Act, these acts remain unaffected and will continue to apply alongside the AI Act, which is consistent and complementary to the EU data protection acquis. Several aspects of these EU data protection rules have been clarified by the CJEU and the European Data Protection Board has adopted a series of guidelines (e.g., on the notion of 'profiling' 41 , which is particularly relevant for the prohibition in Article 5(1)(d) AI Act, since it uses the same notion).
- (46) Concerning the prohibitions/restrictions on the use of biometric categorisation systems and real-time RBI systems for law enforcement purposes, the AI Act applies as lex specialis to Article 10 LED, thus regulating such use and the processing of biometric data involved in an exhaustive manner 42 . In that context, the AI Act is not intended to
39 See also Recital 9 AI Act.
40 Article 2(7) AI Act; see also Recital 10 AI Act.
41 See also Article 29 Data Protection Working Party, Guidelines on Automated individual decision-making and Profiling for the purposes of Regulation 2016/679, WP251rev.01, , and endorsed by the EDPB.
42 Recital 38 AI Act.
provide the legal basis for the processing of personal data under Article 8 of Directive (EU) 2016/680. All other provisions of that Directive apply in addition to the conditions set  out  in  the  AI  Act,  in  particular  for  the  use  of  real-time  (RBI)  systems  for  law enforcement  purposes  when  permitted,  subject  to  the  limited  exceptions  in  Article 5(1)(h) AI Act. More generally, the LED must also be complied with for any processing of personal data by competent law enforcement authorities (i.e., competent authorities under Article 3(7) LED) when they process the data for law enforcement purposes.
- (47) In accordance with Article 2(9) AI Act, EU consumer protection and safety legislation also remain fully applicable to AI systems falling within scope of those acts.
## For example,
- - Social scoring practices by traders (including natural persons acting in a professional capacity in business-to-consumer relations), subject to case-by-case assessment, may also  be  considered ' unfair'  and  therefore  in  breach  of  consumer  law  (i.e.  Directive 2005/29/EC);
- - The use of an AI system to infer emotions may also have to comply with Regulation (EU)  2017/745  (Medical  Device  Regulation)  if  the  AI  system  is  used  for  medical diagnosis or medical treatment purposes.
- (48) Furthermore, the AI Act applies in conjunction with relevant obligations for providers of intermediary services that embed AI systems or models into their services regulated by Regulation (EU) 2022/2065 ('the Digital Services Act'). Specifically, Article 2(5) AI Act indicates that the AI Act does not affect the application of the provisions on the liability of such providers as set out in Chapter II of the Digital Services Act.
- (49) In addition, the prohibitions in the AI Act are without prejudice to any liability that the provider or deployer might incur for the harm caused according to applicable Union or national liability laws. 43
- (50) Finally,  the  prohibitions  in  Article  5  AI  Act  and  the  explicit  exceptions  to  those prohibitions may not be used to circumvent or as a justification to infringe obligations under other Union legislation.
- (51) As  secondary  Union  legislation,  the  AI  Act  must  be  interpreted  in  the  light  of  the fundamental rights and freedoms guaranteed by the EU Treaties and the Charter, as well as those protected by international conventions to which the Union is a party. 44
43 The conditions for liability (related to damage, liable person, fault or burden of proof, etc) will be determined by the applicable law, such as Directive (EU) 2024/2853 of the European Parliament and of the Council of 23 October 2024 on liability for defective products, (Text with EEA relevance), OJ L, 2024/2853,  or the applicable national liability laws (see also Proposal for a Directive of the European Parliament and of the Council on adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive) COM/2022/496 final) .
44 Even if the Union is not yet a party to the European Convention for the Protection of Human Rights and Fundamental Freedoms, Article 59(3) of the Charter states that in so far as the Charter contains rights which correspond to rights guaranteed by the European Convention for the Protection of Human Rights and Fundamental Freedoms, the meaning and scope of those rights shall be the same as those laid down by the said Convention. This provision shall not prevent Union law providing more extensive protection.
- (52) Additional clarifications on the interplay of specific prohibitions with other Union law are provided under the relevant sections below.
## . Enforcement of Article 5 AI Act
## . Market Surveillance Authorities
- (53) Market  surveillance  authorities  designated  by  the  Member  States  as  well  as  the European Data Protection Supervisor (as the market surveillance authority for the EU institutions, agencies and bodies) are responsible for the enforcement of the rules in the AI Act for AI systems, including the prohibitions. Such enforcement takes place within the  system  of  market  surveillance  and  compliance  of  products  established  by Regulation (EU) 2019/1020 45 , in line with other Union product safety legislation. The enforcement powers of market surveillance authorities in relation to AI systems are laid down in the AI Act and in Regulation (EU) 2019/1020.  Those authorities  can take enforcement actions in relation to the prohibitions on their own initiative or following a complaint, which every affected person or any other natural or legal person having grounds  to  consider  such  violations  has  the  right  to  lodge 46 .  Member  States  must designate their competent market surveillance authorities by 2 August 2025.
- (54) The procedure in the AI Act to deal with AI systems presenting a risk at national level is particularly relevant in the context of enforcing the prohibitions 47 . Where there are cross-border implications beyond the territory of the market surveillance authority, the authority of the Member State concerned must inform the Commission and the market surveillance  authorities  of  other  Member  States.  All  market  surveillance  authorities should follow a Union safeguard procedure with a decision taken by the Commission 48 determining whether the AI system constitutes a prohibited practice. That procedure aims to ensure that the prohibitions are applied uniformly across all Member States, so as to provide legal certainty to both providers and deployers of AI systems. To ensure the uniform application of the AI Act, national market surveillance authorities should also strive for a harmonized application of the prohibitions for comparable cases that do not cross the Member State's territory by drawing inspiration from these Guidelines and cooperating within the AI Board 49 .
## . Penalties
- (55) The AI Act follows a tiered approach in setting the penalties for non-compliance with its  various  provisions,  depending  on  the  seriousness  of  the  infringement.  Noncompliance with the prohibitions in Article 5 AI Act are considered to constitute the most severe infringement and they are therefore subject to the highest fine. Providers and deployers engaging in prohibited AI practices may be fined up to EUR 35 000 000
45 See also Recital 156 AI Act.
46 Article 85 AI Act.
47 Article 79 AI Act.
48 Article 81 AI Act.
49 Article 65 and 66 AI Act.
- or, if the offender is an undertaking, up to 7 % of its total worldwide annual turnover for the preceding financial year, whichever is higher. 50 Each Member State should lay down rules if and to the extent that administrative fines may be imposed on public authorities and bodies established in that Member State as providers and deployers of AI systems. EU institutions, bodies and agencies that violate the prohibitions may be subject to administrative fines of up to EUR 1 500 000. 51
- (56) It is possible that one and the same prohibited conduct constitutes a violation of two or more provisions of the AI Act (i.e. the non-labelling of deep fakes may also constitute a deceptive technique under Article 5(1)(a) AI Act). In such cases, the principle of ne bis in idem should be respected. In any event, the criteria for determining the penalty as provided for in Article 99(7) AI Act must be taken into account.
- (57) Since  violations  of  the  prohibitions  in  Article  5  AI  Act  interfere  the  most  with  the freedoms of others and give rise to the highest fines, their scope should be interpreted narrowly.
## 3. ARTICLE  5(1)(A)  AND  (B)  AI  ACT  -  HARMFUL  MANIPULATION, DECEPTION AND EXPLOITATION
- (58) The first two prohibitions in Article 5(1)(a) and (b) AI Act aim to safeguard individuals and vulnerable persons from the significantly harmful effects of AI-enabled manipulation  and  exploitation.  Those  prohibitions  target  AI  systems  that  deploy subliminal,  purposefully  manipulative  or  deceptive  techniques  that  are  significantly harmful and materially influence the behaviour of natural persons or group(s) of persons (Article 5(1)(a) AI Act) or exploit vulnerabilities due to age, disability, or a specific socio-economic situation (Article 5(1)(b) AI Act).
## . Rationale and objectives
- (59) The underlying rationale of these prohibitions is to protect individual autonomy and well-being from manipulative, deceptive, and exploitative AI practices that can subvert and  impair  an  individual's  autonomy,  decision-making,  and  free  choices. 52 The prohibitions aim to protect the right to human dignity (Article 1 of the Charter), which also constitutes the basis of all fundamental rights and includes individual autonomy as an  essential  aspect.  In  particular,  the  prohibitions  aim  to  prevent  manipulation  and exploitation  through  AI  systems  that  reduce  individuals  to  mere  tools  for  achieving certain ends and to safeguard those that are most vulnerable and susceptible to harmful manipulation and exploitation. The prohibitions against significantly harmful manipulative,  deceptive  and  exploitative  AI  practices  fully  align  with  the  broader objectives of the AI Act to promote trustworthy and human-centric AI systems that are safe, transparent, fair and serve humanity and align with human agency and EU values.
50 Article 99 AI Act.
51
Article 100 AI Act.
52 Recital 29 AI Act.
## . Main components of the prohibition in Article 5(1)(a) AI Act - harmful manipulation
## Article 5(1)(a) AI Act provides:
- 1. The following AI practices shall be prohibited:
- (a) the placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person's consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm;
- (60) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(a) AI Act to apply:
- (i) The practice must constitute the 'placing on the market', the 'putting into service', or the 'use' of an AI system.
- (ii) The  AI  system  must  deploy  subliminal  (beyond  a  person's  consciousness), purposefully manipulative or deceptive techniques.
- (iii) The techniques deployed by the AI system should have the objective or the effect of  materially  distorting  the  behaviour  of  a  person  or  a  group  of  persons.  The distortion  must  appreciably  impair  their  ability  to  make  an  informed  decision, resulting  in  a  decision  that  the  person  or  the  group  of  persons  would  not  have otherwise made.
- (iv) The distorted behaviour must cause or be reasonably likely to cause significant harm to that person, another person, or a group of persons.
- (61) For the prohibition to apply, all four conditions must be simultaneously fulfilled and there must be a plausible causal link between the techniques deployed, the material distortion of the behaviour of the person, and the significant harm that has resulted or is reasonably likely to result from that behaviour.
- (62) The first condition, i.e. the 'placing on the market', the 'putting into service' or the 'use' of an AI system, has already been analysed. The prohibition, therefore, applies to both providers and deployers of AI systems, each within their respective responsibilities, not to place on the market, put into service or use such systems. The next sections focus on the other three conditions.
## . Subliminal, purposefully manipulative or deceptive techniques
- (63) Article 5(1)(a) AI Act prohibits three alternative types of manipulative techniques: a) subliminal techniques beyond a person's consciousness: b) purposefully manipulative techniques; and c) deceptive techniques. To fall within scope of Article 5(1)(a) AI Act, an AI system must deploy one or more of these techniques.
## a) Subliminal techniques
- (64) While  the  AI  Act  does  not  define  'subliminal  techniques',  Article  5(1)(a)  AI  Act specifies that subliminal techniques operate beyond (below or above) the threshold of conscious  awareness.  Because  subliminal  techniques  and  the  way  they  operate  are inherently  covert,  such  techniques  bypass  a  person's  rational  defences  against manipulation and are capable of influencing decisions without the conscious awareness of the person, raising significant ethical concerns and impairing individual autonomy, agency and free choice 53 .
- (65) The subliminal techniques must be capable of influencing behaviour in ways in which the  person  remains  unaware  of  such  influence,  how  it  works,  or  its  effects  on  the person's  decision-making  or  value-  and  opinion-formation.  In  particular,  subliminal techniques may use stimuli delivered through audio, visual, or tactile media that are too brief or subtle to be noticed and that have been traditionally known and prohibited in other  sectors,  such  as  media  advertising. 54 These  stimuli,  while  not  consciously perceived, may still be processed by the brain and influence behaviour.
- Examples  of  subliminal  techniques  (not  necessarily  prohibited  unless  all  other conditions listed in Article 5(1)(a) AI Act are fulfilled) include:
- -  Visual  Subliminal  Messages: an  AI  system  may  show  or  embed  images  or  text flashed  briefly  during  video  playback  which  are  technically  visible,  but  flashed  too quickly  for  the  conscious  mind  to  register,  while  still  being  capable  of  influencing attitudes or behaviours.
- - Auditory Subliminal Messages: an AI system may deploy sounds or verbal messages at low volumes or masked by other sounds, influencing the listener without conscious awareness. These sounds are still technically within the range of hearing, but are not consciously noticed by the listener due to their subtlety or masking by other audio.  Tactile Subliminal Stimuli: an AI system may stimulate subtle physical sensations that are perceived unconsciously, capable of influencing emotional states or behaviour.
- - Subvisual and Subaudible Cueing: an AI system may deploy stimuli that are not just subtle or masked, but are presented in a way that makes them entirely undetectable by the human senses under normal conditions, for example flashing visual stimuli (e.g. flashing images) too quickly for the human eye to detect consciously or playing sounds at volumes imperceptible to the human ear.
- - Embedded Images: an AI system may hide images within other visual content which are not consciously perceived, but may still be processed by the brain and influence behaviour.
53 Recital 29 AI Act.
54 See in particular, the Directive 2010/13/EU of the European Parliament and of the Council of 10 March 2010 on the coordination of certain provisions laid down by law, regulation or administrative action in Member States concerning the provision of audiovisual media services (OJ L 95, , p. 1) ('AVMSD'). which strictly prohibits subliminal techniques in audiovisual commercial communications.
- -  Misdirection: an  AI  system  may  draw  attention  to  specific  stimuli  or  content  to prevent noticing other content, often by exploiting cognitive biases and vulnerabilities in attention.
- -Temporal  manipulation: an  AI  system  may  alter  the  perception  of  time  in  user interactions, thus influencing their behaviour and causing impatience and dependence.
- (66) The rapid development of AI and related technologies, such as big data analytics, neuro technologies,  brain-computer  interfaces  and  virtual  reality,  heightens  the  risk  of sophisticated subliminal manipulation and its capability to effectively influence human behaviour in a subconscious manner. 55 AI can also extend to emerging machine-brain interfaces and advanced techniques like dream-hacking and brain spyware.
For example, a game can leverage AI-enabled neuro technologies and machine-brain interfaces that permit users to control (parts of) a game with headgear that detects brain activity.  AI  may  be  used  to  train  the  user's  brain  surreptitiously  and  without  their awareness to reveal or infer from the neural data information that can be very intrusive and sensitive (e.g. personal bank information, intimate information, etc.) in a manner that can cause them significant harm. The prohibition in Article 5(1)(a) AI Act targets only cases of such significantly harmful subliminal manipulation and not machine-brain interface  applications  in  general  when  designed  in  a  safe  and  secure  manner  and respectful of privacy and individual autonomy.
## b) Purposefully manipulative techniques
- (67) 'Purposefully manipulative techniques' are not defined in the AI Act, but they should be understood as techniques that are designed or objectively aim to influence, alter, or control  an  individual's  behaviour  in  a  manner  that  undermines  their  individual autonomy and free choices. Manipulative techniques are typically designed to exploit cognitive biases, psychological vulnerabilities, or situational factors that  make individuals more susceptible to influence. Because of their adaptability, AI systems are also able to respond well to a person's individual circumstances or vulnerabilities and increase the effectiveness and impact of manipulation at scale. While the manipulative capability is an important element to determine the nature of the technique, it is not necessary that the provider or deployer or the system itself deploying the manipulative techniques also intends to cause harm 56 .
- (68) While  not  all  manipulative  techniques  operate  beyond  the  threshold  of  conscious awareness, many do and there may be an overlap with subliminal techniques, since such techniques also ultimately have manipulative effects. Recital 29 AI Act clarifies that the prohibition in Article 5(1)(a) also covers techniques where individuals, even if they are aware of the influence attempt, may not be able to control or resist its manipulative
55 See Recital 29 AI Act.
56 See in this context Recital 28 and sections . and . of the Guidelines.
effect 57 . As a result, individuals are influenced or pushed into behaviour and decisions they would normally not have made if they were not subjected to the manipulative techniques to a point that undermines their individual autonomy or free choice.
An example of purposefully manipulative techniques is sensory manipulation where an AI  system  deploys  background  audio  or  images  that  lead  to  mood  alterations,  for example increasing anxiety and mental distress that influence users' behaviour to the point of creating significant harm.
Another example is personalised manipulation where an AI system that creates and tailors highly persuasive messages based on an individual's personal data or exploits other  individual  vulnerabilities  influences  their  behaviour  or  choices  to  a  point  of creating significant harm.
- (69) The prohibition against purposefully manipulative techniques also covers AI systems that manipulate individuals without any human intending them to do so. Article 5(1)(a) AI  Act  prohibits  AI  systems  that  deploy  certain  techniques  or  exhibit  a  specific manipulative behaviour. Therefore, it could also be the AI system that deploys such manipulative techniques, rather than the provider or the deployer that has designed or used the system in this way.
For example, regardless of whether the provider intends it, an AI system may learn manipulative techniques because the data on which it is trained contain many instances of manipulative techniques, 58 or because reinforcement learning from human feedback can be 'gamed' through manipulative techniques. 59
By contrast, if the manipulative behaviour of the system is merely incidental, the system should not be considered deploying purposefully manipulative techniques as long as the  provider  has  taken  appropriate  preventive  and  mitigating  measures  in  case significant harms are reasonably likely to occur (see section .c) below).
## c) Deceptive techniques
(70) The AI Act does not define 'deceptive techniques'. Recital 29 AI Act clarifies that these are  techniques  that  subvert  or  impair  a  person's  autonomy,  decision-making  or  free choice in ways that the person is not consciously aware or, where it is aware, can still be deceived or is not able to control or resist them. 'Deceptive techniques' deployed by AI systems should be understood to involve presenting false or misleading information with the objective or the effect of deceiving individuals and influencing their behaviour in a manner that undermines their autonomy, decision-making and free choices.
57 Recital 28 AI Act.
58 M. Carroll et al., Characterising Manipulation from AI Systems, In Equity and Access in Algorithms,Mechanisms, and Optimization (EAAMO '23), October 30-November 1, 2023, Boston, MA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10. 1145/ |:.
