In this context, compliance with the data protection rules for transparency, data minimisation,  fairness  and  lawfulness,  for  example  for  personalised  profiling  and advertising,  based  on  off-service  users'  data 95 may  contribute  to  avoid  harmful personalised manipulation and exploitation.
- (138) The interplay with Union non-discrimination law is also relevant for the prohibition in Article 5(1)(b) AI Act, 96 given that vulnerabilities due to age and disability are also protected grounds on which people have the right not to be discriminated, while socioeconomic situation intersects with a variety of other grounds, such as race and ethnic origin. The prohibitions in the AI Act do not affect prohibitions based on other grounds or  discriminatory  practices  that  do  not  entail  significant  harms  and  that  are  already prohibited by Union non-discrimination law.
- (139) The prohibitions in Article 5(1)(a) and (b) AI Act are also complementary to Regulation (EU) 2022/2065 (the Digital Services Act ('DSA') which regulates online intermediary services, such as online platforms and search engines, and ensures transparency and accountability in the provision of those services. Notably, Article 25(1) DSA prohibits
95 Particularly relevant in this respect is the Judgment of the Court (Grand Chamber) of 4 July 2023, Case C-252/21
Meta Platforms Inc and Others v Bundeskartellamt . Although the CJEU finds, inter alia, that the processing of off-service users' personal data for direct marketing purposes by a large social network platform may be regarded as carried out for a legitimate interest of the controller, this cannot be done without consent from a user as a legal basis due to the interests and fundamental rights of such a user, which  under the circumstances of that case, in particular the extensive processing, override the interest of that operator in such personalised advertising through which social platforms finance their activities (see the Meta Platforms judgment, paragraphs 115 to 118).
96 E.g., Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin OJ L 180, , p. 22-26; Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation OJ L 303, , p. 16-22; Directive 2006/54/EC of the European Parliament and of the Council of 5 July 2006 on the implementation of the principle of equal opportunities and equal treatment of men and women in matters of employment and occupation (recast), OJ L 204, , p. 23-36; Council Directive 2004/113/EC of 13 December 2004 implementing the principle of equal treatment between men and women in the access to and supply of goods and services, OJ L 373, , p. 37-43.
dark patterns within the user interface to ensure that providers of online platforms do not mislead or coerce users into actions that may not align with their genuine intentions. Such dark patterns should be understood to constitute an example of manipulative or deceptive techniques within the meaning of Article 5(1)(a) AI Act, when they are likely to cause significant harms.
- (140) The  DSA  also  sets  out  obligations  for  providers  of  online  platforms  to  ensure transparency in advertising (Articles 26 and 38 for very large online platforms or very large  search  engines),  on  the  use  of  recommender  systems  (Article  27)  and  on  the protection of minors (Article 28 DSA). Moreover, if an online platform or search engine is classified as a very large online platform or very large search engine, the provider of that designated service has additional obligations to assess and mitigate systemic risks stemming from the design or functioning of its service and its related systems, including algorithmic systems (Articles 34 and 35 DSA). When conducting risks assessments, providers of very large online platforms and of very large online search engines should consider  how  their  recommender  systems,  advertising,  content  moderation  and  any other relevant algorithmic systems influence such systemic risks. Such risk assessments should  also  analyse  how  systemic  risks  are  influenced  by,  among  other  things,  the intentional manipulation and automated exploitation of the service (c.f. Article 34(2) DSA and recital 83 DSA). Nevertheless, the scope of Article 5(1)(a) or (b) AI Act covers  a  broad  variety  of  other  scenarios  (e.g.,  chatbots,  AI-enabled  services  and products) that may be offered or used by other actors than providers of intermediary services.
- (141) The prohibition of manipulative AI techniques pursuant to Article 5(1)(a) AI Act also supports the objectives of Directive 2010/13/EU (the AVMSD) 97 by preventing harmful AI-driven  advertisements 98 and  other AI-enabled  manipulative  and  exploitative practices that may be significantly harmful in the media sector.
- (142) The  AI  Act  also  complements  Regulation  (EU)  2024/900  (the  Political  advertising regulation) 99 which provides harmonised rules, including transparency and related due diligence obligations, for the provision of political advertising and related services; and on the use of targeting and ad-delivery techniques in the context of online political advertising. This Regulation prohibits profiling based on special categories of personal data in the context of online political advertising and targeting of persons at least one year under the voting age established by national rules. Furthermore, targeting and addelivery techniques in the context of online political advertising can only be done if based on personal data collected from the data subjects and with their explicit consent.
97 Directive 2010/13/EU of the European Parliament and of the Council of 10 March 2010 on the coordination of certain provisions laid down by law, regulation or administrative action in Member States concerning the provision of audiovisual media services (Audiovisual Media Services Directive ('AVMSD') amended by Directive (EU) 2018/1808) that, inter alia, aim to improve the protection for children and tackle hate speech more effectively.
98  Article 9 of AVMSD.
99 Regulation (EU) 2024/900 of the European Parliament and of the Council of 13 March 2024 on the transparency and targeting of political advertising, PE/90/2023/REV/1, OJ L, 2024/900, .
Additional transparency requirements also apply, i.e. disclosure of political advertisement,  describing  the  use  of  such  techniques  and  main  parameters  and additional information on the logic involved, including about the use of AI systems. Targeted political advertising based on processing of personal data in compliance with that  Regulation 100 will  help  to  ensure  that  profiling  of  voters  and  targeting  and  addelivery of political ads operate within the boundaries of lawful persuasion.
(143) The  AI  Act  prohibition  of  harmful  exploitative  and  deceptive  AI  practices  is  also complementary  to  other  applicable  Union  legislation  that  sets  general  transparency rules  on  advertising  and  consumer  protection  and  due  conduct  of  operators  (e.g. Directive 2014/65/EU MIFID, Directive (EU) 2016/97 on Insurance Distribution 101 , Directive (EU) 2023/2225 on Consumer Credit Agreements, Directive (EU) 2002/65 Distance Marketing, Directive 2006/114/EC on misleading and comparative advertising  and  Directive  (EU)  2011/83  on  consumer  rights  sets  general  consumer protection standards). In this regard, the European Insurance and Occupational Pensions Authority  (EIOPA)  has  already  issued  a  supervisory  statement  on  some  unfair exploitative practices in relation to differential pricing that could also fall under the scope of the AI Act when enabled by AI systems 102 .
(144) The prohibitions in Article 5(1)(a) and (b) AI Act are also without prejudice to and complement EU product safety legislation (e.g., for medical devices, toys, machinery), which plays a crucial role in ensuring the safety of products that integrate AI systems. This entails compliance with ex ante safety requirements for regulated products and their  proactive  monitoring  to  ensure  that  they  do  not  pose  safety  risks  leading  to physical and mental harms. The manufacturer of those products embedding AI systems should therefore take into account these prohibitions in their risk assessments and safety mitigating measures to the extent this fits with the logic and the scope of the relevant Union harmonised safety legislation. Union safety legislation is also complementary to the AI Act prohibitions and can also intervene and address safety risks that do not pose significant harm. In particular, Regulation (EU) 2023/988 (the General Product Safety Regulation) 103 acts as a safety net and requires all consumer products not covered by specific  requirements  in  other  sectoral  Union  product  safety  legislation  (including products embedding AI systems not classified as high-risk pursuant to Article 6 and subjected to the requirements in the AI Act) to be safe under normal or reasonably foreseeable  conditions  of  use,  in  particular  addressing  risks  to  physical  and  mental health risks for consumers.
100 Once applicable as of October 2025.
101 Directive (EU) 2016/97 of the European Parliament and of the Council of 20 January 2016 on insurance distribution (recast), OJ L 26, , p. 19-59. E.g., Article 17(1) of the Insurance Distribution Directive for insurance distributors to act honestly, fairly and professionally in accordance with the best interests of their customers.
102  https://www.eiopa.europa.eu/document/download/1e9a8fb2-e688-4bf5-a347-ee0a1ec3aab3\_en?filename=EIOPA-BoS-23-076Supervisory-Statement-on-differential-pricing-practices\_0.pdf.
103 Regulation (EU) 2023/988 of the European Parliament and of the Council of 10 May 2023 on general product safety, amending Regulation (EU) No 1025/2012 of the European Parliament and of the Council and Directive (EU) 2020/1828 of the European Parliament and the Council, and repealing Directive 2001/95/EC of the European Parliament and of the Council and Council Directive 87/357/EEC (Text with  EEA relevance).
(145) Finally, the interplay with criminal law is critical. The prohibitions in Article 5(1)(a) and (b) AI Act aim to prevent harmful behaviour that may constitute or lead to criminal offences, such as fraud, forgery, scams, coercion, or the generation and dissemination of illegal content, such as terrorist content, child sexual abuse material, hate speech and sexually explicit deepfakes. 104 Importantly, as internal market legislation, the prohibitions in Article 5(1)(a) and (b) AI Act cover not only the use, but also the placing on the market of the AI system, thus preventing harm early on by limiting access to such prohibited systems that can facilitate and obscure criminal activities. Furthermore, the  prohibitions  in  Article  5(1)(a)  and  (b)  AI  Act  could  also  cover  other  harmful practices that are not qualified as criminal offences under Union or national law.
## 4. ARTICLE 5(1)(C) AI ACT - SOCIAL SCORING
(146) While AI-enabled scoring can bring benefits to steer good behaviour, improve safety, efficiency or quality of services, there are certain 'social scoring' practices that treat or harm people unfairly and amount to social control and surveillance. The prohibition in Article 5(1)(c) AI Act targets such unacceptable AI-enabled 'social scoring' practices that assess or classify individuals or groups based on their social behaviour or personal characteristics and lead to detrimental or unfavourable treatment, in particular where the data comes from multiple unrelated social contexts or the treatment is disproportionate to the gravity of the social behaviour. The 'social scoring' prohibition has a broad scope of application in both public and private contexts and is not limited to a specific sector or field 105 .
(147) At the same time, the prohibition is not intended to affect lawful practices that evaluate people  for  specific  purposes  that  are  legitimate  and  in  compliance  with  Union  and national law, 106 in particular where those laws specify the types of data relevant for the specific evaluation purposes and ensure that any resulting detrimental or unfavourable treatment of persons is justified and proportionate (see section . out of scope).
## . Rationale and objectives
(148) AI systems enabling 'social scoring' practices may lead to discriminatory and unfair outcomes for certain individuals and groups, including their exclusion from society, as well  as  social  control  and  surveillance  practices  that  are  incompatible  with  Union values. The prohibition of 'social scoring' aims to protect, in particular, the right to human dignity and other fundamental rights, including the right to non-discrimination and equality, to data protection, and to private and family life, as well as relevant social and economic rights, as applicable. It also aims to safeguard and promote the Union
104 Directive (EU) 2024/1385 of the European Parliament and of the Council of 14 May 2024 on combating violence against women and domestic violence, PE/33/2024/REV/1, OJ L, 2024/1385, .
105 The  prohibition  of  social  scoring  differs  from  the  prohibition  in  Article  5(1)(d)  AI  Act,  which  is  more  specialised  in  relation  to  the evaluation/scoring practice which is applicable only to the risk assessment and prediction of the likelihood of a person committing criminal offences by prohibiting AI systems solely based on profiling or assessment of personality traits and characteristics (see section 5).
106
Recital 31 AI Act.
values of democracy, equality (including equal access to public and private services), and justice. 107
## . Main concepts and components of the 'social scoring' prohibition
## Article 5(1)(c) AI Act provides:
The following AI practices shall be prohibited:
- (c) the placing on the market, the putting into service or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following:
- (i) detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected;
- (ii) detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity;
- (149) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(c) AI Act to apply:
- (i) The practice must constitute the 'placing on the market', the 'putting into service' or the 'use' of an AI system.
- (ii) The  AI  system  must  be  intended  or  used  for  the  evaluation  or  classification  of natural persons or groups of persons over a certain period of time based on:
- (a) their social behaviour; or
- (b) known, inferred or predicted personal or personality characteristics.
- (iii)The social score created with the assistance of the AI system must lead or be capable of leading to the detrimental or unfavourable treatment of persons or groups in one or more of the following scenarios:
- (a) in  social  contexts  unrelated  to  those  in  which  the  data  was  originally generated or collected; and/or
- (b) treatment that is unjustified or disproportionate to their social behaviour or its gravity.
- (150) For  the  prohibition  in  Article  5(1)(c)  AI  Act  to  apply,  all  three  conditions  must  be simultaneously fulfilled. The first condition, i.e. the placing on the market, the putting into service or the use of the AI system, has been already analysed in section . The prohibition therefore applies to both providers and deployers of AI systems, each within their respective responsibilities, not to place on the market, put into service or use such AI systems. The remaining criteria for the prohibition on 'social scoring' are further described and analysed below.
107 Recital 31 AI Act.
- . 'Social  scoring':  evaluation  or  classification  based  on  social behaviour  or  personal  or  personality  characteristics  over  a certain period of time
## a) Evaluation or classification of natural persons or group of persons
(151) The second condition for the prohibition in Article 5(1)(c) AI Act to apply is that the AI system is intended or used for the evaluation or classification of natural persons or groups  of  persons  and  assigns  them  scores  based  on  their  social  behaviour  or  their personal  or  personality  characteristics.  The  score  produced  by  the  system  may  take various forms, such as a mathematical number (for example, from 0 to 1), a ranking, or a label.
- (152) The scope of the prohibition is broad covering evaluation and classification practices in both  the  public  and  the  private  sector  (see  section  .).  At  the  same  time  the evaluation or classification concerns only natural persons or groups of natural persons, thereby excluding in principle legal entities (see section . out of scope).
- (153) While 'evaluation' suggests  the  involvement  of  some  form  of  an assessment  or judgement about a person or group of persons, a simple classification of persons or groups of persons based on characteristics, such as their age, sex, and height, need not necessarily lead to evaluation 108 . The scope of 'classification' is therefore broader than 'evaluation'  and  can  also  cover  other  types  of  classifications  or  categorisations  of natural persons or groups of persons based on criteria that do not necessarily involve a particular assessment or judgement about those persons or groups of persons and their characteristics or behaviour.
(154) The term ' evaluation' also relates to the concept of 'profiling', which is regulated by Union data protection legislation 109 and constitutes a specific form of evaluation. While no direct reference is made in Article 5(1)(c) AI Act to that concept or that legislation, 110 they may also be relevant for the prohibition contained in that provision, as well as for other prohibitions in the AI Act, 111 when the evaluation occurs in an automated fashion by an AI system based on personal data. Profiling means the use of information about an individual (or group of individuals) and evaluating their characteristics or behaviour patterns in order to place them into a certain category or group, in particular to analyse and/or make predictions about, for example, their ability to perform a task; interests; or likely behaviour'. 112 Profiling of natural persons under EU data protection law, when conducted through AI systems, may therefore also be covered by Article 5(1)(c) AI Act.
108 Article 29 Working Party, Guidelines on Automated individual decision making and Profiling for the purposes of Regulation 2016/679, WP251rev.01, , p. 7.
109 See Article 4(4) and Article 22 GDPR and Article 11 LED. 