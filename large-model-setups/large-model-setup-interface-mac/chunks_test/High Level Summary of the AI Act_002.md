- ∞ compiling  facial  recognition  databases by  untargeted  scraping  of  facial  images  from  the internet or CCTV footage.
- ∞ inferring  emotions  in  workplaces  or  educational  institutions ,  except  for  medical  or  safety reasons.
- ∞ biometric categorisation systems inferring sensitive attributes (race, political opinions, trade union  membership,  religious  or  philosophical  beliefs,  sex  life,  or  sexual  orientation),  except labelling or filtering  of  lawfully  acquired  biometric  datasets  or  when  law  enforcement categorises biometric data.
- ∞ 'real-time'  remote  biometric  identification  (RBI)  in  publicly  accessible  spaces  for  law enforcement , except when:
- o targeted searching for missing persons, abduction victims, and people who have been human trafficked or sexually exploited;
- o preventing  specific,  substantial  and  imminent  threat  to  life  or  physical  safety,  or foreseeable terrorist attack; or
- o identifying suspects in serious crimes (e.g., murder, rape, armed robbery, narcotic and illegal weapons trafficking, organised crime, and environmental crime, etc.).
## Notes on remote biometric identification:
- ¥ Using AI-enabled real-time RBI is  only  allowed when not using the tool would cause harm, particularly regarding the seriousness, probability and scale of such harm, and must account for affected persons' rights and freedoms.
- ¥ Before  deployment,  police  must  complete  a fundamental  rights  impact  assessment and register the system in the EU database , though, in duly justified cases of urgency, deployment can commence without registration, provided that it is registered later without undue delay.
- ¥ Before deployment,  they  also must  obtain authorisation from  a judicial authority or independent administrative authority , though, in duly justified cases of urgency, deployment 1 can commence without authorisation, provided that authorisation is requested within 24 hours. If authorisation is rejected, deployment must cease immediately, deleting all data, results, and outputs.
Independent administrative authorities may be subject to greater political influence than judicial authorities (Hacker, 1 2024).
## High risk AI systems (Chapter III)
## Classification rules for high-risk AI systems (Art. 6)
## High risk AI systems are those:
- ∞ used  as  a  safety  component  or  a  product  covered  by  EU  laws  in  Annex  I AND required  to undergo a third-party conformity assessment under those Annex I laws; OR
- ∞ those under Annex III use cases (below), except if:
- o the AI system performs a narrow procedural task;
- o improves the result of a previously completed human activity;
- o detects  decision-making  patterns  or  deviations  from  prior  decision-making  patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or
- o performs  a  preparatory  task  to  an  assessment  relevant  for  the  purpose  of  the  use cases listed in Annex III.
- ∞ The Commission can add or modify the above conditions through delegated acts if there is concrete evidence that an AI system falling under Annex III does not pose a significant risk to health,  safety  and  fundamental rights.  They  can  also  delete  any  of  the  conditions  if  there  is concrete evidence that this is needed to protect people.
- ∞ AI systems are always considered high-risk if it profiles individuals, i.e. automated processing of  personal  data  to  assess  various  aspects  of  a  person's  life,  such  as  work  performance, economic situation, health, preferences, interests, reliability, behaviour, location or movement.
- ∞ Providers  that  believe  their  AI  system,  which  fails  under  Annex  III,  is  not  high-risk,  must document such an assessment before placing it on the market or putting it into service.
- ∞ 18 months after entry into force, the Commission will provide guidance on determining if an AI system is high risk, with list of practical examples of high-risk and non-high risk use cases.
## Requirements for providers of high-risk AI systems (Art. 8-17)
## High risk AI providers must:
- ∞ Establish a risk management system throughout the high risk AI system's lifecycle;
- ∞ Conduct data governance , ensuring that training, validation and testing datasets are relevant, sufficiently  representative  and,  to  the  best  extent  possible,  free  of  errors  and  complete according to the intended purpose.
- ∞ Draw up technical documentation to demonstrate compliance and provide authorities with the information to assess that compliance.
- ∞ Design their high risk AI system for record-keeping to enable it to automatically record events relevant  for  identifying  national  level  risks  and  substantial  modifications  throughout  the system's lifecycle.
- ∞ Provide instructions for use to downstream deployers to enable the latter's compliance.
- ∞ Design their high risk AI system to allow deployers to implement human oversight.
- ∞ Design their high risk  AI  system  to  achieve  appropriate  levels  of accuracy, robustness, and cybersecurity .
