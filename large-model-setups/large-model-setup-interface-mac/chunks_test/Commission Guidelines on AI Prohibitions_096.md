If this is not the purpose or outcome of the biometric categorisation, the prohibition does not apply, for example if a whole group is categorised without looking at the individual.
Examples of individual categorisation include:
- -  AI systems that conduct ' Attribute Estimation' (count demographics), including for example  ' age, gender, ethnicity', on the basis of for example bodily features, such as face, height, or skin, eye and hair colour (or a combination thereof).
- -  AI  systems  capable  of  categorising  individuals  and  singling  them  out  based  on  a specific feature (e.g. a scar under the right eye), or because they have a tattoo on their right hand.
These  use-cases  are  examples  for  individual  biometric  categorisation.  For  these examples to fall within the prohibition of Article 5(1)(g) AI Act all conditions of that provision must be fulfilled.
## . To deduce or infer their race, political opinions, trade union membership,  religious  or  philosophical  beliefs,  sex  life  or sexual orientation
- (283) Article 5(1)(g) AI Act prohibits only biometric categorisation systems which have as their objective to deduce or infer a limited number of sensitive characteristics: race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation.
For example, systems prohibited under Article 5(1)(g) AI Act include:
- - a biometric categorisation system that claims to be capable of deducing an individual's race  from  their  voice  (This  is  different  from  a  system  that  categorises  persons
according to skin or eye colour, or a system that analyses the DNA of victims of crimes in view of their origin. Those systems would not be prohibited).
- - a biometric categorisation system that claims to be capable of deducing an individual's religious orientation from their tattoos or faces.
## . Out of scope
(284) The prohibition in Article 5(1)(g) AI Act does not cover AI systems engaged in the labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data, including in the area of law enforcement. This is further explained in recital 30 AI Act 177 .
(285) The labelling or filtering of biometric datasets may be done by biometric categorisation systems precisely to guarantee that the data equally represent all demographic groups, and not, for example, over-represent one specific group. If the data used for training an algorithm are biased against a specific group (i.e. systematic differences in the data exist between groups due to the way the data are collected, or data is historically biased), the algorithm may replicate this bias, possibly resulting in unlawful discrimination against persons or groups of persons. 178 For this reason, labelling on the basis of some protected sensitive  information  may  be  necessary  for  high-quality  data,  precisely  to  prevent discrimination. The AI Act may even require labelling operations to conform to the AI Act's requirements for high-risk AI systems. 179 Such labelling or filtering of biometric data is therefore explicitly exempted from the prohibition in Article 5(1)(g) AI Act. The prohibition  only  applies  where  biometric  data  is  categorised  to  infer  race,  political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation.
Examples of permissible labelling or filtering include:
- - the labelling of biometric data to avoid cases where a member of an ethnic group has a lower chance of being invited to a job interview because the algorithm was 'trained' based on data where that particular group performs worse, i.e. has worse outcomes than other groups. 180
- - the categorisation of patients using images according to their skin or eye colour may be important for medical diagnosis, for example cancer diagnoses.
177 Recital 30 AI Act: 'That prohibition should not cover the lawful labelling, filtering or categorisation of biometric data sets acquired in line with Union or national law according to biometric data, such as the sorting of images according to hair colour or eye colour, which can for example be used in the area of law enforcement'.
178 Ibid.
179 See e.g., Article 10 and 17 AI Act.
