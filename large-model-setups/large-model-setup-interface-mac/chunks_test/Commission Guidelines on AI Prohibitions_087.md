141 Article 20 of Regulation (EU) 2024/1624.
- - A customs authority uses AI risk analytic tools to predict the likelihood of the location of narcotics or illicit goods, for example on the basis of known trafficking routes.
- - A police department uses AI-driven systems to detect and locate gunshots in real time. The system employs acoustic sensors in urban areas to identify gunfire sounds and triangulate their location, providing officers with actionable data to aid detection and investigation of crimes.
(213) It  may,  however,  not  always  be  evident  how  to  distinguish location-based  crime predictive systems from individual predictive systems assessing the risk of a person committing  a  crime.  To  the  extent  that  an  AI  system  carries  out  location-based predictive policing and then considers the risk score of the location as an aspect in the profiling of a person, that system should be considered person-based and in principle covered  by  Article  5(1)(d)  AI  Act,  although  it  may  fall  outside  the  scope  of  the prohibition on other grounds.
For example, if location-based or geospatial information or place-based information is linked to information relating to an individual (e.g., the place of residences of a given person) and the AI system would assess the risk that that person is likely to commit crime based solely on profiling of the individual concerned, including based on his or her residence where crime is high, that system should be considered person-based.
## . AI systems that support human assessments based on objective and verifiable facts linked to a criminal activity
(214) Article 5(1)(d) AI Act provides that the prohibition does not apply to AI systems used to support the human assessment of the involvement of a natural person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity.  In such a case, the individual crime risk assessments and predictions would also not be based solely on profiling or the assessment of personal characteristics and would therefore not be prohibited.
Examples of AI systems falling outside the scope of the prohibition  for  this reason include:
- - The use of an AI system for profiling and categorisation of actual behaviour, such as reasonably suspicious dangerous behaviour in a crowd that someone is preparing and is likely to commit a crime, and there is a meaningful human assessment of the AI classification. In this case, the risk assessment made by the human with the support of AI is  not  solely  based  on  the  personal  traits  or  the  profiling,  but  on  objective  and verifiable facts linked to the threatening criminal behaviour of that person that has been reviewed by a human before action taken.
- -  The  police  is  investigating  the  risk  of  a  possible  armed  robbery  and  suspects  two individuals. Several verifiable and objective facts are present on which that suspicion is based, such as verifiable participation and dialogues in dark web chat groups for
purchasing  arms.  An  AI  system  combining  geospatial  predictive  or  place-based policing information and Automated Number Plate Registration (ANPR) information of  vehicles  belonging  to  the  suspects  supports  the  human  assessment  in  the investigation  based  on  verifiable  and  objective  facts  directly  linked  to  a  specific criminal activity.
- - The use of an AI system that assesses the risk whether a prisoner should receive the benefit of an early release. The AI profile of the affected person or assessment of their personality traits and characteristics only support the human assessment of objective and  verifiable  facts  related  to  past  criminal  offences  and  demonstrated  behaviour relevant to rehabilitation.
- -  A  judge  conducts  a  pre-trial  detention  hearing  for  a  person  accused  of  a  serious criminal  offence  to  assess  whether  non-custodial  measures  can  be  applied.  The decision is based on an assessment of the existence of valid grounds to imposed pretrial detention, such as the likelihood that the suspect or accused person will commit another offence if not detained or that they will abscond or obstruct the proper conduct of the investigation. To assist in this process, the judge uses an AI risk assessment tool trained on data including past criminal history of individuals in similar cases, as well as factors such as age group, social behaviour, income, and employment status.
- - AI system is used to support the assessment of a human officer to assess the risk of an individual serving a non-custodial sentence violating release conditions or absconding based on past criminal behaviours and objective facts that give grounds for suspicion such as adherence to conditions of release, psychological assessment outcomes and recommendations from other community services the individual may be using. Based on this information, the officer decides whether to maintain the status quo or revise the conditions of release.
- - AI systems used by custom authorities to assess the risk of goods entering the EU not complying with the legislation applicable at the border (e.g. which may include bans on import of illicit drugs, export sanctions contravention or other illegal activity) to identify  situations  where  a  customs  control  should  be  carried  out.  The  AI  system assesses objective and verifiable information provided to the customs related to the goods and their supply chains (e.g. nature and value of the goods, container number, means of transport for concealment of other goods, prior knowledge relating to the compliance of goods of the particular description and origin with requirements for their  importation  to  or  exportation  from  the  Union).  In  certain  cases,  it  may  also process  information  about  the  prior  involvement  of  the  importer  or  exporter  in irregularities related to import of goods, their affiliation to criminal organizations or a criminal record for drug trafficking. Such systems are out of scope of the prohibition because any prediction of a likelihood of a natural person to be involved in an import or  export  of  illicit  goods  is  not  solely  based  on  profiling,  but  on  objective  and verifiable  information  related  to  the  goods  and  the  importer  or  exporter's  prior
involvement in criminal activity and subject to a human review to determine whether or not the situation requires a customs control or risk mitigation action.
## . AI  systems  used  for  crime  predictions  and  assessments  in relation to legal entities
(215) The prohibition in Article 5(1)(d) AI Act applies only to individual predictions and risk assessments of natural persons, thus typically excluding crime predictive systems that profile legal entities such as companies or non-governmental organisations.
## For example,
- - A tax or customs authority is using an AI system to analyse large amounts of data on transactions and tax declarations and customs data of companies for assessing the risk of a company committing tax or customs fraud constituting a criminal offence.
- -  AI  systems  used  to  assist  customs  authorities  to  help  identify  situations  where  an instruction not to send illicit goods to the EU should be issued to legal entities.
(216) At the same time, there may be borderline cases where a natural person acts via a legal entity  as  a  'sole  trader'  or  as  an  independent  professional  (e.g.  a  lawyer).  In  such circumstances, the prohibition in Article 5(1)(d) AI Act may apply provided that all conditions  are  fulfilled,  since  the  AI  system  profiles  a  specific  natural  person  and assesses or predicts their risk of committing a criminal offence, even if this is done for purposes in relation to the commercial activity undertaken by the natural person.
## . AI systems used for individual predictions of administrative offences
(217) The prohibition in Article 5(1)(d) AI Act applies only for the prediction of criminal offences,  thus  excluding  administrative  offences  from  its  scope,  the  prosecution  of which is, in principle, less intrusive for the fundamental rights and freedoms of people.
For  example,  a  public  authority  using  AI  in  the  context  of  an  administrative investigation  to  assess  the  risk  of  potential  offenders  of  committing  minor  offences (such  as  petty  traffic  offences)  or  irregularities  in  tax,  procurement  or  expenditure processes would not fall within the scope of the prohibition in Article 5(1)(d) AI Act, even in cases where information might be gathered for possible involvement of the natural persons in criminal offences as a result of the administrative investigations and checks.
(218) Whether an offence is administrative or criminal in nature may depend on Union or national law. For offences that are not directly regulated by Union law, the national qualification of the offence is subject to scrutiny by CJEU since 'criminal offence' is a concept that has autonomous meaning within the EU law and should be interpreted consistently across Member States. The CJEU has concluded, in a different context, that the classification of the offences by the Member States is not conclusive in that
respect 142 . Relevant criteria used to assess the nature of the offence (criminal or not) may be found in relevant case-law of the CJEU and the European Court of Human Rights (ECtHR). 143
## . Interplay with other Union legal acts
(219) The interplay of the prohibition in Article 5(1)(d) AI Act with the LED and GDPR is relevant when assessing the lawfulness of personal data processing under Union data protection law, such as the GDPR and the LED. In particular, Article 5(1)(d) AI Act imposes a specific prohibition for law enforcement authorities, other public authorities and private entities falling within the scope of the prohibition to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics. With regard to  LED,  Article  5(1)(d)  AI  Act  is  without  prejudice  to  Article  11(3)  LED,  which prohibits profiling resulting in (direct or indirect) discrimination.
(220) The interplay of the prohibition in Article 5(1)(d) AI Act with Directive (EU) 2016/343 on the presumption of innocence is also relevant, since both acts are concerned - directly in the case of the Directive and indirectly in the case of the AI Act (see its Recital 42), with the fundamental right to be presumed innocent until proven guilty according to law. 144 While  the  Directive  applies  from  the  moment  that  a  person  is  suspected  or accused of having committed a criminal offence 145 , the AI Act has a broader scope of application and applies already at the stage of prediction and crime prevention before a formal criminal investigation is opened against a particular person and even in cases when such predictions and risk assessments are made by private actors falling within the scope of Article 5(1)d) AI Act and not by competent law enforcement authorities, including judicial authorities.
(221) Even in  cases  where  the  prohibition  in  Article  5(1)(d)  AI  Act  does  not  apply,  it  is important to emphasize that applicable Union and national law remains fully applicable, including  in  particular  data  protection,  criminal  procedural  and  police  law  and safeguards  that  may  further  restrict  or  impose  additional  conditions  on  the  use  of individual crime predictive AI systems.
142 See, for example, Judgment of the Court (Grand Chamber) of 14 November 2013 Proceedings concerning the enforcement of a financial penalty issued against - Marián Baláž, Case C-60/12, ECLI identifier: ECLI:EU:C:2013:733.
143 According to the CJEU's case law, it is for national courts to determine whether a non-criminal penalty may be regarded as 'criminal' in light of the so-called 'Engel criteria', See: ECtHR, judgment of 8 June 1976, Engel and Others v. the Netherlands, Application nos. 5100/71, 5101/71, 5102/71, 5354/72 and 5370/72, CE:ECHR:1976:0608JUD000510071, paragraph 82. Originally developed by the European Court of Human Rights (ECtHR) and subsequently endorsed by the CJEU, these criteria are alternative and not cumulative. When examining whether a penalty has a criminal nature, the competent national court should assess: (1) the classification of the relevant provisions under domestic law; (2) the very nature of the offence; and (3) the severity of the penalty. In evaluating the nature of the offence, aspects taken into account include inter alia whether the proceedings are instituted by a public body with statutory powers of enforcement; whether the legal rule has a punitive or deterrent purpose; whether the legal rule seeks to protect the general interests of society usually protected by criminal law; whether the imposition of any penalty is dependent upon a finding of guilt. Regarding the severity of the penalty, relevant reference is the maximum potential penalty provided in the national law. These criteria are alternative and not necessarily cumulative. See European Court of Human Rights, Guide on Article 6 of the European Convention on Human Rights, Right to a fair trial (criminal limb), updated 29 February 2024. 