- (61) Certain  AI  systems  intended  for  the  administration  of  justice  and  democratic  processes  should  be  classified  as high-risk, considering their potentially significant impact on democracy, the rule of  law, individual freedoms as well as the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high-risk AI systems intended to be used by a judicial authority or on its behalf to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. AI systems intended to be used by alternative dispute resolution bodies for  those purposes should also be considered to be high-risk when the outcomes of the alternative dispute resolution proceedings produce legal effects for  the parties. The use of AI tools can support the decision-making power of judges or judicial independence, but should  not  replace  it:  the  final  decision-making  must  remain  a  human-driven  activity.  The  classification  of  AI systems as high-risk should not, however, extend to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks.
- (62) Without  prejudice  to  the  rules  provided  for  in  Regulation  (EU)  2024/900  of  the  European  Parliament  and  of  the Council ( 34 ),  and  in  order  to  address  the  risks  of  undue  external  interference  with  the  right  to  vote  enshrined  in Article 39 of the Charter, and of adverse effects on democracy and the rule of law, AI systems intended to be used to influence  the  outcome  of an  election  or  referendum  or  the  voting  behaviour  of  natural  persons  in  the  exercise  of their  vote  in  elections  or  referenda  should  be  classified  as  high-risk  AI  systems  with  the  exception  of  AI  systems whose  output  natural  persons  are  not  directly  exposed  to,  such  as  tools  used  to  organise,  optimise  and  structure political  campaigns  from  an  administrative  and  logistical  point  of  view.
- (63) The fact that an AI system is classified as a high-risk AI system under  this Regulation should not be interpreted as indicating that the use of the system is lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable  requirements  resulting  from  the  Charter  and  from  the  applicable  acts  of  secondary  Union  law  and national law. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant, unless it is specifically otherwise provided for in this  Regulation.
- (64) To mitigate the risks from high-risk AI systems placed on the market or put into service and to ensure a high level of trustworthiness,  certain  mandatory  requirements  should  apply  to  high-risk  AI  systems,  taking  into  account  the intended  purpose  and  the  context  of  use  of  the  AI  system  and  according  to  the  risk-management  system  to  be established by the provider. The measures adopted by the providers to comply with the mandatory requirements of this  Regulation  should  take  into  account  the  generally  acknowledged  state  of  the  art  on  AI,  be  proportionate  and effective  to  meet  the  objectives  of  this  Regulation.  Based  on  the  New  Legislative  Framework,  as  clarified  in Commission  notice  'The  'Blue  Guide'  on  the  implementation  of  EU  product  rules  2022',  the  general  rule  is  that more  than  one  legal  act  of  Union  harmonisation  legislation  may  be  applicable  to  one  product,  since  the  making available  or  putting  into  service  can  take  place  only  when  the  product  complies  with  all  applicable  Union harmonisation  legislation.  The  hazards  of  AI  systems  covered  by  the  requirements  of  this  Regulation  concern different aspects than the existing Union harmonisation legislation and therefore the requirements of this Regulation would complement the existing body of  the Union  harmonisation  legislation.  For  example,  machinery or  medical devices  products  incorporating  an  AI  system  might  present  risks  not  addressed  by  the  essential  health  and  safety
( 32 ) Regulation (EC) No 810/2009 of the European Parliament and of the Council of 13 July 2009 establishing a Community Code on Visas  (Visa  Code)  (OJ  L  243,  ,  p.  1).
( 33 ) Directive  2013/32/EU  of  the  European  Parliament  and  of  the  Council  of  26  June  2013  on  common procedures  for  granting and withdrawing international  protection  (OJ  L  180,  ,  p.  60).
( 34 ) Regulation (EU) 2024/900 of the European parliament and of the Council of 13 March 2024 on the transparency and targeting of political  advertising  (OJ  L,  2024/900,  ,  ELI:  http://data.europa.eu/eli/reg/2024/900/oj).
requirements  set  out  in  the  relevant  Union  harmonised  legislation,  as  that  sectoral  law  does  not  deal  with  risks specific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To ensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product that  contains  one  or  more  high-risk  AI  system,  to  which  the  requirements  of  this  Regulation  and  of  the  Union harmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply, should have flexibility with regard to operational decisions on how to ensure compliance of a product that contains one  or  more  AI  systems  with  all  the  applicable  requirements  of  that  Union  harmonised  legislation  in  an  optimal manner.  That  flexibility  could  mean,  for  example  a  decision  by  the  provider  to  integrate  a  part  of  the  necessary testing and reporting processes, information and documentation required under this Regulation into already existing documentation  and  procedures  required  under  existing  Union  harmonisation  legislation  based  on  the  New Legislative  Framework  and  listed  in  an  annex  to  this  Regulation.  This  should  not,  in  any  way,  undermine  the obligation  of  the  provider  to  comply  with  all  the  applicable  requirements.
- (65) The risk-management system should consist of a continuous, iterative process that is planned and run throughout the entire lifecycle of a high-risk AI system. That process should be aimed at identifying and mitigating the relevant risks  of  AI  systems  on  health,  safety  and  fundamental  rights.  The  risk-management  system  should  be  regularly reviewed  and  updated  to  ensure  its  continuing  effectiveness,  as  well  as  justification  and  documentation  of  any significant  decisions  and  actions  taken  subject  to  this  Regulation.  This  process  should  ensure  that  the  provider identifies  risks  or  adverse  impacts  and  implements  mitigation  measures  for  the  known  and  reasonably  foreseeable risks  of  AI  systems  to  the  health,  safety  and  fundamental  rights  in  light  of  their  intended  purpose  and  reasonably foreseeable  misuse,  including  the  possible  risks  arising  from  the  interaction  between  the  AI  system  and  the environment within which it operates. The risk-management system should adopt the most appropriate risk-management  measures  in  light  of the state of the  art  in  AI.  When  identifying  the  most  appropriate risk-management  measures,  the  provider  should  document  and  explain  the  choices  made  and,  when  relevant, involve experts and external stakeholders. In identifying the reasonably foreseeable misuse of high-risk AI systems, the  provider  should  cover  uses  of  AI  systems  which,  while  not  directly  covered  by  the  intended  purpose  and provided  for  in  the  instruction  for  use  may  nevertheless  be  reasonably  expected  to  result  from  readily  predictable human  behaviour  in  the  context  of  the  specific  characteristics  and  use  of  a  particular  AI  system.  Any  known  or foreseeable circumstances related to the use of  the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights  should  be  included  in  the  instructions  for  use  that  are  provided  by  the  provider.  This  is  to  ensure  that  the deployer  is  aware  and  takes  them  into  account  when  using  the  high-risk  AI  system.  Identifying and  implementing risk mitigation measures for foreseeable misuse under this Regulation should not require specific additional training for the high-risk AI system by the provider to address foreseeable misuse. The providers however are encouraged to consider such additional training measures to mitigate reasonable foreseeable misuses as necessary and appropriate.
- (66) Requirements should apply to high-risk AI systems as regards  risk management, the quality and relevance of data sets used, technical documentation and record-keeping, transparency and the provision of information to deployers, human  oversight,  and  robustness,  accuracy  and  cybersecurity.  Those  requirements  are  necessary  to  effectively mitigate the risks for health, safety and fundamental rights. As no other less trade restrictive measures are reasonably available  those  requirements  are  not  unjustified  restrictions  to  trade.
- (67) High-quality  data  and  access  to  high-quality  data  plays  a  vital  role  in  providing  structure  and  in  ensuring  the performance of many AI systems, especially when techniques involving the training of models are used, with a view to  ensure  that  the  high-risk  AI  system  performs  as  intended  and  safely  and  it  does  not  become  a  source  of discrimination  prohibited  by  Union  law.  High-quality  data  sets  for  training,  validation  and  testing  require  the implementation  of  appropriate  data  governance  and  management  practices.  Data  sets  for  training,  validation  and testing,  including  the  labels,  should  be  relevant,  sufficiently  representative,  and  to  the  best  extent  possible  free  of errors and complete in view of the intended purpose of the system. In order to facilitate compliance with Union data protection  law,  such  as  Regulation  (EU)  2016/679,  data  governance  and  management  practices  should  include,  in the case of personal data, transparency about the original purpose of the data collection. The data sets should also have the appropriate statistical properties, including as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used, with specific attention to the mitigation of possible biases in the data sets, that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination  prohibited  under  Union  law,  especially  where  data  outputs  influence  inputs  for  future  operations
(feedback loops). Biases can for example be inherent in underlying data sets, especially when historical data is being used, or generated when the systems are implemented in real world settings. Results provided by AI systems could be influenced by such inherent biases that are inclined to gradually increase and thereby perpetuate and amplify existing discrimination,  in particular  for  persons  belonging  to  certain  vulnerable  groups,  including  racial  or  ethnic  groups. The requirement for the data sets to be to the best extent possible complete and free of errors should not affect the use of privacy-preserving techniques in the context of the development and testing of AI systems. In particular, data sets  should  take  into  account,  to  the  extent  required  by  their  intended  purpose,  the  features,  characteristics  or elements that are particular  to the specific geographical, contextual, behavioural or functional setting which the AI system is intended to be used. The requirements related to data governance can be complied with by having recourse to  third  parties  that offer  certified  compliance services  including verification of data governance, data set integrity, and data training, validation and testing practices, as far as compliance with the data requirements of this Regulation are  ensured.
- (68) For  the  development and assessment of high-risk AI systems, certain actors, such as providers, notified bodies and other relevant entities, such as European Digital Innovation Hubs, testing experimentation facilities and researchers, should be able to access and use high-quality data sets within the fields of activities of those actors which are related to this Regulation. European common data spaces established by the Commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful, accountable and  non-discriminatory  access  to  high-quality  data  for  the  training,  validation  and  testing  of  AI  systems.  