Instructions for  use should be made available in a language which can be easily understood by target deployers, as determined by the Member State concerned.
- (73) High-risk  AI  systems  should  be  designed  and  developed  in  such  a  way  that  natural  persons  can  oversee  their functioning, ensure that they are used as intended and that their impacts are addressed over the system's lifecycle. To that end, appropriate human oversight measures should be identified by the provider of the system before its placing on  the  market  or  putting  into  service.  In  particular,  where  appropriate,  such  measures  should  guarantee  that  the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. It is also essential, as appropriate, to ensure that high-risk AI systems include mechanisms to guide and inform a natural person to whom human oversight has been assigned to make informed decisions if, when and how to intervene in order to avoid negative consequences or risks, or stop the system if it does not perform as intended. Considering the significant consequences for persons in the case of an incorrect  match  by  certain  biometric  identification  systems,  it  is  appropriate  to  provide  for  an  enhanced  human oversight requirement for those systems so that no action or decision may be taken by the deployer on the basis of the  identification  resulting  from  the  system  unless  this  has  been  separately  verified  and  confirmed  by  at  least  two natural persons. Those persons could be from one or  more entities and include the person operating or  using the system. This requirement should not pose unnecessary burden or delays and it could be sufficient that the separate verifications  by  the  different  persons  are  automatically  recorded  in  the  logs  generated  by  the  system.  Given  the specificities  of  the  areas  of  law  enforcement,  migration,  border  control  and  asylum,  this  requirement  should  not apply where Union or  national law considers the application  of  that  requirement  to be  disproportionate.
- (74) High-risk  AI  systems  should  perform  consistently  throughout  their  lifecycle  and  meet  an  appropriate  level  of accuracy,  robustness  and  cybersecurity,  in  light  of  their  intended  purpose  and  in  accordance  with  the  generally acknowledged state of the art. The Commission and relevant organisations and stakeholders are encouraged to take due  consideration  of  the  mitigation  of  risks  and  the  negative  impacts  of  the  AI  system.  The  expected  level  of performance  metrics  should  be  declared  in the accompanying  instructions  of  use.  Providers  are urged  to communicate that information to deployers in a clear and easily understandable way, free of misunderstandings or misleading statements. Union law on legal metrology, including Directives 2014/31/EU ( 35 )  and  2014/32/EU ( 36 )  of the  European  Parliament  and  of  the  Council,  aims  to  ensure  the  accuracy  of  measurements  and  to  help  the transparency and fairness of commercial transactions. In that context, in cooperation with relevant stakeholders and organisation, such as metrology and benchmarking authorities, the Commission should encourage, as appropriate, the  development  of  benchmarks  and  measurement  methodologies  for  AI  systems.  In  doing  so,  the  Commission should  take  note  and  collaborate  with  international  partners  working  on  metrology  and  relevant  measurement indicators  relating  to  AI.
( 35 ) Directive 2014/31/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating to the making available on the market of non-automatic weighing instruments (OJ L 96, , p. 107). ( 36 ) Directive 2014/32/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating  to  the  making available  on  the  market of  measuring  instruments  (OJ  L  96,  ,  p.  149).
- (75) Technical robustness is a key requirement for high-risk AI systems. They should be resilient in relation to harmful or otherwise undesirable behaviour  that may result from limitations within the systems or  the environment in which the systems operate (e.g. errors, faults, inconsistencies, unexpected situations). Therefore, technical and organisational  measures  should  be  taken  to  ensure  robustness  of  high-risk  AI  systems,  for  example  by  designing and developing appropriate technical solutions to prevent or minimise harmful or otherwise undesirable behaviour. Those technical solution may include for instance mechanisms enabling the system to safely interrupt its operation (fail-safe  plans)  in  the  presence  of  certain  anomalies  or  when  operation  takes  place  outside  certain  predetermined boundaries.  Failure  to  protect  against  these  risks  could  lead  to  safety  impacts  or  negatively  affect  the  fundamental rights,  for  example  due  to  erroneous  decisions  or  wrong  or  biased  outputs  generated  by  the  AI  system.
- (76) Cybersecurity  plays  a  crucial  role  in  ensuring  that  AI  systems  are  resilient  against  attempts  to  alter  their  use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system's vulnerabilities.  Cyberattacks  against  AI  systems  can  leverage  AI  specific  assets,  such  as  training  data  sets  (e.g.  data poisoning) or  trained models (e.g. adversarial attacks or  membership inference), or exploit vulnerabilities in the AI system's digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures, such as security controls, should therefore be taken by the providers of high-risk AI systems, also taking  into  account  as  appropriate  the  underlying  ICT  infrastructure.
- (77) Without  prejudice  to  the  requirements  related  to  robustness  and  accuracy  set  out  in  this  Regulation,  high-risk  AI systems  which fall  within  the  scope  of  a  regulation  of  the  European  Parliament  and  of  the  Council  on  horizontal cybersecurity requirements for products with digital elements, in accordance with that regulation may demonstrate compliance  with  the cybersecurity  requirements of this Regulation by fulfilling the essential cybersecurity requirements set out in that regulation. When high-risk AI systems fulfil the essential requirements of a regulation of the  European  Parliament  and  of  the  Council  on  horizontal  cybersecurity  requirements  for  products  with  digital elements, they should be deemed compliant with the cybersecurity requirements set out in this Regulation in so far as  the  achievement  of  those  requirements  is  demonstrated  in  the  EU  declaration  of  conformity  or  parts  thereof issued  under  that  regulation.  To  that  end,  the  assessment  of  the  cybersecurity  risks,  associated  to  a  product  with digital elements classified as high-risk AI system according to this Regulation, carried out under a regulation of the European  Parliament  and  of  the  Council  on  horizontal  cybersecurity  requirements  for  products  with  digital elements,  should  consider  risks  to  the  cyber  resilience  of  an  AI  system  as  regards  attempts  by  unauthorised  third parties  to  alter  its  use,  behaviour  or  performance,  including  AI  specific  vulnerabilities  such  as  data  poisoning  or adversarial  attacks,  as  well  as,  as  relevant,  risks  to  fundamental  rights  as  required  by  this  Regulation.
- (78) The  conformity  assessment  procedure  provided  by  this  Regulation  should  apply  in  relation  to  the  essential cybersecurity  requirements  of a  product  with  digital  elements  covered  by a  regulation  of  the  European  Parliament and  of  the  Council  on  horizontal  cybersecurity  requirements  for  products  with  digital  elements  and  classified  as a high-risk AI system under this Regulation. However, this rule should not result in reducing the necessary level of assurance for critical products with digital elements covered by a regulation of the European Parliament and of the Council  on  horizontal  cybersecurity  requirements  for  products  with  digital  elements.  Therefore,  by  way  of derogation from this rule, high-risk AI systems that fall within the scope of this Regulation and are also qualified as important and critical products with digital elements pursuant to a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for  products with digital elements and to which the conformity assessment  procedure  based  on  internal  control  set  out  in  an  annex  to  this  Regulation  applies,  are  subject  to  the conformity  assessment  provisions  of  a  regulation  of  the  European  Parliament  and  of  the  Council  on  horizontal cybersecurity requirements for products with digital elements insofar as the essential cybersecurity requirements of that  regulation  are  concerned.  In  this  case,  for  all  the  other  aspects  covered  by  this  Regulation  the  respective provisions on conformity assessment based on internal control set out in an annex to this Regulation should apply. Building on the knowledge and expertise of ENISA on the cybersecurity policy and tasks assigned to ENISA under the Regulation (EU) 2019/881 of the European Parliament and of the Council ( 37 ), the Commission should cooperate with ENISA on issues related to  cybersecurity of  AI  systems.
( 37 ) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency for  Cybersecurity)  and  on  information  and  communications  technology  cybersecurity  certification  and  repealing  Regulation (EU)  No  526/2013  (Cybersecurity  Act)  (OJ  L  151,  ,  p.  15).
- (79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on the market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is the  person  who  designed  or  developed  the  system.
- (80) As  signatories  to  the  United  Nations  Convention  on  the  Rights  of  Persons  with  Disabilities,  the  Union  and  the Member States are legally obliged to protect persons with disabilities from discrimination and promote their equality, to ensure that persons with disabilities have access, on an equal basis with others, to information and communications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the growing importance and use of AI systems, the application of universal design principles to all new technologies and services should ensure full and equal access for everyone potentially affected by or using AI technologies, including persons  with  disabilities,  in  a  way  that  takes  full  account  of  their  inherent  dignity  and  diversity.  It  is  therefore essential that providers ensure full compliance with accessibility requirements, including Directive (EU) 2016/2102 of  the  European  Parliament  and  of  the  Council ( 38 )  and  Directive  (EU)  2019/882.  Providers  should  ensure compliance with these requirements by design. Therefore, the necessary measures should be integrated as much as possible  into  the  design  of  the  high-risk  AI  system.
- (81) The  provider  should  establish  a  sound  quality  management  system,  ensure  the  accomplishment  of  the  required conformity  assessment  procedure,  draw  up  the  relevant  documentation  and  establish  a  robust  post-market monitoring system. Providers of high-risk AI systems that are subject to obligations regarding quality management systems  under  relevant  sectoral  Union  law  should  have  the  possibility  to  include  the  elements  of  the  quality management system provided for in this Regulation as part of the existing quality management system provided for in  that  other  sectoral  Union  law.  The  complementarity  between  this  Regulation  and  existing  sectoral  Union  law should also be taken into account in future standardisation activities or guidance adopted by the Commission. Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality  management  system  as  part  of  the  quality  management  system  adopted  at  a  national  or  regional  level,  as appropriate, taking into account the specificities of  the sector and the competences and organisation of  the public authority concerned.
- (82) To enable enforcement of this Regulation and create a level playing field for operators, and, taking into account the different  forms  of  making  available  of  digital  products,  it  is  important  to  ensure  that,  under  all  circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI  system.  Therefore,  prior  to  making  their  AI  systems  available  in  the  Union,  providers  established  in  third countries should, by written mandate, appoint an authorised representative established in the Union. This authorised representative plays a pivotal role in ensuring the compliance of  the high-risk AI systems placed on the market or put into service in the Union by those providers who are not established in the Union and in serving as their contact person established  in  the  Union.
- (83) In  light  of  the  nature  and  complexity  of  the  value  chain  for  AI  systems  and  in  line  with  the  New  Legislative Framework, it is essential to ensure legal certainty and facilitate the compliance with this Regulation. Therefore, it is necessary  to  clarify  the  role  and  the  specific  obligations  of  relevant  operators  along  that  value  chain,  such  as importers  and  distributors  who  may  contribute  to  the  development  of  AI  systems.  In  certain  situations  those operators  could  act  in  more  than  one  role  at  the  same  time  and  should  therefore  fulfil  cumulatively  all  relevant obligations associated with those roles. For example, an operator could act as a distributor and an importer at the same time.
- (84) To ensure legal certainty, it  is  necessary  to clarify  that,  under  certain  specific  conditions,  any distributor,  importer, deployer or other third-party should be considered to be a provider of a high-risk AI system and therefore assume all the  relevant obligations.  This  would be the  case if  that  party  puts  its  name  or  trademark on a  high-risk  AI  system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations  are  allocated  otherwise.  This  would  also  be  the  case  if  that  party  makes  a  substantial  modification  to a high-risk AI system that has already been placed on the market or has already been put into service in a way that it remains a high-risk  AI  system  in  accordance  with  this  Regulation,  or  if  it  modifies  the  intended  purpose  of  an  AI system,  including  a  general-purpose  AI  system,  which  has  not  been  classified  as  high-risk  and  has  already  been placed on the market or put into service, in a way that the AI system becomes a high-risk AI system in accordance with  this  Regulation.  Those  provisions  should  apply  without  prejudice  to  more  specific  provisions  established  in certain  Union  harmonisation  legislation  based  on  the  New  Legislative  Framework,  together  with  which  this
( 38 ) Directive (EU) 2016/2102 of the European Parliament and of the Council of 26 October 2016 on the accessibility of the websites and mobile applications  of  public  sector bodies  (OJ  L  327,  ,  p.  