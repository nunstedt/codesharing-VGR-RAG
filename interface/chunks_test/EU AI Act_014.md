- (42) In  line  with  the  presumption  of  innocence,  natural  persons  in  the  Union  should  always  be  judged  on  their  actual behaviour.  Natural  persons  should  never  be  judged  on  AI-predicted  behaviour  based  solely  on  their  profiling, personality traits or characteristics, such as nationality, place of birth, place of residence, number of children, level of debt  or  type  of  car,  without  a  reasonable  suspicion  of  that  person  being  involved  in  a  criminal  activity  based  on objective verifiable facts and without human assessment thereof. Therefore, risk assessments carried out with regard to  natural  persons  in  order  to  assess  the  likelihood  of  their  offending  or  to  predict  the  occurrence  of  an  actual  or potential criminal offence based solely on profiling them or on assessing their  personality traits and characteristics should be prohibited. In any case, that prohibition does not refer to or touch upon risk analytics that are not based on the profiling of individuals or on the personality traits and characteristics of individuals, such as AI systems using risk  analytics  to  assess  the  likelihood  of  financial  fraud  by  undertakings  on  the  basis  of  suspicious  transactions  or risk analytic tools to predict the likelihood of the localisation of narcotics or illicit goods by customs authorities, for example on the basis  of  known trafficking  routes.
- (43) The placing on the market, the putting into service for that specific purpose, or the use of AI systems that create or expand  facial  recognition  databases  through  the  untargeted  scraping  of  facial  images  from  the  internet  or  CCTV footage,  should  be  prohibited  because  that  practice  adds  to  the  feeling  of  mass  surveillance  and  can  lead  to  gross violations  of  fundamental  rights,  including  the  right  to  privacy.
- (44) There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularly as  expression  of  emotions  vary  considerably  across  cultures  and  situations,  and  even  within  a  single  individual. Among  the  key  shortcomings  of  such  systems  are  the  limited  reliability,  the  lack  of  specificity  and  the  limited generalisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basis of their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of the concerned persons.  Considering  the  imbalance  of  power  in  the  context  of  work  or  education,  combined  with  the intrusive  nature  of  these  systems,  such  systems  could  lead  to  detrimental  or  unfavourable  treatment  of  certain natural persons or whole groups thereof. 