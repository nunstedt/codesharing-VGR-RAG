- -  Companies have a legitimate interest to evaluate customers for financial fraud and those practices are not affected by the prohibition, if the evaluation is based on relevant data such as transactional behaviour and metadata in the context of the services, past history and other factors from sources that are objectively relevant to determine the risk  of  fraud  and  if  the  detrimental  treatment  is  justified  and  proportionate  as  a consequence of the fraudulent behaviour.
- - Information collected through telematic devices that show that a driver is speeding or not maintaining safe driving practices used by an insurer that offers telematics-based tariffs  in  relation  to  a  policyholder's  high-risk  driving  behaviour  may  be  used  to increase the premium of that policyholder due to the higher risk of an accident caused by that driving behaviour, provided the increase in the premium is proportionate to the risky behaviour of the driver.
- - The collection and processing of data that is relevant and necessary for the intended legitimate purpose of the AI systems (e.g., health and schizophrenic data collected from various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in particular because it process relevant and necessary data and typically does not entail unjustified detrimental or unfavourable treatment of certain natural persons.
- -  Online platforms profiling users for safety reasons on their services based on data which is relevant for the context and purpose of assessment is out of scope of Article
125 Recital 31 AI Act.
126 See in particular Directive (EU) 2023/2225 of 18 October 2023 on credit agreements for consumers and repealing Directive 2008/48/EC and the European Banking Authority' Guidelines on loan origination and monitoring from 29 May 2020, EBA/GL/2020/06.
5(1)(c) AI Act, when the evaluation does not result in detrimental treatment that is disproportionate to the gravity of the user's misbehaviour.
- -  AI-enabled  targeted  commercial  advertising  is  out  of  scope,  where  it  is  based  on relevant data (e.g. users' preferences), it is done in line with Union law on consumer protection, data protection and digital services, and it does not result in detrimental or unfavourable treatment disproportionate to the gravity of the user's social behaviour (e.g. exploitative and unfair differential pricing).
- - AI systems using data collected in refugee camps (e.g., behavioural compliance) for decisions about resettlement or employment is not affected by the prohibition, given that this data is relevant for the purpose of assessment and provided that the procedures under applicable Union migration law are fulfilled to ensure the treatment is justified and proportionate.
- -  AI-enabled scoring by an online shopping platform which offers privileges to users with a strong purchase history and a low rate of product returns, such as a faster returns application process or returnless refunds are out of scope of Article 5(1)(c) AI Act, given that the advantages are justified and proportionate to reward positive behaviour and other users continue to have access to the standard return process.
- -  AI-evaluation  and  scoring  of  individuals  by  police  and  other  law  enforcement authorities that collect data about individuals' social behaviour from multiple contexts are out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific purposes  of  the  prevention,  detection,  prosecution  and  punishment  of  criminal offences,  and  where  the  detrimental  treatment  is  justified  and  proportionate  in accordance with substantive and procedural Union and national criminal and police law. It is also relevant to consider in this context the prohibition in Article 5(1)(d) AI Act,  which  imposes  additional  and  more  specific  conditions  for  AI-enabled  risk assessments  and  predictions  of  the  likelihood  of  a  person  committing  a  criminal offence which must not be solely based on profiling or the assessment of personality traits (see section 5).
## . Interplay with other Union legal acts
(178) Providers and deployers should carefully assess whether other applicable Union and national legislation applies to any particular AI scoring system used in their activities, in particular if there is more specific legislation that strictly regulates the types of data that can be used as relevant and necessary for specific evaluation purposes and if there are more specific rules and procedures to ensure justified and fair treatment.
- (179) AI-enabled social scoring practices by private parties acting as traders in business-toconsumer  relations  may  also  be  in  breach  of  Union  consumer  protection  law,  i.e., Directive  2005/29/EC  on  unfair  business-to-consumer  commercial  practices  (the 'UCPD').  The  UCPD  prohibits  commercial  practices  if  they  are  contrary  to  the
requirements of professional diligence and materially distort or are likely to materially distort  the  economic  behaviour  of  the  average  consumer  or  average  member  of  the group with regard to the product (Article 5 UCPD). The scoring practices may also be found misleading (Articles 6-7 UCPD) subject to case-by-case assessment of the impact of the commercial practice on the consumer's transactional decision.
- (180) Social scoring, whether by public or by private parties, may also be in breach of Union data  protection  laws,  for  example  as  regards  the  legal  ground  for  processing (lawfulness),  the  data  protection  principles  (e.g.  data  minimisation  and  necessity, fairness,  transparency),  and  any  other  obligations,  including  the  rules  on  solely automated individual decision-making, where relevant.
- (181) Where the evaluation or classification is based on one of the grounds protected from discrimination (e.g., age, religion, racial or ethnic origin, sex etc.) or results directly or indirectly in discrimination of those groups, such a practice will also be subject to Union non-discrimination law.
- (182) The Consumer Credit Directive (EU) 2023/2225 127 may also be relevant in this context. Article 18(3) CCD requires that the assessment of creditworthiness is carried out based on relevant and accurate information on the consumer's income and expenses and other financial  and  economic  circumstances  which  is  necessary  and  proportionate  to  the nature, duration, value and risks of the credit for the consumer. That information may include evidence of income or other sources of repayment, information on financial assets  and  liabilities,  or  information  on  other  financial  commitments.  The  CCD explicitly prohibits special categories of personal data to be included in the information and to obtain information from social networks. The European Banking Authority's Guidelines on loan origination and monitoring 128 further specify the relevant information for the purpose of creditworthiness assessments. This specification of the type  of  data  in  these  sectoral  laws  for  specific  evaluation  purposes  are  relevant considerations  to  be  taken  into  account  when  determining  whether  a  practice  falls within the scope of the prohibition in Article 5(1)(c) AI Act.
- (183) Similarly, AI systems used for the evaluation and classification of persons for antimoney laundering and terrorism financing purposes should also comply with relevant Union legislation on these matters.
## 5. ARTICLE  5(1)(D)  AI  ACT  -  INDIVIDUAL  RISK  ASSESSMENT  AND PREDICTION OF CRIMINAL OFFENCES
(184) Article 5(1)(d) AI Act prohibits AI systems assessing or predicting the risk of a natural person committing a criminal offence based solely on profiling or assessing personality traits and characteristics.
127 Directive 2008/48/EC of the European Parliament and of the Council of 23 April 2008 on credit agreements for consumers and repealing Council Directive 87/102/EEC, OJ L 133, 22/05/2008, p. 66-92.
128 European Banking Authority, Guidelines on loan origination and monitoring from 29 May 2020, EBA/GL/2020/06.
- (185) The provision indicates, in its last phrase, that the prohibition does not apply if the AI system is used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to that activity. Such AI systems that fall outside the scope of the prohibition intended to be used by law enforcement authorities, or on their behalf, or by Union institutions, bodies, offices or agencies in support of law enforcement authorities, for assessing the risk of a natural person offending or re-offending not solely on the basis of profiling, or the assessment of personality traits and characteristics or past criminal behaviour are classified as 'high-risk' AI systems (Annex III, point 6, letter (d) AI Act) and must comply with all relevant requirements and obligations under the AI Act.
## . Rationale and objectives
(186) Recital 42 AI Act explains the background and rationale of the prohibition in Article 5(1)(d) AI Act, namely, that natural persons should be judged on their actual behaviour and not on AI-predicted behaviour based solely on their profiling, personality traits or characteristics.
## . Main concepts and components of the prohibition
## Article 5(1)(d) AI Act provides
The following AI practices shall be prohibited:
d) the placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person  or on  assessing their personality traits  and characteristics; this prohibition shall not apply to AI systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity;
(187) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(d) AI Act to apply:
- (i) The practice must constitute the 'placing on the market', 'the putting into service for this specific purpose' or the 'use' of an AI system.
- (ii) The AI system must make risk assessments that assess or predict the risk of a natural person committing a criminal offence.
- (iii)The risk assessment or the prediction must be based solely on either, or both, of the following:
- (a) the profiling of a natural person,
- (b) assessing a natural person's personality traits and characteristics.
(188) For the prohibition to apply all three conditions must be simultaneously fulfilled. The first  condition,  i.e.  the  placing  on  the  market,  putting  into  service  or  use  of  the  AI system, has been already analysed in section . The prohibition, therefore, applies to
both providers and deployers of AI systems, each within their respective responsibilities not to place on the market, put into service, or use such AI systems for this specific purpose. The other two conditions for the prohibition to apply are analysed below.
## . Assessing  the  risk  or  predicting  the  likelihood  of  a  person committing a crime
- (189) Risk assessments to assess or predict the risk of an individual committing a criminal offence are often referred to as individual 'crime prediction' or 'crime forecasting'. While  there  is  no  generally  agreed  definition  of  'crime  prediction'  or  'crime forecasting' 129 ,  these terms refer in general to a variety of advanced AI technologies and analytical methods applied to large amount of often historical data (including socioeconomic data, but also police records, etc.) which, in combination with criminology theories, are used to forecast crime as a basis to inform police and law enforcement strategies and action to combat, control, and prevent crime. 130
(190) Crime  prediction  AI  systems  identify  patterns  within  historical  data,  associating indicators with the likelihood of a crime occurring, and then generate risk scores as predictive outputs. For example, such systems may be used for planning police task forces,  for  monitoring  high-risk  situations,  and  for  conducting  controls  of  persons predicted as likely (re-)offenders. Such systems bring opportunities for law enforcement authorities, especially those with scarce resources, increasing efficiency, and enabling a proactive approach for detecting, deterring, and anticipating criminal offences. 131 However, such use of historical data on crimes committed to predict other persons' future behaviour may perpetuate or even reinforce biases, and may result in crucial individual circumstances being 'overlooked' when these circumstances are not part of the data set or considered in the algorithms on which the particular AI system operates.  This  may  also  undermine  public  trust  in  law  enforcement  and  the  justice system in general 132 .
- (191) Such risk assessments and predictions are, in principle, forward-looking and concern future criminal offences (not yet committed) or crimes that are assessed as a risk of being  committed  at  the  moment,  including  in  cases  of  an  attempt  or  preparatory activities undertaken to commit a criminal offence. 133  They can be made at any stage of the law enforcement activities, such as during prevention and detection of crimes, but also during the investigation, prosecution and execution of criminal penalties (including
129 For example, see systems mentioned in the EU Fundamental Rights Agency handbook, such as the Criminality Awareness System (CAS) in the Netherlands and Precobs in Germany and Switzerland, Handbook, 2018, p.138. Preventing unlawful profiling today and in the future: a guide , Handbook, 2018, p.138.
130 See Europol, AI and policing The benefits and challenges of artificial intelligence for law enforcement, An Observatory Report from the Europol Innovation Lab, 23 September 2024. See also F. Yang, 'Predictive Policing' in Oxford Research Encyclopedia, Criminology and Criminal Justice , Oxford University Press, 2019.
131 For example, OxRec (Dutch Probation Office, 'Reclassering Nederland') Prediction of violent reoffending in prisoners and individuals on probation: a Dutch validation study (OxRec) - PMC (nih.gov)
132 See  for  instance  EU  Fundamental Rights Agency (8 December 2022) Bias in algorithms - Artificial intelligence and discrimination | European Union Agency for Fundamental Rights.
133 See in this respect Recital 42 AI Act that refers in this respect to the 'likelihood of their offending' and the 'occurrence  of actual or potential criminal offences' which are used in present, but not past tense.
when judicial authorities assess the risk of re-offending e.g. in the context of making decisions on the imposition of pre-trial detention) as well as part of the individuals' plan for re-integration into society after serving a criminal sentence 134 .
(192) The prohibition in Article 5(1)(d) AI Act does not outlaw crime prediction and risk assessment practices as such. It only applies to AI systems for making risk assessments to assess or predict the risk of a natural person committing a criminal offence, where also the third condition referred to above is met. Moreover, as noted, the prohibition does not apply in the situations described in the express exclusion contained in the last phrase of Article 5(1)(d) AI Act.
## . Solely based on profiling of a natural person or on assessing their personality traits and characteristics
(193) The third condition for the prohibition in Article 5(1)(d) AI Act to apply is that the risk assessment to assess or predict the risk of a natural person committing a crime must be based solely on a) the profiling of the person or b) on the assessment of their personality traits and characteristics.
(194) The prohibition in Article 5(1)(d) AI Act applies, irrespective of whether the AI system profiles or assesses the personality traits and characteristics of only one natural person or a group of natural persons simultaneously, since the prohibition aims to protect every individual  in  respect  of  whom  the  risk  of  committing  a  criminal  offence  is  being predicted or assessed.
## a) Profiling of a natural person
(195) Unlike Article 5(1)(c) AI Act, Article 5(1)(d) explicitly uses the term 'profiling'. Article 3(52) AI Act defines that term by reference to its definition in Article 4(4) GDPR 135 . The concept of profiling includes the objective to 'evaluate certain personal aspects' as one of its core elements. 136 In the context of Article 5(1)(d) AI Act, the profiling is done for the purposes of assessing or predicting the risk of a person committing a crime.
(196) The concept of so-called group profiling 137 may also be relevant in this context. That concept refers to the construction and the application of a descriptive profile for a given group,  for  example  categories  of  perpetrators  of  criminal  offences  (e.g.,  terrorists, gangsters etc.) constructed on historic data about previously committed crimes by other
134 As  an  example,  Art.  24(4)  of  EU  Directive  2011/93  on  combating  the  sexual  abuse  and  sexual  exploitation  of  children  and  child pornography, requires persons undergoing criminal proceedings or convicted of acts linked to child sexual abuse to undergo an assessment of the danger they pose of recidivism.
135 Article 3(4) LED, which is relevant for the prohibition in Article 5(1)(d) AI Act, defines profiling in an identical manner to that in Article 4(4) GDPR as 'any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements'.  The same definition is also contained in Article 3(5) of Regulation (EU) 2018/1725 on the processing of personal data by the Union institutions, bodies, offices and agencies, (OJ L 295, , p. 39).
136 See also Article 29 Data Protection Working Party, Guidelines on Automated individual decision-making and Profiling for the purposes of Regulation 2016/679, WP251rev.01, , and endorsed by the EDPB, p. 7. See also Fundamental Right Agency, Preventing unlawful profiling today and in the future: a guide, Handbook, 2018, p.138.
137 See about a group profiling, e.g., Fundamental Right Agency, Preventing unlawful profiling today and in the future: a guide, Handbook, 2018, p. 21.
persons. Those group profiles may be used later to assess and predict the risk of other persons committing similar offences. Whenever an AI system makes prediction and applies such a (group) profile to a specific individual, this constitutes profiling of the person and may therefore fall within the prohibition of Article 5(1)(d) AI Act.
## b) Assessment of personality traits and characteristics
(197) The prohibition also applies if the risk assessment to assess or predict the risk of the person  committing  a  criminal  offence  is  only  based  on  assessing  the  person's personality traits and characteristics. Such an assessment or prediction is often included in the concept of profiling, but it could also be seen as an alternative, if profiling as defined in Article 4(4) GDPR cannot be established.
- (198) As mentioned in section .c), personality traits and characteristics constitute a broad category of characteristics related to a particular natural person, for which there is no generally agreed taxonomy. Recital 42 AI Act provides examples of personality traits and characteristics which may be assessed for predicting the risk of a person committing an offence, such as 'nationality, place of birth, place of residence, number of children, level of debt or type of car'. This is only an illustrative and not an exhaustive list.
## c) 'Solely'
- (199) Article 5(1)(d) AI Act provides that the risk assessments covered by that provision are only  prohibited  where  they  are  based  'solely'  on  the  profiling  of  a  person  or  the assessment of their personality traits and characteristics. It is clear from Recital 42 AI Act that 'solely' is intended to apply both to profiling or to the assessment of personality traits and characteristics.
- (200) The condition that the risk assessment must be based 'solely' on profiling or assessing personality traits and characteristics may not be fulfilled in a number of situations.
(201) As is evident from the last phrase of Article 5(1)(d) AI Act, such a situation arises, in any  event,  where  the  AI  system  is  used  to  support  the  human  assessment  of  the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity. As Recital 42 makes clear, in this context one should think, in particular but not necessarily exclusively, of a situation in which a reasonable suspicion in respect of the natural person concerned already exists. After all, in such cases there will normally have been a human assessment, which will normally be based on relevant objective and verifiable facts.
(202) However, there can also be other situations, which will always need to be assessed on a  case-by-case basis.  On the one hand, the use of the term 'solely' leaves open the possibility of various other elements being taken into account in the risk assessment, which makes that it is no longer based on profiling or assessing personality traits or characteristics  alone.  On  the  other  hand,  in  order  to  avoid  circumvention  of  the prohibition and ensure its effectiveness, any such other elements will have to be real,
substantial  and  meaningful  for  them  to  be  able  to  justify  the  conclusion  that  the prohibition  does  not  apply.  A  reading  of  the  prohibition  of  Article  5(1)(d)  AI  Act together  with  the  exclusion  contained  in  the  last  phrase  thereof  suggests  that,  in particular, the existence of certain pre-established objective and verifiable facts may justify that conclusion.
## For example,
- -  A  law  enforcement  authority  uses  an  AI  system  to  predict  criminal  behaviour  for crimes such as terrorism solely based on individuals' age, nationality, address, type of a  car,  and  marital  status.  With  that  system,  individuals  are  deemed  more  likely  to commit future offences that they have not yet committed solely based on their personal characteristics. Such a system may be assumed to be prohibited under Article 5(1)(d) AI Act.
- - National tax authorities use an AI predictive tool to review all taxpayers' tax returns to predict potential criminal tax offences to identify cases requiring further investigation. This is done solely on the basis of the profile built by the AI system, which uses for its assessment personality traits, such as double nationality, place of birth, number of children, and opaque variables, especially inferred information that is  predictive  and  therefore  non-objective  and  hard  to  verify.  Such  a  system  will normally  fall  under  the  prohibition  of  Article  5(1)(d)  AI  Act,  since  there  is  no reasonable suspicion of the involvement of a particular person in a criminal activity or other objective and verifiable facts linking that to that criminal activity. This is also an example that falls within the scope of social scoring prohibited under Article 5(1)(c) AI Act involving unfavourable treatment with data from unrelated social contexts.
- - A police department uses AI-based risk assessment tool to assess the risk of young children and adolescents being involved in ' future violent and property offending'. The system assesses children based on their relationships with other people and their supposed  risk  levels,  meaning  that  children  may  be  deemed  at  a  higher  risk  of offending simply by being linked to another individual with a high-risk assessment, such as a sibling or a friend. The parents' risk levels may also impact a child's risk level. The risk assessments result in police 'registering' these children in their systems, monitoring  them  with  additional  inspections,  and  referring  them  to  youth  'care' services. Such a system is also likely to fall under the prohibition of Article 5(1)(d) AI Act.
## . Exclusion  of  AI  systems  to  support  the  human  assessment based  on  objective  and  verifiable  facts  directly  linked  to  a criminal activity
(203) Article 5(1)(d) AI Act provides, in its last phrase, that the prohibition does not apply to AI systems used to support the human assessment of the involvement of a person in
a criminal  activity,  which  is  already  based  on  objective  and  verifiable  facts  directly linked to a criminal activity. Although, as noted, the situation described in this express exclusion  is  not  necessarily  the  only  one  in  which  the  prohibition  does  not  apply, including that situation expressly in that provision offers legal certainty by delineating the scope of the prohibition and by making it clear that, where that situation is at issue, the prohibition does not in any event apply.
(204) Where the system falls within the scope of the exclusion and is therefore not prohibited, it will be classified as a high-risk AI system (as referred to in Annex III, point 6(d), AI Act)  if  intended  to  be  used  by  law  enforcement  authorities  or  on  their  behalf  and therefore  subject  to  the  requirements  and  safeguards,  including  human  oversight (Article 14 and Article 26 AI Act). These requirements include that the human oversight must be assigned to persons with the necessary competence, training and authority who should be able to properly understand the capabilities and limitations of the AI system, correctly interpret its output and address the risk of automation bias. Those persons should have clear procedures, training and the necessary competence and authority to meaningfully assess the outputs of the AI system. In this specific case, their human assessment should ensure that any AI prediction or assessment of the risk of a person committing  a  crime  is  based  on  objective  and  verifiable  facts  linked  to  a  criminal activity. Those persons should also intervene in order to avoid negative consequences or risks, or stop the use of the AI system if it does not perform as intended.
(205) Furthermore, the concept of 'human intervention' has been subject to CJEU case-law, in particular in the context of solely automated decision-making predicting the risk of air passengers being involved in serious crimes. That case-law may also be relevant for the application of the concept of 'human assessment' as used in Article 5(1)(d) AI Act.
In the Ligue des droits humains case 138 , the CJEU examined the legality of the use of an advanced AI system for the systematic processing of passenger name record (PNR) data of air travellers to assess their likelihood of being involved in terrorism and other serious crimes.
The  CJEU  interpreted  the  rule  in  Directive  (EU)  2016/681  ('PNR  Directive')  that prohibits adverse legal decisions based solely on automated processing and required individual human assessment and review for any positive matches by non-automated means to identify false positives and ensure non-discriminatory results.
According  to  the  CJEU,  that  human  assessment , subject  to  which  any  results  of automated processing of PNR data must be based, must rely on objective criteria to evaluate whether a positive match concerns someone who might be involved in this specific case in terrorist offenses or serious crime, and to ensure the non-discriminatory nature of automated processing.
138 Judgment of the Court of Justice of 21 June 2022, Ligue des droits humains , C-817/19, ECLI:EU:C:2022:491.
(206) As to the content of the exclusion, one of its central elements is that the AI system is used to support human assessment, rather than involving the AI system itself making the risk assessment as occurs in the situations covered by the prohibition. However, for the exclusion to apply, that human assessment must, in addition, already be based on objective and verifiable facts directly linked to a criminal activity.
## . Extent to which private actors' activities may fall within scope
(207) Besides law enforcement authorities that are in principle the main deployers of AI crime predictive  systems,  the  activities  of  private  entities  may  also  be  covered  by  the prohibition in Article 5(1)(d) AI Act in some cases. That follows from the fact that, based on its wording, the prohibition does not apply exclusively to law enforcement authorities. Moreover, otherwise the prohibition might be easily circumvented, which would call into question its effectiveness.
(208) That being so, the prohibition may be assumed to apply, in particular, when private actors  are  entrusted  by  law  to  exercise  public  authority  and  public  powers  for  the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties 139 . Private actors may be also explicitly requested on a case-bycase basis to act on behalf of law enforcement authorities and carry out individual crime risk  predictions.  In  those  cases,  the  activities  of  those  private  actors  could  also  fall within the scope of the prohibition, if the applicable conditions are fulfilled and the exclusion does not apply.
For example, a private company providing advanced AI-based crime analytic software may be asked by a law enforcement authority to analyse a large amount of data from multiple  sources  and  databases,  such  as  national  registers,  banking  transactions, communication data, geo-spatial data, etc., to predict or assess the risk of individuals as potential offenders of human trafficking offences. If all the criteria for Article 5(1)(d) are met, such a use case could be prohibited.
(209) Furthermore, the prohibition may apply to private entities assessing or predicting the risk  of  a  person  of  committing  a  crime  where  this  is  objectively  necessary  for compliance with a legal obligation to which that private operator is subject to assess or predict the risk of persons committing specific criminal offences (e.g., in case of antimoney laundering, terrorism financing).
For  example,  a  banking  institution  has  an  obligation  under  Union  anti-money laundering  legislation 140 to  screen  and  profile  customers  for  money-laundering offences. If the bank uses an AI system to fulfil its obligations, that should be done based only on the data as specified in that law which are objective and verifiable to ensure that the persons singled out as suspect are reasonably likely to commit anti-
139 See definition of law enforcement authorities in Article 3(45) AI Act.
140  Anti-Money Laundering Regulation (EU) 2024/1624 of 31 May 2024.
money laundering offences. The predictions must also be subject to human assessment and verification in compliance with that legislation 141 in order to ensure the accuracy and appropriateness of such assessments. Compliance with that legislation will ensure that  the  use  of  individual  crime  prediction  AI  system  for  anti-money  laundering purposes fall outside the scope of the prohibition in Article 5(1)(d) AI Act.
(210) However,  having  regard  to  the  focus  on  risk  assessments  relating  specifically  and exclusively to the commission of criminal offences that is evident from the wording of the prohibition as well as to the purpose of the prohibition as explained in Recital 42, if a private entity profiles customers for its regular business operations and safety or to protect its financial interests (e.g. detecting financial irregularities) without the purpose of  assessing  or  predicting  the  risk  of  the  customer  committing  a  specific  criminal offence, the activities of the private entities should not be considered to fall under the scope of the prohibition of Article 5(1)(d) AI Act.
(211) In other words, in the absence of private parties having been entrusted by law certain specific law enforcement tasks, acting on behalf of law enforcement authorities or being subject  to  specific  legal  obligations  as  described  above,  the  use  of  AI  systems  for making risk assessments in the context of private entities' ordinary course of business and with the aim of protecting their own private interests, whilst the fact that those risk assessments may relate to the risk of criminal offences being committed merely as a purely  accidental  and  secondary  circumstance,  is  not  deemed  to  be  covered  by  the prohibition.
## . Out of scope
## . Location-based or geospatial predictive or place-based crime predictions
(212) Location-based or geospatial or place-based crime predictions is based on the place or location of crime or the likelihood that in those areas a crime would be committed. In principle, such policing does not involve an assessment of a specific individual. They therefore fall outside the scope of the prohibition.
Examples of location-based or geospatial predictive or place-based crime predictions
- - AI-based predictive policing system provides a score of the likelihood of criminality in  different  areas  in  a  city  based  on  previous  criminality  rates  by  area  and  other supporting information such as street maps, to highlight elevated risk of specific types of criminalities e.g. burglaries, knife crime etc. and help law enforcement authorities determine  where  to  deploy  less  or  more  police  patrols/presence  to  carry  out community policing to disrupt and deter criminal activity.
