59 D. Amodei, et al., Concrete Problems in AI Safety, 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:; J. Skalse et al. Defining and Characterizing Reward Gaming, Advances in Neural Information Processing Systems 35 (NeurIPS 2022) C. Denison et al., Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language, 36th Conference on Neural Information Processing Systems (NeurIPS 2022), Models, arXiv:.
- (71) In this context, the interplay between the prohibition in Article 5(1)(a) AI Act and the deployer's obligations in Article 50(4) AI Act to label  'deep fakes' and certain AIgenerated text publications on matters of public interest 60 ,  as  well  as  the  provider's obligation to ensure AI systems interacting with people are designed in a way to inform people that they are interacting with AI and not a human 61 , should be clarified. Such visible disclosure constitutes a mitigating measure that should also be enabled through design  features  embedded  in  the  AI  system  provided  by  the  provider,  including technical measures enabling the detection of AI-generated and manipulated content 62 . The visible labelling of 'deep fakes' and chatbots reduces the risk of deception that is likely to arise once the AI-generated content is disseminated to the public and reduces the risk of harmful distorting effects on the individual's opinion- and belief-formation and behaviour.
- (72) By contrast, the prohibition in Article 5(1)(a) AI Act has a much more limited scope. It may,  for  example,  cover  cases  where  a  chatbot  or  deceptive  AI-generated  content presents  false  or  misleading  information  in  ways  that  aim  to  or  have  the  effect  of deceiving individuals and distorting their behaviour that would not have happened if they  were  not  exposed  to  the  interaction  with  the  AI  system  or  the  deceptive  AI generated content, in particular if this has not been visibly disclosed 63 .
- (73) As with purposefully manipulative techniques, the prohibition of deceptive techniques may also cover AI systems that deceive individuals without any human intending them to do so (see section .b)above). For example, regardless of whether their providers intend such an outcome, AI systems may learn deceptive techniques simply because this increases their performance for the task for which they were developed, for example by reinforced learning 64 .
An example of deceptive techniques that may be deployed by AI is an AI chatbot that impersonates a friend of a person or a relative with synthetic voice and tries to pretend it is the person causing scams and significant harms.
Another example is an AI system that learns to identify when it is under evaluation and temporarily halts any undesired behaviour, only to resume such behaviour once the evaluation period is over. 65 Such deceptive behaviour is particularly dangerous, since it
60 Article 50(4) AI Act.
61 Article 50(1) AI Act.
62 Article 50(2) AI Act.
63 While in principle the transparency obligations in Article 50 AI Act aim to minimise the manipulative effects of deep fakes and chatbots, there might be instances and contexts where despite the information notices these deceptive techniques may still have significant effects on individuals and distort their behaviour to a point that undermine persons' individual autonomy and informed decision-making, so they should not be misused for disinformation and manipulation purposes and might still be covered in some cases by the prohibition in Article 5(1)a) if all other conditions of the ban are fulfilled (including the significant harms).
64 F. Ward, F. Toni, F. Belardinelli, T. Everitt, Honesty Is the Best Policy: Defining and Mitigating AI Deception (neurips.cc); Advances in Neural Information Processing Systems 36 (NeurIPS 2023); P. Park. et al. AI deception: A survey of examples, risks, and potential solutions [] Patterns, Volume 5, Issue 5, 100988.
65 J. Lehman, J. Clune, D. Misevic, C. Adami, L. Altenberg, J. Beaulieu, et al. The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. Artificial life, 26(2):274-306, 2020.
defies  any  external  human  oversight  over  the  system  and  may  be  prohibited  if  it  is reasonably likely to cause significant harms.
By  contrast,  a  generative  AI  system  that  incidentally  presents  false  or  misleading information and hallucinates 66 may not be considered to deploy deceptive techniques within the meaning of Article 5(1)(a) AI Act, taking into account the limitations and the  state  of  the  art  of  generative  AI.  In  particular,  this  may  be  the  case  where  the provider of the system has properly informed users about the system's limitations and integrated  appropriate  safeguards  into  the  system  to  minimise  such  outcomes  and provided that the system is not intended for, nor deployed in, sensitive contexts (e.g. health, education, elections) where serious harmful consequences are likely to occur (see also considerations in section .c) below).
## d) Combination of techniques
- (74) Article 5(1)(a) AI Act applies to subliminal, purposefully manipulative, or deceptive techniques, or to combinations of such techniques that can have a compound impact. As  stated  above,  purposefully  manipulative  techniques  may  be  also  subliminal  in nature, if they operate beyond the threshold of conscious awareness.
- (75) Furthermore, when purposefully manipulative and deceptive techniques are applied in combination,  this  may  significantly  influence  the  behaviour  of  individuals,  leading them to make decisions based on unconscious manipulations and false beliefs. This combination may create a feedback loop where individuals are less likely to question or critically evaluate the information received, since the manipulative elements have already primed their cognitive biases and emotional responses.
## . With the  objective  or  the  effect  of  materially  distorting  the behaviour of a person or a group of persons
- (76) A third  condition  for  the  prohibition  in  Article  5(1)(a)  AI  Act  to  apply  is  that  the deployed subliminal, purposefully manipulative, or deceptive technique must have 'the objective, or the effect of materially distorting the behaviour of a person or a group of persons'. This implies a substantial impact on the behaviour where a person's autonomy and free choices are undermined, rather than a minor influence. However, intent is not a necessary requirement, since Article 5(1)(a) AI Act also covers practices that may only have the 'effect' of causing material distortion. There should be a plausible/reasonably likely causal link between the potential material distortion of the behaviour  and  the  subliminal,  purposefully  manipulative  or  deceptive  technique deployed by the AI system.
66 'Hallucination' is a term used to describe a technical flaw in generative AI systems when they generate unwanted information that is fabricated or factually incorrect without this being intended by their developers. See more Ji Ziwei et al., Survey of Hallucination in Natural Language Generation | ACM Computing Surveys, 55, Issue 12, Article No.: 248, Pages 1 - 38.
## a) The concept of 'material distortion of the behaviour'
- (77) The concept of ' material distortion of the behaviour ' of a person or a group of persons is  central  to  Article  5(1)(a)  AI  Act.  It  involves  the  deployment  of  subliminal, purposefully  manipulative  or  deceptive  techniques  that  are  capable  of  influencing people's  behaviour  in  a  manner  that  appreciably  impairs  their  ability  to  make  an informed decision, thereby causing them to behave in a way or to take a decision that they would otherwise not have taken.
- (78) ' Appreciable impairment ' refers to a substantially reduced ability to make informed and autonomous decisions, thereby  causing  individuals  to  behave  in  a  way  or  to  take  a decision that they would otherwise not have taken.  It goes beyond minor or negligible impacts and involves a significant distortion or hindrance in decision-making and free choice,  including  in  relation  to  opinion-  and  belief-formation.  This  suggests  that 'material distortion' involves a degree of coercion, manipulation, or deception that goes beyond lawful persuasion, which falls outside the scope of the prohibition (see section . below).
- (79) An  'informed  decision'  requires  an  understanding  and  knowledge  of  the  relevant information, including the available options, the risks and benefits of each choice, the possible  effects  of  the  AI  system  on  their  behaviour,  and,  as  appropriate,  other contextual information that is important for the decision-making or the behaviour of the person.
- (80) For  the  interpretation  of  the  concept  of  'material  distortion  of  behaviour',  Union consumer  protection  law,  in  particular,  Directive  2005/29/EC  (Unfair  Commercial Practices Directive or 'UCPD'), may constitute a valid source of inspiration. The UCPD prohibits various unfair, misleading, and aggressive commercial practices (Articles 5 to 9 UCPD) capable of causing consumers to make transactional decisions that they would otherwise not have made. According to the CJEU and the Commission guidance on the UCPD 67 , there is no need to prove that a consumer's economic behaviour has been distorted, it suffices to establish that a commercial practice is 'likely' (i.e., capable) of impacting  an  average consumer's  transactional decision. 68 The  CJEU  has  also underscored that even accurate information may be misleading if presented in a way that distorts the consumer's decision-making process. 69 National enforcement authorities are tasked to investigate the specific facts and circumstances of each case ( in  concreto) and  to  evaluate  the  potential  impact  of  the  practice  on  the  average consumer's decision-making process ( in abstract ). 70 For that purpose, they must take
67 See also Commission Guidance on the interpretation and application of Directive 2005/29/EC of the European Parliament and of the Council concerning unfair business-to-consumer commercial practices in the internal market, (OJ C 526, , p. 1)
68 Judgment of the Court of Justice of Judgment of the Court (Fifth Chamber) of 26 October 2016. Canal Digital Danmark A/S . 