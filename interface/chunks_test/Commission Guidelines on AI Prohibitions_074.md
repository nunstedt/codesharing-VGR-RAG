New  Media  &amp;  Society  146144482211420  (2022) doi:/14614448221142007; Neugnot-Cerioli, M. &amp; Laurenty, O. M. The Future of Child Development in the AI Era. Cross-Disciplinary Perspectives Between AI and Child Development Experts. Preprint at https://doi.org//ARXIV. (2024).
92 2023 EIOPA Consumer Trends Report, page 16, last paragraph.
- (118) Persons  with  disabilities also  represent  a  vulnerable  group  that  exploitative  and manipulative AI systems may significantly harm.
For instance, an AI system that uses emotion recognition to support mentally disabled individuals in their daily life may also manipulate them into making harmful decisions, like purchasing products promising unrealistic mental health benefits. This is likely to worsen their mental health condition and financially exploit them through the purchase of  ineffective  and  expensive  products,  which  is  likely  to  cause  them  significant psychological and financial harms.
- (119) Socio-economically  disadvantaged  individuals are  particularly  susceptible  to  AI systems exploiting their financial desperation and precarious social situation and are often less informed and digitally literate.
For  instance,  an  AI  chatbot  could  target  specific  socio-economically  disadvantaged groups  inciting  them  to  commit  acts  of  violence  or  injuries  of  other  persons,  by identifying  their  heightened  susceptibility  to  certain  types  of  content,  fear-based narratives,  or  exploitative  offers.  The  system's  targeted  approach  exacerbates  the existing vulnerabilities of these socio-economically disadvantaged individuals, deepening  their  challenges.  In  certain  cases  this  may  lead  to  increased  anxiety, depression, feelings of helplessness, social isolation, or self-harm and radicalisation to a point that reaches the threshold of significant harm under Article 5(1)(b) AI Act.
- (120) Unlike Article 5(1)(a) AI Act, Article 5(1)(b) AI Act does not explicitly refer to group harms, while recital 29 AI Act refers for both prohibitions to harms suffered by both specific  persons  and  groups  of  individuals.  The  two  prohibitions  should  thus  be interpreted in a consistent manner aligned also with the safety logic of the AI Act and the objective of the prohibition in Article 5(1)(b) to protect all individuals belonging to the  specific  vulnerable  groups  due  to  age,  disability  and  specific  socio-economic situation. Harms that can be externalised and affect other persons, even if not directly affected by the system, should therefore also be taken into account in the assessment of the significance of the harm under Article 5(1)(b) AI Act.
## For instance,
- - The AI-enabled exploitation of children's vulnerabilities may have long-term societal impacts, including increased prevalence of mental health concerns, healthcare costs, and reduced productivity due to chronic health issues.
- - An AI system exploiting the financial vulnerabilities of economically disadvantaged people  may  lead  to  financial  exclusion  and  create  a  downward  spiral  of  socioeconomic  hardship  for  those  disadvantaged  groups.  Such  exploitation  may  cause societal  harms  with  broader  negative  impacts  on  societal  structures  and  values, including the perpetuation and exacerbation of discrimination and social inequality and the exclusion of those groups.
- - A chatbot targeting certain vulnerable socio-economic groups with misinformation or hate  speech  may  lead  to  social  polarisation  and  radicalisation,  possibly  igniting violence and even injuries and deaths of other persons.
- (121) These examples of exploitative AI practices should be distinguished from numerous other  AI  systems  which  do  not  exploit  the  vulnerabilities  of  children,  persons  with disabilities,  or  persons  in  specific  socio-economic  situations  and  are  not  reasonably likely  to  cause  significant  harms,  but  aim  to  benefit  those  persons  when  properly designed and used (see also section . out of scope).
## For instance,
- - AI systems that support children in their learning and in games;
- -  AI  systems that help older persons in their daily life and improve their health and medical treatment, such as personal assistants or assistive robots, or improve their digital skills;
- - AI systems that support the economic and other integration of socio-disadvantaged persons in the society, improve their skills, etc.;
- - AI systems and devices that support visually or hearing impaired persons or provide adapted and personalised learning;
- - AI systems that generate accessible solutions removing barriers for the use by persons with disabilities of products and services;
- - AI-enabled prosthetics etc. that help disabled persons in their daily life and enable their integration and full participation in society.
## . Interplay between the prohibitions in Article 5(1)(a) and (b) AI Act
- (122) The interplay between the prohibitions in Article 5(1)(a) and (b) AI Act requires the delineation of the specific contexts that each provision covers to ensure that they are applied in a complementary manner.
- (123) The primary focus of the prohibition in Article 5(1)(a) AI Act is placed on the nature of  the  techniques,  specifically  those  that  operate  below  the  threshold  of  conscious awareness  or  other  purposefully  manipulative  or  deceptive  techniques.  The  key elements here are the primarily covert nature of the influence and its impact on the individual affected by the system that undermines their cognitive autonomy to make informed and autonomous decisions.
- (124) By  contrast,  the  primary  focus  of  the  prohibition  in  Article  5(1)(b)  AI  Act  is  the protection of particularly vulnerable persons due to their age, disability, or a specific socio-economic situation, which are in principle more susceptible to AI exploitation due to inherent or situational factors and, therefore, require additional protection against exploitation. The key elements here are the characteristics of the affected vulnerable
persons and the fact that their specific vulnerabilities are being  exploited by the AI system.
For instance, if an AI system uses rapid image flashes to influence purchasing decisions, it may fall under Article 5(1)(a) AI Act due to the subliminal nature of the manipulation. Conversely, an AI system that targets older persons with insurance offers by exploiting their reduced cognitive capacity may fall under Article 5(1)(b) AI Act.
(125) In  scenarios  where  both  provisions  may  seem  applicable,  the  primary  criterion  for differentiation should be the dominant aspect of the exploitation. If the  exploitation applies  regardless  of  the  specific  vulnerabilities  of  the  persons  concerned,  Article 5(1)(a) AI Act should take precedence, while taking into account the particular effects of the manipulative or deceptive technique on the vulnerable persons' behaviour and the  specific  harms  that  those  persons  are  likely  to  experience.  If  the  AI-enabled manipulation and exploitation is targeted at a specific vulnerable group of persons due to their age, disability, or specific socio-economic situation or aimed to exploit their vulnerabilities, then Article 5(1)(b) AI Act should be applied instead. Exploitation of vulnerabilities of other groups may be covered as part of Article 5(1)(a) AI Act if the purposefully manipulative practice leverage on specific vulnerabilities and weaknesses of those persons.
## . Out of scope
(126) For the prohibitions in Article 5(1)(a) and (b) AI Act to apply, all conditions listed in the relevant provisions must be fulfilled, as examined above. All other AI systems that do not fulfil these conditions are outside the scope of those prohibitions, with some examples described below.
## . Lawful persuasion
(127) Distinguishing manipulation from persuasion is crucial to delineate the scope of the prohibition  in  Article  5(1)(a)  AI  Act,  which  does  not  apply  to  lawful  persuasion practices. While both manipulation and persuasion influence individuals' decisions and behaviours, they differ significantly in methods and ethical implications.
(128) Manipulation  involves,  in  most  cases,  covert  techniques  undermining  autonomy, leading individuals to make decisions they might not have otherwise made if they were fully  aware  of  the  influences  at  play.  These  techniques  often  exploit  psychological weaknesses or cognitive biases. By contrast, persuasion operates within the bounds of transparency and respect for individual autonomy. It involves presenting arguments or information in a way that appeals to reason and emotions, but explains the AI system's objectives  and  functioning,  provide  relevant  and  accurate  information  to  ensure informed  decision-making  and  supports  the  individual's  ability  to  evaluate  the information and make free and autonomous choices.
For example, an AI system using personalised recommendations based on transparent algorithms  and  user  preferences  and  controls  engages  in  persuasion.  By  contrast,  a system that uses subliminal clues (e.g. imperceptible images) to influence users towards specific choices without their knowledge and understanding constitutes manipulation.
(129) The objective and impact of these techniques also differ. Manipulation often aims at benefitting the manipulator at the expense of the individual's autonomy and well-being. By contrast, persuasion aims to inform and convince, aligning interests and benefits for both parties. Ethical persuasion respects an individual's autonomy to make informed choices and avoids exploiting vulnerabilities.
For  example,  an  AI  system  which  operates  in  a  transparent  manner  and  analyses customers' emotions to improve customer interactions and provide support with the knowledge  of  the  users  engages  in  persuasion  and  aligns  with  their  interests.  By contrast,  an  emotion  recognition  system  used  for  targeted  advertising  that  infers emotions  of  consumers  in  a  hidden  manner  to  offer  products  of  higher  prices  at  a specific moment when the user is more likely to buy them engages in manipulation and is to the detriment of the consumers.
(130) Consent  also  plays  an  important  role  in  certain  cases.  In  persuasive  interactions, individuals are aware of the influence attempt and can freely and autonomously choose it. In manipulative interactions, the lack of awareness of the techniques or their impact negates the freedom of choice and informed and autonomous decision-making.
For example, an AI system that aims to help users learn a foreign language better and faster through the deployment of subliminal techniques is not manipulative if it operates in a transparent manner and respects individual autonomy and user's free and informed choice to consent to the use of the system or not.
(131) Compliance  with  legal  and  regulatory  frameworks  also  plays  an  important  role  in measuring manipulation as compared to lawful persuasion. AI practices that comply with  applicable  laws  that  uphold  transparency,  fairness,  and  individuals'  rights  and autonomy are therefore more likely not to be prohibited under the AI Act.
For example, compliance with data protection laws, such as the GDPR, which mandates transparency obligations in data processing, namely that the information to be provided to  the  data  subjects  should  avoid  deceptive  or  manipulative  language 93 .  In  some instances, consent may be required for personal data processing to be lawful, as for certain online personalised advertisements based on off-service users' data in social networks 94 . That consent must be, amongst others, free and informed. AI systems that meet these legal standards are more likely to engage in lawful persuasion. Conversely,
93 European Data Protection Board Guidelines, https://www.edpb.europa.eu/system/files/2023-02/edpb\_03-2022\_guidelines\_on\_deceptive\_design\_patterns\_in\_social\_media\_platform\_interfaces\_v2\_en\_0.pdf, para. 18.
94 Judgment of the Court of Justice of 4 July 2023, Meta Platforms and Others , C-252/21, ECLI:EU:C:2023:537 (hereinafter referred to as the ' Meta Platforms judgment').
systems that circumvent these requirements to influence behaviour are likely engaging in manipulation.
- (132) In particular, recital 29 AI Act clarifies that the prohibitions in Article 5(1)(a) and (b) AI Act do not affect lawful practices in the context of medical treatment under certain conditions.
For  example,  AI-enabled  subliminal  techniques  may  be  used  in  the  psychological treatment of a mental disease or physical rehabilitation when carried out in accordance with the applicable law and medical standards, including obtaining the explicit consent of the individual or its legal representatives as a condition for use.
- (133) Furthermore,  recital  29  AI  Act  clarifies  that  common  and  legitimate  commercial practices, such as advertising, should not be regarded 'in themselves' or by their very nature as harmful manipulative, deceptive or exploitative AI-enabled practices.
## For example,
- - Advertising techniques that use AI to personalise content based on user preferences are  not  inherently  manipulative  if  they  do  not  deploy  subliminal,  purposefully manipulative  or  deceptive  techniques  that  subvert  individual  autonomy  or  exploit vulnerabilities in harmful ways as prohibited under Article 5(1)(a) and (b) AI Act. Compliance with the relevant obligations under the GDPR, consumer protection law and Regulation (EU) 2022/2065 ('the DSA') help to mitigate such risks.
- - The generation of child sexual abuse material to train and improve the effectiveness of  AI  models  and  classifiers  to  detect  child  sexual  material  online  are  common legitimate practices that are not exploitative of children's vulnerabilities and are, on the contrary, essential to improve child safety online.
- - AI systems used for providing banking services, such as mortgages and loans, that use the age or the specific socio- economic situation of the client as an input, in compliance with Union legislation on financial service, consumer protection, data protection and non-discrimination,  do  not  qualify  as  the  exploitation  of  vulnerabilities  within  the meaning of Article 5(1)(b)  AI  Act  when  they  are  designed  to  protect  and  support people identified as vulnerable due to their age, disability or specific socio-economic circumstances and are beneficial for those groups, contributing also to fairer and more sustainable financial services for those groups.
- -  AI  systems  that  detect  drowsiness  and  fatigue  in  drivers  and  alert  them  to  rest  in compliance  with  safety  laws  are  beneficial  and  do  not  qualify  as  exploitation  of vulnerabilities within the meaning of Article 5(1)(b) AI Act.
## . Manipulative, deceptive and exploitative AI systems that are not likely to cause significant harm
- (134) An essential condition for the prohibitions in Article 5(1)(a) and (b) AI Act to apply is that the AI-enabled manipulation and exploitation of vulnerabilities should cause or be reasonably likely to cause significant harm. All manipulative, deceptive and exploitative AI applications that are not reasonably likely to cause significant harms are in principle outside the scope of the prohibitions without prejudice to other Union law that still applies (see section . below).
Examples of AI systems that are not likely to cause significant harm include:
- -  An  AI  companionship  system  is  designed  in  an  anthropomorphic  way  and  with affective computing to make the system more appealing and effectively makes users more engaged, but is not engaging in other manipulative or deceptive practices in a manner that is reasonably likely to cause them serious psychological, physical or other harms, unhealthy attachment and dependency.
- -  A therapeutic chatbot uses subliminal techniques to steer users towards a healthier lifestyle and to quit bad habits, such as smoking. Even if the users who follow the chatbot's  advice  and  subliminal  therapy  experience  some  physical  discomfort  and psychological stress due to the effort made to quit smoking, the AI-enabled chatbot cannot be considered likely to cause significant harm. Such temporary discomfort is unavoidable and outweighed by the long-term benefits for users' health. There are no hidden attempts to influence decision-making beyond promoting healthy habits.
- - An online music platform uses an emotion recognition system to infer users' emotions and automatically recommends them songs in line with their moods, while avoiding excessive exposure to depressive songs. Since users are just listening to music and are not otherwise harmed or led to depression and anxiety, the system is not reasonably likely to cause significant harm.
- - AI-enabled manipulative and deceptive techniques used in security training and other learning simulations that mimic phishing attempts to educate users on cybersecurity threats.  These  systems  may  deploy  purposefully  manipulative  techniques  (e.g., exploiting cognitive biases) without users' awareness that distort the behaviour, but this is done temporarily for beneficial training and awareness raising purposes and without causing significant harms.
## . Interplay with other Union law
(135) The  prohibitions  in  Article  5(1)(a)  and  (b)  AI  Act  are  without  prejudice  to  and complement other Union law. The same practice that falls within the prohibition of Article 5(1)(a) or (b) AI Act may also constitute an infringement of other Union law legislation and be subject to enforcement under both the AI Act and those other acts. 