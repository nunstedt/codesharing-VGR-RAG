Where such a delay in the adoption of a harmonised standard is due to the technical complexity of that standard, this should
( 41 ) Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending  Council  Directives  89/686/EEC  and  93/15/EEC  and  Directives  94/9/EC,  94/25/EC,  95/16/EC,  97/23/EC,  98/34/EC, 2004/22/EC,  2007/23/EC,  2009/23/EC  and  2009/105/EC  of  the  European  Parliament  and  of  the  Council  and  repealing  Council Decision 87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, , p. 12).
be  considered  by  the  Commission  before  contemplating  the  establishment  of  common  specifications.  When developing  common  specifications,  the  Commission  is  encouraged  to  cooperate  with  international  partners  and international  standardisation  bodies.
- (122) It is appropriate that, without prejudice to the use of harmonised standards and common specifications, providers of a  high-risk  AI  system  that  has  been  trained  and  tested  on  data  reflecting  the  specific  geographical,  behavioural, contextual or functional setting within which the AI system is intended to be used, should be presumed to comply with  the  relevant  measure  provided  for  under  the  requirement  on  data  governance  set  out  in  this  Regulation. Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, in accordance with  Article  54(3)  of  Regulation  (EU)  2019/881,  high-risk  AI  systems  that  have  been  certified  or  for  which a  statement  of  conformity  has  been  issued  under  a  cybersecurity  scheme  pursuant  to  that  Regulation  and  the references of which have been published in the Official Journal of the European Union should be presumed to comply with  the  cybersecurity  requirement  of  this  Regulation  in  so  far  as  the  cybersecurity  certificate  or  statement  of conformity or parts thereof cover the cybersecurity requirement of this Regulation. This remains without prejudice to  the  voluntary  nature  of  that  cybersecurity  scheme.
- (123) In  order  to  ensure  a  high  level  of  trustworthiness  of  high-risk  AI  systems,  those  systems  should  be  subject  to a  conformity  assessment  prior  to  their  placing  on  the  market  or  putting  into  service.
- (124) It is appropriate that, in order to minimise the burden on operators and avoid any possible duplication, for high-risk AI  systems  related  to  products  which  are  covered  by  existing  Union  harmonisation  legislation  based  on  the  New Legislative  Framework,  the  compliance  of  those  AI  systems  with  the  requirements  of  this  Regulation  should  be assessed as part of the conformity assessment already provided for in that law. The applicability of the requirements of  this  Regulation  should  thus  not  affect  the  specific  logic,  methodology  or  general  structure  of  conformity assessment under  the relevant  Union  harmonisation  legislation.
- (125) Given the complexity of high-risk AI systems and the risks that are associated with them, it is important to develop an  adequate  conformity  assessment  procedure  for  high-risk  AI  systems  involving  notified  bodies,  so-called  third party conformity assessment. However, given the current experience of professional pre-market certifiers in the field of  product safety  and  the  different  nature  of  risks  involved,  it  is  appropriate to  limit,  at  least  in  an  initial  phase  of application  of  this  Regulation,  the  scope  of  application  of  third-party  conformity  assessment  for  high-risk  AI systems  other  than  those  related  to  products.  Therefore,  the  conformity  assessment  of  such  systems  should  be carried  out  as  a  general  rule  by  the  provider  under  its  own  responsibility,  with  the  only  exception  of  AI  systems intended  to  be  used  for  biometrics.
- (126) In order to carry out third-party conformity assessments when so required, notified bodies should be notified under this  Regulation  by  the  national  competent  authorities,  provided  that  they  comply  with  a  set  of  requirements,  in particular  on  independence,  competence,  absence of  conflicts  of  interests  and  suitable  cybersecurity  requirements. Notification  of  those  bodies  should  be  sent  by  national  competent  authorities  to  the  Commission  and  the  other Member States by means of the electronic notification tool developed and managed by the Commission pursuant to Article  R23  of  Annex  I  to  Decision  No  768/2008/EC.
- (127) In line with Union commitments under the World Trade Organization Agreement on Technical Barriers to Trade, it is adequate to facilitate the  mutual  recognition  of  conformity  assessment  results  produced  by competent conformity assessment  bodies,  independent  of  the  territory  in  which  they  are  established,  provided  that  those  conformity assessment bodies established under the law of a third country meet the applicable requirements of this Regulation and the Union has concluded an agreement to that extent. In this context, the Commission should actively explore possible  international  instruments  for  that  purpose  and  in particular  pursue  the  conclusion  of  mutual  recognition agreements with third  countries.
- (128) In  line  with  the  commonly  established  notion  of  substantial  modification  for  products  regulated  by  Union harmonisation  legislation,  it  is  appropriate  that  whenever  a  change  occurs  which  may  affect  the  compliance  of a  high-risk  AI  system with  this  Regulation  (e.g.  change of operating  system  or  software  architecture),  or  when  the intended purpose of the system changes, that AI system should be considered to be a new AI system which should undergo  a  new  conformity  assessment.  However,  changes  occurring  to  the  algorithm  and  the  performance  of  AI systems  which  continue  to  'learn'  after  being  placed  on  the  market  or  put  into  service,  namely  automatically adapting  how  functions  are  carried  out,  should  not  constitute  a  substantial  modification,  provided  that  those changes have been pre-determined by the provider and  assessed at the  moment of  the  conformity  assessment.
- (129) High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely  within  the  internal  market.  For  high-risk  AI  systems  embedded  in  a  product,  a  physical  CE  marking should  be  affixed,  and  may  be  complemented  by  a  digital  CE  marking.  For  high-risk  AI  systems  only  provided digitally,  a  digital  CE  marking  should be used. Member States should not create unjustified obstacles to the placing on the market or  the putting into service of high-risk AI systems that comply with the requirements laid down in this  Regulation  and  bear  the  CE  marking.
- (130) Under  certain  conditions,  rapid  availability  of  innovative  technologies  may  be  crucial  for  health  and  safety  of persons, the protection of the environment and climate change and for society as a whole. It is thus appropriate that under  exceptional  reasons  of  public  security  or  protection  of  life  and  health  of  natural  persons,  environmental protection  and  the  protection  of  key  industrial  and  infrastructural  assets,  market  surveillance  authorities  could authorise  the  placing  on  the  market  or  the  putting  into  service  of  AI  systems  which  have  not  undergone a conformity assessment. In duly justified situations, as provided for in this Regulation, law enforcement authorities or  civil  protection  authorities  may  put  a  specific  high-risk  AI  system  into  service  without  the  authorisation  of  the market surveillance authority, provided that such authorisation is requested during or after  the use without undue delay.
- (131) In  order  to  facilitate  the  work of  the  Commission  and  the  Member  States in  the  AI  field  as  well  as  to  increase  the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system listed in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be required  to  register  themselves  and  information  about  their  AI  system  in  an  EU  database,  to  be  established  and managed  by  the  Commission.  Before  using  an  AI  system  listed  in  the  high-risk  use  cases  in  an  annex  to  this Regulation,  deployers  of  high-risk  AI  systems  that  are  public  authorities,  agencies  or  bodies,  should  register themselves in such database and select the system that they envisage to use. Other deployers should be entitled to do so voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should be easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example by  providing  search  functionalities,  including  through  keywords,  allowing  the  general  public  to  find  relevant information  to  be  submitted  upon  the  registration  of  high-risk  AI  systems  and  on  the  use  case  of  high-risk  AI systems,  set  out  in  an  annex  to  this  Regulation,  to  which  the  high-risk  AI  systems  correspond.  Any  substantial modification  of  high-risk  AI  systems should  also  be  registered  in  the  EU  database.  For  high-risk  AI  systems  in  the area of  law enforcement, migration, asylum and border control management, the registration obligations should be fulfilled in a secure non-public section of the EU database. Access to the secure non-public section should be strictly limited to the Commission as well as to market surveillance authorities with regard to their national section of that database. High-risk AI systems in the area of critical infrastructure should only be registered  at national level. The Commission should be the controller of the EU database, in accordance with Regulation (EU) 2018/1725. In order to  ensure  the  full  functionality  of  the  EU  database,  when  deployed,  the  procedure  for  setting  the  database  should include  the  development  of  functional  specifications  by  the  Commission  and  an  independent  audit  report.  The Commission should take into account cybersecurity risks when carrying out its tasks  as data controller on the EU database. In order to maximise the availability and use of the EU database by the public, the EU database, including the  information  made  available  through  it,  should  comply  with  requirements  under  the  Directive  (EU)  2019/882.
- (132) Certain  AI  systems  intended  to  interact  with  natural  persons  or  to  generate  content  may  pose  specific  risks  of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems  should  therefore  be  subject  to  specific  transparency  obligations  without  prejudice  to  the requirements  and  obligations  for  high-risk  AI  systems  and  subject  to  targeted  exceptions  to  take  into  account  the special need of law enforcement. In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect taking into account the circumstances and the context of use. When implementing that obligation, the characteristics of natural persons belonging to vulnerable groups due to their age or disability should be taken into account to the extent the AI system is intended to interact with those groups as well. Moreover, natural persons should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer the emotions or intentions of those persons or assign them to specific categories. Such specific categories can relate to  aspects  such  as  sex,  age,  hair  colour,  eye  colour,  tattoos,  personal  traits,  ethnic  origin,  personal  preferences  and interests.  Such information and notifications should be provided in accessible formats for persons with disabilities.
- (133) A variety of AI systems can generate large quantities of synthetic content that becomes increasingly hard for humans to  distinguish  from  human-generated  and  authentic  content.  The  wide  availability  and  increasing  capabilities  of those systems have a significant impact on the integrity and trust in the information ecosystem, raising new risks of misinformation and manipulation at scale, fraud, impersonation and consumer deception. In light of those impacts, the  fast  technological  pace  and  the  need  for  new  methods  and  techniques  to  trace  origin  of  information,  it  is appropriate  to  require  providers  of  those  systems  to  embed  technical  solutions  that  enable  marking  in  a  machine readable format and detection that the output has been generated or manipulated by an AI system and not a human. 