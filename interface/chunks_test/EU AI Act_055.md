The  technical  solutions  aiming  to  ensure  the  cybersecurity  of  high-risk  AI  systems  shall  be  appropriate  to  the  relevant circumstances  and  the  risks.
The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond  to,  resolve  and  control  for  attacks  trying  to  manipulate  the  training  data  set  (data  poisoning),  or  pre-trained components  used  in  training  (model  poisoning),  inputs  designed  to  cause  the  AI  model  to  make  a  mistake  (adversarial examples or  model evasion), confidentiality  attacks  or  model  flaws.
## SECTION 3
## Obligations of providers and deployers of high-risk AI systems and other  parties
## Article  16
## Obligations of providers of high-risk AI systems
Providers  of  high-risk  AI  systems  shall:
- (a) ensure  that  their  high-risk  AI  systems  are  compliant  with  the  requirements  set  out  in  Section  2;
- (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as  applicable,  their  name,  registered  trade  name or  registered  trade mark, the  address at which they can be contacted;
- (c) have  a  quality  management system  in place  which complies  with  Article  17;
- (d) keep the documentation referred to in Article 18;
- (e) when  under  their  control,  keep  the  logs  automatically  generated  by  their  high-risk  AI  systems  as  referred  to  in Article  19;
- (f) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43, prior  to  its  being  placed  on  the  market  or  put  into  service;
- (g) draw up an EU declaration of conformity in accordance with Article  47;
- (h) affix  the  CE  marking  to  the  high-risk  AI  system  or,  where  that  is  not  possible,  on  its  packaging  or  its  accompanying documentation, to indicate conformity with this Regulation,  in  accordance  with  Article  48;
- (i) comply with the registration  obligations  referred  to  in  Article  49(1);
- (j) take  the  necessary corrective  actions  and  provide  information  as  required  in  Article  20;
- (k) upon a reasoned request of a national competent authority, demonstrate the conformity of the high-risk AI system with the  requirements  set  out  in  Section  2;
- (l) ensure  that  the  high-risk  AI  system  complies  with  accessibility  requirements  in  accordance  with  Directives  (EU) 2016/2102 and (EU) 2019/882.
## Article  17
## Quality management system
- 1. Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions,  and  shall  include  at  least  the  following aspects:
- (a) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the  management of modifications  to the high-risk  AI  system;
- (b) techniques, procedures and systematic actions to be used for  the design, design control and design verification of the high-risk  AI  system;
- (c) techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the  high-risk  AI  system;
- (d) examination, test and validation procedures to be carried out before, during and after the development of the high-risk AI  system,  and  the  frequency  with  which they  have  to  be  carried  out;
- (e) technical  specifications,  including  standards,  to  be  applied  and,  where  the  relevant  harmonised  standards  are  not applied in full or do not cover all of the relevant requirements set out in Section 2, the means to be used to ensure that the  high-risk  AI  system  complies  with  those  requirements;
- (f) systems and procedures for data management, including data acquisition, data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for  the purpose of the placing on the market or  the putting into service of high-risk AI systems;
- (g) the  risk  management  system  referred  to  in  Article  9;
- (h) the setting-up, implementation and maintenance of a post-market monitoring system, in accordance with Article 72;
- (i) procedures related to the  reporting  of a  serious  incident  in  accordance  with  Article  73;
- (j) the  handling  of  communication  with  national  competent  authorities,  other  relevant  authorities,  including  those providing  or  supporting  the  access  to  data,  notified  bodies,  other  operators,  customers  or  other  interested  parties;
- (k) systems  and  procedures  for  record-keeping  of all  relevant  documentation  and  information;
- (l) resource  management,  including  security-of-supply  related measures;
- (m) an accountability framework setting out the responsibilities of  the management and other staff with regard to all the aspects  listed  in  this  paragraph.
- 2. The  implementation  of  the  aspects  referred  to  in  paragraph  1  shall  be  proportionate  to  the  size  of  the  provider's organisation.  Providers shall,  in  any event,  respect  the  degree  of  rigour  and  the  level  of  protection  required  to  ensure  the compliance of  their  high-risk  AI  systems  with  this  Regulation.
- 3. Providers  of  high-risk  AI  systems  that  are  subject  to  obligations  regarding  quality  management  systems  or  an equivalent function under relevant sectoral Union law may include the aspects listed in paragraph 1 as part of the quality management systems pursuant to that law.
- 4. For providers that are financial institutions subject to requirements regarding their internal governance, arrangements or  processes  under  Union  financial  services  law,  the  obligation  to  put  in  place  a  quality  management  system,  with  the exception of paragraph 1, points (g), (h) and (i) of this Article, shall be deemed to be fulfilled by complying with the rules on internal  governance  arrangements  or  processes  pursuant  to  the  relevant  Union  financial  services  law.  To  that  end,  any harmonised standards referred to in  Article  40  shall  be  taken  into  account.
## Article  18
## Documentation keeping
- 1. The provider shall, for a period ending 10 years after  the high-risk AI system has been placed on the market or put into  service,  keep  at  the  disposal  of  the  national  competent  authorities:
- (a) the  technical  documentation  referred  to  in  Article  11;
- (b) the  documentation concerning the quality management system referred  to in  Article  17;
- (c) the  documentation concerning the changes approved by notified bodies,  where applicable;
- (d) the  decisions  and  other  documents  issued  by  the  notified  bodies,  where  applicable;
- (e) the  EU  declaration  of  conformity  referred  to  in  Article  47.
- 2. Each Member State shall determine conditions under which the documentation referred to in paragraph 1 remains at the disposal of the national competent authorities for the period indicated in that paragraph for the cases when a provider or  its  authorised  representative  established  on  its  territory  goes  bankrupt  or  ceases  its  activity  prior  to  the  end  of  that period.
- 3. Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the technical documentation as part of the documentation kept under  the  relevant  Union  financial  services  law.
## Article  19
## Automatically generated logs
- 1. Providers  of  high-risk  AI  systems  shall  keep  the  logs  referred  to  in  Article  12(1),  automatically  generated  by  their high-risk AI systems, to the extent such logs are under their control. Without prejudice to applicable Union or national law, the logs shall be kept for a period appropriate to the intended purpose of  the high-risk AI system, of at least six months, unless provided otherwise in the applicable Union or national law, in particular in Union law on the protection of personal data.
- 2. Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs automatically generated by their high-risk AI systems as  part  of  the  documentation  kept  under  the  relevant  financial  services  law.
## Article  20
## Corrective actions and duty of  information
- 1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed  on  the  market  or  put  into  service  is  not  in  conformity  with  this  Regulation  shall  immediately  take  the  necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall inform  the  distributors  of  the  high-risk  AI  system  concerned  and,  where  applicable,  the  deployers,  the  authorised representative  and  importers  accordingly.
- 2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of that  risk,  it  shall  immediately  investigate  the  causes,  in  collaboration  with  the  reporting  deployer,  where  applicable,  and inform  the  market  surveillance  authorities  competent  for  the  high-risk  AI  system  concerned  and,  where  applicable,  the notified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature of  the  non-compliance  and  of any relevant  corrective  action  taken.
## Article  21
## Cooperation with competent authorities
- 1. Providers of high-risk AI systems shall, upon a reasoned request by a competent authority, provide that authority all the  information  and  documentation  necessary  to  demonstrate  the  conformity  of  the  high-risk  AI  system  with  the requirements  set  out  in  Section  2,  in  a  language  which  can  be  easily  understood  by  the  authority  in  one  of  the  official languages  of  the  institutions  of  the  Union  as  indicated  by  the  Member  State  concerned.
- 2. Upon a reasoned request by a competent authority, providers shall also give the requesting competent authority, as applicable, access to the automatically generated logs of  the high-risk AI system referred to in Article 12(1), to the extent such  logs  are  under  their  control.
- 3. Any information obtained by a competent authority pursuant to this Article shall be treated in accordance with the confidentiality obligations  set  out  in  Article  78.
## Article  22
## Authorised representatives of providers of high-risk AI systems
- 1. Prior  to  making  their  high-risk  AI  systems  available  on  the  Union  market,  providers  established  in  third  countries shall,  by  written  mandate,  appoint  an  authorised  representative  which is  established  in  the  Union.
- 2. The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.
- 3. The  authorised  representative  shall  perform  the  tasks  specified  in  the  mandate  received  from  the  provider.  It  shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institutions of  the Union, as indicated by the competent authority. For  the purposes of  this Regulation, the mandate shall empower the authorised representative to carry out the  following tasks:
- (a) verify  that  the  EU  declaration  of  conformity  referred  to  in  Article  47  and  the  technical  documentation  referred  to  in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by the provider;
- (b) keep at  the  disposal  of  the  competent  authorities  and  national  authorities  or  bodies  referred  to  in  Article  74(10),  for a period of 10 years after the high-risk AI system has been placed on the market or put into service, the contact details of the provider  that appointed the authorised representative, a copy of the EU declaration of conformity referred to in Article  47,  the  technical  documentation  and,  if  applicable,  the  certificate  issued  by  the  notified  body;
- (c) provide  a  competent  authority,  upon  a  reasoned  request,  with  all  the  information  and  documentation,  including  that referred to in point (b) of this subparagraph, necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2, including access to the logs, as referred to in Article 12(1), automatically generated by the  high-risk  AI  system,  to  the  extent  such  logs  are  under  the  control  of  the  provider;
- (d) cooperate with competent authorities, upon a reasoned request, in any action the latter take in relation to the high-risk AI  system,  in  particular  to  reduce  and  mitigate  the  risks  posed  by  the  high-risk  AI  system;
- (e) where applicable, comply with the registration obligations referred to in Article 49(1), or, if  the registration is carried out  by  the  provider  itself,  ensure  that  the  information  referred  to  in  point  3  of  Section  A  of  Annex  VIII  is  correct.
The mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the competent authorities,  on  all  issues  related  to  ensuring  compliance  with  this  Regulation.
- 4. The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to this Regulation. In such a case, it shall immediately inform the relevant market surveillance authority, as well as, where applicable, the relevant notified body, about the termination of the mandate and the reasons  therefor.
## Article  23
## Obligations of  importers
- 1. Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation  by verifying  that:
- (a) the  relevant  conformity  assessment  procedure  referred  to  in  Article  43  has  been  carried  out  by  the  provider  of  the high-risk  AI  system;
- (b) the  provider  has  drawn  up  the  technical  documentation  in  accordance  with  Article  11  and  Annex  IV;
- (c) the  system  bears  the  required  CE  marking  and  is  accompanied  by  the  EU  declaration  of  conformity  referred  to  in Article  47  and  instructions  for  use;
- (d) the  provider  has  appointed  an  authorised  representative  in  accordance  with  Article  22(1).
- 2. Where  an  importer  has  sufficient  reason  to  consider  that  a  high-risk  AI  system  is  not  in  conformity  with  this Regulation, or is falsified, or accompanied by falsified documentation, it shall not place the system on the market until it has been  brought  into  conformity.  Where  the  high-risk  AI  system  presents  a  risk  within  the  meaning  of  Article  79(1),  the importer shall inform the provider of  the system, the authorised representative and the market surveillance authorities to that  effect.
- 3. Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they can be  contacted on  the  high-risk  AI  system  and  on  its  packaging  or  its  accompanying  documentation,  where  applicable.
- 4. Importers shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable,  do  not  jeopardise  its  compliance  with  the  requirements  set  out  in  Section  2.
- 5. Importers shall keep, for a period of 10 years after the high-risk AI system has been placed on the market or put into service,  a  copy of  the  certificate  issued  by  the  notified  body,  where  applicable,  of  the  instructions  for  use,  and  of  the  EU declaration  of  conformity  referred  to  in  Article  47.
- 6. Importers  shall  provide  the  relevant  competent  authorities,  upon  a  reasoned  request,  with  all  the  necessary information and documentation, including that referred to in paragraph 5, to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2 in a language which can be easily understood by them. For this purpose, they shall  also  ensure  that  the  technical  documentation  can  be  made  available  to  those  authorities.
- 7. Importers shall cooperate with the relevant competent authorities in any action those authorities take in relation to a  high-risk  AI  system  placed  on  the  market  by  the  importers,  in  particular  to  reduce  and  mitigate  the  risks  posed  by  it.
## Article  24
## Obligations of distributors
- 1. Before  making  a  high-risk  AI  system  available  on  the  market,  distributors  shall  verify  that  it  bears  the  required  CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid  down in  Article  16,  points  (b)  and  (c)  and  Article  23(3).
- 2. Where  a  distributor  considers  or  has  reason  to  consider,  on  the  basis  of  the  information  in  its  possession,  that a  high-risk  AI  system  is  not  in  conformity  with  the  requirements  set  out  in  Section  2,  it  shall  not  make  the  high-risk  AI system available on the market until the system has been brought into conformity with those requirements. Furthermore, where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall inform the provider or  the  importer  of  the  system,  as  applicable,  to  that  effect.
- 3. Distributors  shall  ensure  that,  while  a  high-risk  AI  system  is  under their  responsibility,  storage  or  transport conditions, where applicable, do not jeopardise the compliance of  the system with the requirements set out in Section 2.
- 4. A distributor that considers or has reason to consider, on the basis of the information in its possession, a high-risk AI system which it has made available on the market not to be in conformity with the requirements set out in Section 2, shall take the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recall it,  or  shall  ensure  that  the  provider,  the  importer  or  any  relevant  operator,  as  appropriate,  takes  those  corrective  actions. Where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall immediately inform the provider or importer of the system and the authorities competent for the high-risk AI system concerned, giving details, in  particular,  of  the  non-compliance  and  of  any corrective  actions  taken.
- 5. Upon a reasoned request from a relevant competent authority, distributors of a high-risk AI system shall provide that authority with all the information and documentation regarding their actions pursuant to paragraphs 1 to 4 necessary to demonstrate the conformity of  that system with  the  requirements  set out  in  Section  2.
- 6. Distributors shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system made available on the market by the distributors, in particular to reduce or mitigate the risk posed by it.
## Article  25
## Responsibilities along the AI value chain
- 1. Any distributor, importer, deployer or other third-party shall be considered to be a provider of a high-risk AI system for  the  purposes of  this Regulation and shall be subject to the obligations of  the provider under Article 16, in any of  the following  circumstances:
- (a) they put their  name or  trademark on a high-risk AI system already placed on the market or  put into service, without prejudice  to  contractual  arrangements  stipulating  that  the  obligations  are  otherwise  allocated;
- (b) they make a substantial modification to a high-risk AI system that has already been placed on the market or has already been put into service  in  such  a  way  that  it  remains  a  high-risk  AI  system pursuant  to  Article  6;
- (c) they modify the intended purpose of an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service in such a way that the AI system concerned becomes a high-risk AI system in  accordance with Article 6.
- 2. Where  the  circumstances  referred  to  in  paragraph  1  occur,  the  provider  that  initially  placed  the  AI  system  on  the market or put it into service shall no longer be considered to be a provider of that specific AI system for  the purposes of this  Regulation.  That  initial  provider  shall  closely  cooperate  with  new  providers  and  shall  make  available  the  necessary information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of the  obligations  set  out  in  this  Regulation,  in  particular  regarding  the  compliance  with  the  conformity  assessment  of high-risk  AI  systems.  This  paragraph  shall  not  apply  in  cases  where  the  initial  provider  has  clearly  specified  that  its  AI system is not to be changed into a high-risk AI system and therefore does not fall under  the obligation to hand over  the documentation.
- 3. In  the  case  of  high-risk  AI  systems  that  are  safety  components  of  products  covered  by  the  Union  harmonisation legislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk AI  system,  and  shall  be  subject  to  the  obligations  under  Article  16  under  either  of  the  following  circumstances:
- (a) the high-risk AI system is placed on the market together with the product under the name or trademark of the product manufacturer;
- (b) the high-risk AI system is put into service under the name or trademark of the product manufacturer after the product has  been  placed  on  the  market.
- 4. The provider of a high-risk AI system and the third party that supplies an AI system, tools, services, components, or processes that are used or integrated in a high-risk AI system shall, by written agreement, specify the necessary information, capabilities,  technical access and other assistance based on the generally acknowledged state of  the art, in order  to enable the provider of the high-risk AI system to fully comply with the obligations set out in this Regulation. This paragraph shall not  apply  to  third  parties  making  accessible  to  the  public  tools,  services,  processes,  or  components,  other  than general-purpose  AI  models,  under  a  free  and  open-source  licence.
The AI Office may develop and recommend voluntary model terms for contracts between providers of high-risk AI systems and  third  parties  that  supply  tools,  services,  components  or  processes  that  are  used  for  or  integrated  into  high-risk  AI systems.  When  developing  those  voluntary  model  terms,  the  AI  Office  shall  take  into  account  possible  contractual requirements applicable in specific sectors or business cases. The voluntary model terms shall be published and be available free  of  charge  in  an  easily  usable  electronic  format.
- 5. Paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual property rights, confidential business  information  and  trade  secrets  in  accordance  with  Union  and  national  law.
## Article  26
## Obligations of deployers of high-risk AI systems
- 1. Deployers  of  high-risk  AI  systems  shall  take  appropriate  technical  and  organisational  measures  to  ensure  they  use such  systems  in  accordance  with  the  instructions  for  use  accompanying  the  systems,  pursuant  to  paragraphs  3  and  6.
- 2. Deployers  shall  assign  human  oversight  to  natural  persons  who  have  the  necessary  competence,  training  and authority,  as  well  as  the  necessary  support.
- 3. The obligations set out  in paragraphs  1  and  2,  are  without  prejudice  to  other  deployer  obligations  under  Union  or national law and to the deployer's freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.
- 4. Without  prejudice  to  paragraphs  1  and  2,  to  the  extent  the  deployer  exercises  control  over  the  input  data,  that deployer  shall  ensure  that  input  data  is  relevant  and  sufficiently  representative  in  view  of  the  intended  purpose  of  the high-risk  AI  system.
- 5. Deployers shall monitor the operation of the high-risk AI system on the basis of the instructions for use and, where relevant,  inform  providers  in  accordance  with  Article  72.  Where  deployers  have  reason  to  consider  that  the  use  of  the high-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of Article  79(1),  they  shall,  without  undue  delay,  inform  the  provider  or  distributor  and  the  relevant  market  surveillance authority,  and  shall  suspend  the  use  of  that  system.  Where  deployers  have  identified  a  serious  incident,  they  shall  also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of  that  incident.  If  the  deployer  is  not  able  to  reach  the  provider,  Article  73  shall  apply mutatis  mutandis .  This  obligation shall  not  cover  sensitive  operational  data  of  deployers  of  AI  systems  which  are  law  enforcement  authorities.
For  deployers  that  are  financial  institutions  subject  to  requirements  regarding  their  internal  governance,  arrangements  or processes under Union financial services law, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to the relevant  financial  service  law.
- 6. Deployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system, of at least six months,  unless  provided  otherwise  in  applicable  Union  or  national  law,  in  particular  in  Union  law  on  the  protection  of personal  data.
Deployers  that  are  financial  institutions  subject  to  requirements  regarding  their  internal  governance,  arrangements  or processes  under  Union  financial  services  law  shall  maintain  the  logs  as  part  of  the  documentation  kept  pursuant  to  the relevant  Union  financial  service  law.
- 7. Before putting into service or using a high-risk AI system at the workplace, deployers who are employers shall inform workers'  representatives  and  the  affected  workers  that  they  will  be  subject  to  the  use  of  the  high-risk  AI  system.  This information  shall  be  provided,  where  applicable,  in  accordance  with  the  rules  and  procedures  laid  down  in  Union  and national  law  and  practice  on  information  of  workers  and  their  representatives.
- 8. Deployers of high-risk AI systems that are public authorities, or Union institutions, bodies, offices  or agencies  shall comply with the registration obligations referred to in Article 49. When such deployers find that the high-risk AI system that they envisage using has not been registered in the EU database referred to in Article 71, they shall not use that system and shall  inform  the  provider  or  the  distributor.
- 9. Where  applicable,  deployers  of  high-risk  AI  systems  shall  use  the  information  provided  under  Article  13  of  this Regulation to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU)  2016/679 or  Article  27  of  Directive  (EU)  2016/680.
- 10. Without  prejudice  to  Directive  (EU)  2016/680,  in  the  framework  of  an  investigation  for  the  targeted  search  of a  person  suspected  or  convicted  of  having  committed  a  criminal  offence,  the  deployer  of  a  high-risk  AI  system  for post-remote biometric identification shall  request  an  authorisation, ex  ante ,  or  without  undue  delay  and  no  later  than  48 hours, by a judicial authority or an administrative authority whose decision is binding and subject to judicial review, for the use of that system, except when it is used for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence. Each use shall be limited to what is strictly necessary for  the investigation of a specific criminal  offence.
- If  the  authorisation  requested  pursuant  to  the  first  subparagraph  is  rejected,  the  use  of  the  post-remote  biometric identification  system  linked  to  that  requested  authorisation  shall  be  stopped  with  immediate  effect  and  the  personal  data linked  to  the  use  of  the  high-risk  AI  system  for  which  the  authorisation  was  requested  shall  be  deleted.
In no case shall such high-risk AI system for post-remote biometric identification be used for law enforcement purposes in an  untargeted  way,  without  any  link  to  a  criminal  offence,  a  criminal  proceeding,  a  genuine  and  present  or  genuine  and foreseeable threat of a criminal offence, or the search for a specific missing person. It shall be ensured that no decision that produces an adverse legal effect on a person may be taken by the law enforcement authorities based solely on the output of such  post-remote biometric  identification  systems.
This paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and Article 10 of Directive (EU) 2016/680 for  the  processing  of  biometric  data.
Regardless of the purpose or deployer, each use of such high-risk AI systems shall be documented in the relevant police file and shall be made available to the relevant market surveillance authority and the national data protection authority upon request,  excluding  the  disclosure  of  sensitive  operational  data  related  to  law  enforcement.  This  subparagraph  shall  be without prejudice  to the  powers  conferred  by  Directive  (EU)  2016/680  on  supervisory  authorities.
Deployers shall submit annual reports to the relevant market surveillance and national data protection authorities on their use  of  post-remote  biometric  identification  systems,  excluding  the  disclosure  of  sensitive  operational  data  related  to  law enforcement. The reports may be aggregated to cover  more than one deployment.
Member States may introduce, in accordance with  Union  law,  more restrictive  laws on  the  use  of  post-remote biometric identification  systems.
- 11. Without prejudice to Article 50 of  this  Regulation, deployers of high-risk AI systems referred to  in Annex III that make decisions or assist in making decisions related to natural persons shall inform the natural persons that they are subject to the use of the high-risk AI system. For high-risk AI systems used for law enforcement purposes Article 13 of Directive (EU)  2016/680 shall  apply.
- 12. Deployers shall cooperate with the relevant competent authorities in any action those authorities take in relation to the  high-risk  AI  system  in  order  to  implement  this  Regulation.
## Article  27
## Fundamental rights impact assessment for high-risk AI systems
- 1. Prior  to  deploying  a  high-risk  AI  system  referred  to  in  Article  6(2),  with  the  exception  of  high-risk  AI  systems intended to be used in the area listed in point 2 of Annex III, deployers that are bodies governed by public law, or are private entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III, shall perform an assessment of  the impact on fundamental rights that the use of such system may produce. For  that purpose, deployers shall  perform  an  assessment  consisting  of:
- (a) a description of the deployer's processes in which the high-risk AI system will be used in line with its intended purpose;
- (b) a description of the period of time within which, and the frequency with which, each high-risk AI system is intended to be  used;
- (c) the  categories  of  natural  persons  and  groups  likely  to  be  affected  by  its  use  in  the  specific  context;
- (d) the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant  to  point  (c)  of  this  paragraph,  taking  into  account  the  information  given  by  the  provider  pursuant  to Article  13;
- (e) a  description  of  the  implementation  of  human  oversight  measures,  according  to  the  instructions  for  use;
- (f) the  measures  to  be  taken  in  the  case  of  the  materialisation  of  those  risks,  including  the  arrangements  for  internal governance  and  complaint  mechanisms.
- 2. The  obligation  laid  down  in  paragraph  1  applies  to  the  first  use  of  the  high-risk  AI  system.  The  deployer  may,  in similar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried out  by  provider.  If,  during  the  use  of  the  high-risk  AI  system,  the  deployer  considers  that  any  of  the  elements  listed  in paragraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.
- 3. Once  the  assessment  referred  to  in  paragraph  1  of  this  Article  has  been  performed,  the  deployer  shall  notify  the market surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of  this Article as part  of  the  notification.  In  the  case  referred  to  in  Article  46(1),  deployers  may  be  exempt  from  that  obligation  to  notify.
- 4. If  any  of  the  obligations  laid  down  in  this  Article  is  already  met  through  the  data  protection  impact  assessment conducted pursuant to Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental rights  impact  assessment  referred  to  in  paragraph  1  of  this  Article  shall  complement  that  data  protection  impact assessment.
- 5. The AI Office shall develop a template for a questionnaire, including through an automated tool, to facilitate deployers in  complying  with  their  obligations  under  this  Article  in  a  simplified  manner.
## SECTION 4
## Notifying authorities and notified  bodies
## Article  28
## Notifying authorities
- 1. Each Member State shall designate or establish at least one notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring.  Those procedures shall  be  developed  in  cooperation  between  the  notifying  authorities  of all  Member  States.
